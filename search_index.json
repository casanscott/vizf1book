[["index.html", "Visualizing Formula 1 Data in R Chapter 1 About the book 1.1 Cover Art 1.2 About me 1.3 Why did I write this book? 1.4 Organization of this book 1.5 R 1.6 ggplot2 1.7 The tidyverse 1.8 Data 1.9 Data Visualization", " Visualizing Formula 1 Data in R Casan Scott, Ph.D. 2024-02-24 Chapter 1 About the book This is a book for anyone that finds themselves located within the fairly obscure intersection of the data enthusiast-Formula 1 fan Venn diagram (see Figure below: 1.9 Data Visualization)). The overarching goal of the book is to provide a gentle introduction to analyzing Formula 1 data using the R programming language. It includes some brief commentary on the sport of Formula 1, lots of data visualizations, and R code used to create each plot. While this book is primarily written for people interested in data visualization using the R programming language, I do hope that some of the nerdier Formula 1 fans will find these data visualizations interesting. 1.1 Cover Art The cover art is the work of Alison Gaspard. Alison is a tattoo artist, BFA in studio art, and former Visual Arts/Art History educator located in Houston, Texas. You can find Alison here: https://www.alisonlenayfineart.com/ 1.2 About me TLDR: Casan Scott, Ph.D., Senior Data Scientist More information: I am a bit of a nomad, professionally. Academically, I graduated with B.S. and Ph.D. degrees in environmental science and my Ph.D. research focused on the toxicology of fish. Naturally, I decided to not use my doctorate degree and became a data scientist! I fell in love with the R programming language during graduate school, and decided I would rather analyze data than collect it. Over the past five years, I’ve worked as a data scientist in both the public and private sectors. At times, I also contribute to research on human performance topics. Outside of my nerdier professional pursuits, I enjoy competing in powerlifting, trail running, reading westerns, and hanging out with my wife and dogs. 1.3 Why did I write this book? To scratch my own itch per se. Formula 1 is actually a very complicated sport, and while learning about it I had lots of questions: Do practice times mean anything? Does the qualifying time predict race pace? How much do the cars improve each year? Why is Red Bull so good in Mexico City? Of course, there were plenty of answers to these questions online, but I really wanted to try and answer some of these questions myself. So, this book documents my attempt at doing so. Along the way, I include the code to create each plot and some brief annotations about the functions used. 1.4 Organization of this book I aimed to loosely organize this book like a typical Formula 1 weekend: (1) practice data, (2) qualifying data , and (3) results from the Grand Prix. For the most part, I try to summarize the historical data, visualize possible trends in the data, explore important differences, plot potential correlations, and perhaps build a model that can explain some relationship(s) that seems interesting. Along the way, I may pursue a tangent or two. Each chapter begins with a racing-centric introduction, followed by various analyses. The organization of chapters in this book are as follows: Chapter 2: Introduction to Formula 1 An introduction to Formula 1 racing and the sessions in a weekend Pulling data using the drs package Basic visualizations of practice data Chapter 3: Qualifying Plotting distributions of qualifying data Visualizing qualifying times over time Chapter 4: Relating practice times to qualifying times Simple linear regression models Confidence intervals Interactions Chapter 5: Drivers Using heatmaps to visualize race results for drivers Comparisons between teammates Chapter 6: Race Results Starting position vs Final Placing Ordinal regression models Plotting ordinal regression results Chapter 7: DNFs Rates of DNFs (Did not finish) Logistic regression models Interpreting logistic regression models Chapter 8: Champions Visualizing championship fights 1.5 R R is a programming language that is largely used for statistical analysis and data visualization. One of R’s biggest strengths is the ease with which high quality data visualizations can be be produced. This feature is what drew me to the language back in 2013, and is the reason that I am able to write this book! For information on R, visit this link: https://www.r-project.org/about.html 1.6 ggplot2 I am assuming that you have some experience using the R programming language. If you have never used R before, you will quickly become very confused! This entire book is based on the ggplot2 package in R. ggplot2 is a package used for producing statistical and other data graphics. What makes ggplot2 unique is it has an underlying grammar, based on the Grammar of Graphics (Wilkinson 2005). This attribute allows you to create graphs by combining independent components to suit your particular problem. That may seem like a trivial distinction, but it makes ggplot2 very powerful! Note: At times, I may refer to ggplot2 as simply ggplot. If that is annoying… apologies in advance! 1.7 The tidyverse Throughout this book, I rely on the tidyverse. The tidyverse is actually a collection of other packages that share common data representations and design structures. This enables these packages to work together very conveniently. For instance, I nearly always use functions from the dplyr package to wrangle data prior to plotting with ggplot2. You can install the tidyverse core packages with a single command: # Install the package from CRAN install.packages(&quot;tidyverse&quot;) # Load the package library(tidyverse) The core pakcages in tidyverse are: ggplot2, for data visualization. dplyr, for data manipulation. tidyr, for data tidying. readr, for data import. purrr, for functional programming. tibble, for tibbles, a modern re-imagining of data frames. stringr, for strings. forcats, for factors. lubridate, for date/times. Aside from ggplot2, I most heavily use dplyr in this book. There are six key dplyr functions that can solve most data manipulation challenges: select(): subset columns from a dataframe. In other words, you use this to choose variables by name. filter(): filter the dataframe by values of a variable. This can be both numeric (i.e. age &gt; 21 years) or categorical (i.e. month == June). arrange(): order the rows of the dataframe mutate(): create a new variable summarize(): collapse values down to a summarized metric (i.e. mean, minimum, maximum). group_by(): operates any of the previous functions on a group basis (i.e. a grouped mean). I highly recommend this book chapter for more information on dplyr basics: https://r4ds.had.co.nz/transform.html For a thorough introduction to the tidyverse, read the entire fantastic book: https://r4ds.had.co.nz/index.html 1.8 Data The data used in this book was pulled from formula1.com. To make it easier for readers to use this data, I developed the drs (Data for Racing Simulations) R package. If you currently follow Formula 1, you will recognize that DRS also stands for Drag Reduction System, which is a crucial component of Formula 1 cars that opens space in the rear wing thereby reducing drag. The drs R package utilizes functions from the rvest and dplyr packages to scrape and tidy data from the formula1.com website. It is designed to scrape data for all Grand Prix weekends during a given year. Each function returns a dataframe that you can then use for analysis and visualizations. Notes about a Formula 1 Grand Prix Weekend A Grand Prix weekend consists of practice sessions, qualifying, and a race. Additionally, some weekends will also include a sprint race (an abbreviated sprint race that is typically about 1/3 the length of a normal race). A typical Formula 1 weekend begins with three practice sessions. The first two practice sessions (FP1 and FP2) are held on Friday. On Saturday, the third practice session (FP3) is held, followed by Qualifying. Qualifying determines the grid for the race on Sunday. Currently, there are three heats in qualifying (Q1, Q2, and Q3). All cars compete in the first heat (Q1), and the top 15 fastest times advance to the 2nd heat, Q2. From there, the top 10 fastest times during heat 2 advance to Q3 (heat 3). The starting grid is determined by a driver’s final qualifying position (pending penalties). The race takes place on Sunday. drs currently consists of four scraping functions: practice_session_scraper(): Scrapes the best times for a given practice session. qualifying_scraper(): Scrapes the best qualifying times during Q1, Q2, and Q3. starting_grid_scraper(): Scrapes the final starting grid positions for the Grand Prix. race_result_scraper()(): Scrapes the race results for a Grand Prix (i.e. finishing position and total time). 1.8.0.1 Installation of drs You can install the development version of drs here: install_github(&quot;casanscott/drs&quot;) 1.8.0.2 Using the drs package These functions from drs require both the tidyverse and rvest packages. The practice_session_scraper() requires two arguments: year and practice_session_number. After loading those libraries, along with drs, you can easily scrape data using a function call like this: library(tidyverse) library(rvest) library(drs) # pull FP3 practice data p32022 &lt;- practice_session_scraper(2022, 3) # View the first 6 rows head(p32022) ## Position CarNumber First Last Driver Car Time Race Circuit Year Time_secs ## 1 1 1 Max Verstappen VER Red Bull Racing RBPT 1:32.544 bahrain bahrain 2022 92.544 ## 2 2 16 Charles Leclerc LEC Ferrari 1:32.640 bahrain bahrain 2022 92.640 ## 3 3 11 Sergio Perez PER Red Bull Racing RBPT 1:32.791 bahrain bahrain 2022 92.791 ## 4 4 63 George Russell RUS Mercedes 1:32.935 bahrain bahrain 2022 92.935 ## 5 5 55 Carlos Sainz SAI Ferrari 1:33.053 bahrain bahrain 2022 93.053 ## 6 6 44 Lewis Hamilton HAM Mercedes 1:33.121 bahrain bahrain 2022 93.121 The rest of the drs web scraping functions require a single argument: year. The following function will scrape all qualifying results from 2022: # pull qualifying data quali2022 &lt;- qualifying_scraper(2022) # View the first 6 rows head(quali2022) ## Position CarNumber First Last Driver Car Laps Q1 Q2 Q3 Race Circuit ## 1 1 16 Charles Leclerc LEC Ferrari 15 1:31.471 1:30.932 1:30.558 bahrain bahrain ## 2 2 1 Max Verstappen VER Red Bull Racing RBPT 14 1:31.785 1:30.757 1:30.681 bahrain bahrain ## 3 3 55 Carlos Sainz SAI Ferrari 15 1:31.567 1:30.787 1:30.687 bahrain bahrain ## 4 4 11 Sergio Perez PER Red Bull Racing RBPT 18 1:32.311 1:31.008 1:30.921 bahrain bahrain ## 5 5 44 Lewis Hamilton HAM Mercedes 17 1:32.285 1:31.048 1:31.238 bahrain bahrain ## 6 6 77 Valtteri Bottas BOT Alfa Romeo Ferrari 15 1:31.919 1:31.717 1:31.560 bahrain bahrain ## Year Q1_secs Q2_secs Q3_secs ## 1 2022 91.471 90.932 90.558 ## 2 2022 91.785 90.757 90.681 ## 3 2022 91.567 90.787 90.687 ## 4 2022 92.311 91.008 90.921 ## 5 2022 92.285 91.048 91.238 ## 6 2022 91.919 91.717 91.560 To scrape the starting grids for every Grand Prix during 2022, use the following function call: # pull starting grids grids2022 &lt;- starting_grid_scraper(2022) # View the first 6 rows head(grids2022) ## Position CarNumber First Last Driver Car Time Race Circuit Year Time_secs ## 1 1 16 Charles Leclerc LEC Ferrari 1:30.558 bahrain bahrain 2022 90.558 ## 2 2 1 Max Verstappen VER Red Bull Racing RBPT 1:30.681 bahrain bahrain 2022 90.681 ## 3 3 55 Carlos Sainz SAI Ferrari 1:30.687 bahrain bahrain 2022 90.687 ## 4 4 11 Sergio Perez PER Red Bull Racing RBPT 1:30.921 bahrain bahrain 2022 90.921 ## 5 5 44 Lewis Hamilton HAM Mercedes 1:31.238 bahrain bahrain 2022 91.238 ## 6 6 77 Valtteri Bottas BOT Alfa Romeo Ferrari 1:31.560 bahrain bahrain 2022 91.560 To scrape the race results for every Grand Prix during 2022, use the following function call: # Pull race results races2022 &lt;- race_result_scraper(2022) # View the first 6 rows head(races2022) ## # A tibble: 6 × 13 ## Position CarNumber First Last Driver Car Laps Time Points Race Circuit Year Time_secs ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 16 Charles Leclerc LEC Ferrari 57 1:37:33.584 26 bahr… bahrain 2022 5854. ## 2 2 55 Carlos Sainz SAI Ferrari 57 +5.598s 18 bahr… bahrain 2022 6752. ## 3 3 44 Lewis Hamilton HAM Mercedes 57 +9.675s 15 bahr… bahrain 2022 7069. ## 4 4 63 George Russell RUS Mercedes 57 +11.211s 12 bahr… bahrain 2022 6725. ## 5 5 20 Kevin Magnussen MAG Haas Ferrari 57 +14.754s 10 bahr… bahrain 2022 7448. ## 6 6 77 Valtteri Bottas BOT Alfa Romeo Ferrari 57 +16.119s 8 bahr… bahrain 2022 6933. You can then use these dataframes to create cool data visualizations like this one: 1.9 Data Visualization In addition to commentary about Formula 1 racing, I’ll also include brief instructions about how to create the data visualizations in this book. For example, use this chunk of code… library(ggvenn) library(ggplot2) x &lt;- list(`Formula 1 fan` = rep(&#39;This book!&#39;, 1), `Data enthusiast` = rep(&#39;This book!&#39;, 1)) ggvenn( x, show_elements = T, show_percentage = F, fill_color = c(&quot;#CD534CFF&quot;, &quot;#0073C2FF&quot;), stroke_size = 0.5, stroke_alpha = 0.5, set_name_size = 4 ) … to create this figure: I’ll also occasionally include shaded boxes with supplemental information about a particular chart type, an interesting R package, or anything else that seems noteworthy. For example, the following shaded box includes additional information on the R package used to create the Venn diagram (ggvenn): How to create a Venn diagram in R I used the ggvenn and ggplot2 packages to create this simple Venn diagram. The ggvenn package was created by Linlin Yan. In my opinion, ggvenn is the easiest way to create simple Venn diagrams that follow the typical ggplot2 styling and syntax. For more information about the ggvenn package, visit this link: https://cran.r-project.org/web/packages/ggvenn/index.html If you’ve never used ggplot2 before (boy, are you in for a treat!), check out this link to the ggplot book written by legends of the tidyverse Hadley Wickham, Danielle Navarro, and Thomas Lin Pedersen: https://ggplot2-book.org/ In the next chapter, we will dive into Formula 1 data. I’ll start with a gentle introduction to Formula 1 and a typical Grand Prix weekend. "],["introduction-to-formula-1.html", "Chapter 2 Introduction to Formula 1 2.1 Data 2.2 A Formula 1 Weekend 2.3 Practice Sessions 2.4 Practice Times by Driver 2.5 Next Chapter", " Chapter 2 Introduction to Formula 1 What is Formula 1? The word formula refers to a particular ruleset that teams’ cars must conform to, and 1 designates that this classification is the highest level of motorsport competition. So, Formula 1 racing is the highest class of international open-wheel single-seater formula racing sanctioned by the Fédération Internationale de l’Automobile (FIA). A Formula 1 season includes a series of Grands Prix (races) that take place across the globe on various types of tracks, ranging from closed public city streets to designated racing circuits and modern manufactured street-circuits. Like most professional sports, Formula 1 is also big business! Thanks in part to the popular Netflix series Drive to Survive, Formula 1 has experienced a recent explosion of popularity in the United States (I am one of these recent converts!). Along with this uptick in popularity, Formula 1 teams have also experienced a surge in valuation. As an initial example using ggplot, I will create a bar chart to visualize this trend in team valuation. Prior to creating the bar chart with the ggplot() function, I have to create the dataframe and then reshape it to a longer format using the pivot_longer() function. How to pivot data into a longer format in R pivot_longer() lengthens data, increasing the number of rows and decreasing the number of columns. To widen the data, use the pivot_wider() function, which is the inverse transformation. For more information on pivot_longer() and pivot_wider(), visit this link: https://tidyr.tidyverse.org/reference/pivot_longer.html The following figure was created using data compiled by https://thesportsdaily.com/, and originally retrieved from Sportico and Statista: library(tidyverse) valuation_table &lt;- data.frame(`F1 Team` = c(&#39;Ferrari&#39;, &#39;Mercedes&#39;, &#39;Red Bull&#39;, &#39;McLaren&#39;, &#39;Aston Martin&#39;, &#39;Alpine&#39;, &#39;Alpha Tauri&#39;, &#39;Alfa Romeo&#39;, &#39;Williams&#39;, &#39;Haas&#39;), `2018 Valuation` = c(1350, 1015, 640, 620, NA, 430, 200, 105, 400, 115), `2023 Valuation` = c(3130, 2700, 2420, 1560, 1140, 1080, 905, 815, 795, 710)) %&gt;% pivot_longer(- F1.Team, names_to = &#39;year&#39;, values_to = &#39;valuation&#39;) %&gt;% mutate(year = str_remove(year, &#39;X&#39;), year = str_remove(year, &#39;.Valuation&#39;)) valuation_table %&gt;% ggplot(aes(y = fct_reorder(F1.Team, valuation), x = valuation, fill = year)) + geom_histogram(stat = &#39;identity&#39;, position = &#39;dodge&#39;, alpha = 0.5) + labs(title = &#39;Increase in Formula 1 Team Valuations&#39;, subtitle = &#39;2018 vs 2023&#39;, y = &#39;F1 Team&#39;, x = &#39;Valuation (Millions)&#39;, caption = &#39;data sources: \\n https://thesportsdaily.com/news/f1-teams-valuations-increase-by-an-average-of-280-over-last-five-years/ \\n https://www.sportico.com/leagues/motorsports/2023/richest-formula-1-teams-most-valuable-1234727541/ \\n https://www.sportico.com/leagues/motorsports/2023/richest-formula-1-teams-most-valuable-1234727541/&#39;) + theme_bw() + scale_x_continuous(labels = scales::dollar_format()) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), plot.caption = element_text(hjust = 0, size = 9)) How to create a bar chart in R To create the figure above, I used the bar geometry, geom_bar(), in ggplot2. I often use bar charts to plot counts or sums by some grouping variable. I used two arguments within geom_bar(): stat = 'identity' ensures that ggplot uses the dollar sum as listed in the column, while position = 'dodge' preserves the position along the x-axis. For more information on dodging in ggplot2, visit this link: https://ggplot2.tidyverse.org/reference/position_dodge.html Each Formula 1 team has two cars driven by two drivers. Drivers and teams are awarded points based on their finishing position in each Grand Prix. The final tally of points scored will decide two competitions that are taking place during a Formula 1 season: (1) a driver’s championship, and (2) a constructors championship (team championship). Currently, Formula 1 has 10 teams, 20 drivers, and awards the following points for each finishing position. Additionally, 1 point is awarded to the driver with the fastest lap during a Grand Prix. library(tidyverse) library(gt) library(knitr) points_dataframe &lt;- data.frame(Placing = c(&#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;, &#39;5&#39;, &#39;6&#39;, &#39;7&#39;, &#39;8&#39;, &#39;9&#39;, &#39;10&#39;, &#39;11 - 20&#39;), Pts = c(25, 18, 15, 12, 10, 8, 6, 4, 2, 1, 0)) gt(points_dataframe) %&gt;% cols_align(align = c(&quot;center&quot;), columns = 2) %&gt;% tab_options(column_labels.font.weight = &quot;bold&quot;) #ycsgbccovu table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #ycsgbccovu thead, #ycsgbccovu tbody, #ycsgbccovu tfoot, #ycsgbccovu tr, #ycsgbccovu td, #ycsgbccovu th { border-style: none; } #ycsgbccovu p { margin: 0; padding: 0; } #ycsgbccovu .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #ycsgbccovu .gt_caption { padding-top: 4px; padding-bottom: 4px; } #ycsgbccovu .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #ycsgbccovu .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #ycsgbccovu .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #ycsgbccovu .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #ycsgbccovu .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #ycsgbccovu .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: bold; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #ycsgbccovu .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: bold; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #ycsgbccovu .gt_column_spanner_outer:first-child { padding-left: 0; } #ycsgbccovu .gt_column_spanner_outer:last-child { padding-right: 0; } #ycsgbccovu .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #ycsgbccovu .gt_spanner_row { border-bottom-style: hidden; } #ycsgbccovu .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #ycsgbccovu .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #ycsgbccovu .gt_from_md > :first-child { margin-top: 0; } #ycsgbccovu .gt_from_md > :last-child { margin-bottom: 0; } #ycsgbccovu .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #ycsgbccovu .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #ycsgbccovu .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #ycsgbccovu .gt_row_group_first td { border-top-width: 2px; } #ycsgbccovu .gt_row_group_first th { border-top-width: 2px; } #ycsgbccovu .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #ycsgbccovu .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #ycsgbccovu .gt_first_summary_row.thick { border-top-width: 2px; } #ycsgbccovu .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #ycsgbccovu .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #ycsgbccovu .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #ycsgbccovu .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #ycsgbccovu .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #ycsgbccovu .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #ycsgbccovu .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #ycsgbccovu .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #ycsgbccovu .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #ycsgbccovu .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #ycsgbccovu .gt_left { text-align: left; } #ycsgbccovu .gt_center { text-align: center; } #ycsgbccovu .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #ycsgbccovu .gt_font_normal { font-weight: normal; } #ycsgbccovu .gt_font_bold { font-weight: bold; } #ycsgbccovu .gt_font_italic { font-style: italic; } #ycsgbccovu .gt_super { font-size: 65%; } #ycsgbccovu .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #ycsgbccovu .gt_asterisk { font-size: 100%; vertical-align: 0; } #ycsgbccovu .gt_indent_1 { text-indent: 5px; } #ycsgbccovu .gt_indent_2 { text-indent: 10px; } #ycsgbccovu .gt_indent_3 { text-indent: 15px; } #ycsgbccovu .gt_indent_4 { text-indent: 20px; } #ycsgbccovu .gt_indent_5 { text-indent: 25px; } Placing Pts 1 25 2 18 3 15 4 12 5 10 6 8 7 6 8 4 9 2 10 1 11 - 20 0 How to create a table in R To create the table above, I simply pipe (i.e. %&gt;%) a manually-created dataframe to the gt() function (i.e. df %&gt;% gt()). The gt() function is found in the gt package. The gt package is the best way (in my opinion!) to create beautiful tables using the R programming language. gt functions work great with dataframes and tibbles, and can easily become an extension of yur typical tidy workflow. This example above is an absolute bare-bones examples of a gt table. Another way to create this type of table would be to use the kable() function from the knitr package. The kable() function is a very simple table generator in R. For more information about the gt package, visit this link: https://gt.rstudio.com/ This guide provides lots of information about styling aesthetics of a table, including table headers and footers, column labels, and much, much more! 2.1 Data In this book, I will limit the data to include only years 2014 through 2023. Why start in 2014? 2014 marks the beginning of the turbo-hybrid era in Formula 1. In 2014, F1 made perhaps the most significant rule change in its’ history by replacing naturally aspirated V8 engines with 1.6 liter turbocharged V6 engines. In fact, because F1 engines now include an electric component, they are actually referred to as power units. This new set of regulations seemed like an ideal place to start my analyses. At times in this book, I will note changes occurring after the 2021 season. I do this because 2022 marked another year of significant change in the regulations, this time aerodynamically. After a forty-year ban, ground effect returned to Formula 1. The goal for a Formula 1 engineering and design team is to optimize ground effect so that the bottom of the car generates low pressure, which will suck the car to the track. This new set of regulations in 2022 disrupted Mercedes’ period of dominance from 2014 to 2021. 2.2 A Formula 1 Weekend A Formula One Grand Prix is actually not just a single race, but rather a sporting event spanning three days, typically from Friday to Sunday. Beginning on Friday, there is typically two practices (FP1 and FP2), followed by a third practice (FP3) and a qualifying session on Saturday. Practice sessions provide teams an opportunity to practice on the circuit, and experiment with the setup of the car. Qualifying determines the grid for the race on Sunday. Currently, there are three heats in qualifying. All cars compete in the first heat (Q1), and the top 15 fastest times advance to the 2nd heat (Q2). From there, the top 10 fastest times during heat 2 advance to Q3 (heat 3). Qualifying times determine the starting grid for the Grand Prix. The fastest qualifying times start the race at the front of the grid, and the slowest start at the back. If a driver incurs a penalty during qualifying, the starting grid will be adjusted accordingly. The race takes place on Sunday, and the top three placed drivers take their places upon a podium. A podium is a particularly noteworthy career achievement. 2.3 Practice Sessions Currently, there are three free practice sessions (often abbreviated to FP1, FP2, and FP3) that are held before the race. The first (FP1) is held on Friday morning, the second (FP2) on Friday afternoon, and the third session (FP3) is held on Saturday morning. Since 2021, practice sessions last for one hour, but previously Friday sessions were 90 minutes long. Typically, teams will use each practice session for a different purpose. For instance, FP1 is often used to test the car and ensure it is working as expected, while also collecting information on the track and car setup. FP2 is often used for additional reconnaissance and testing of the car’s performance on long runs. FP3 is most commonly used to understand the car’s speed over lap (i.e. qualifying pace). When I first became interested in Formula 1, I had tons of questions about the cars, racing, and structure of the Grand Prix weekend. While there are countless resources available to learn about Formula (shout out to r/formula1), I particularly enjoy learning by exploring data. Luckily, you can find practice, qualifying, and Grand Prix data on https://formula1.com/. In this chapter, we will be focusing on practice data. To make that data a little more accessible, feel free to use the practice_session_scraper() function from my drs package. To scrape FP1 data for 2014 through 2023, use the following code: # Scrape FP1 data fp1_2023 &lt;- practice_session_scraper(2023, 1) fp1_2022 &lt;- practice_session_scraper(2022, 1) fp1_2021 &lt;- practice_session_scraper(2021, 1) fp1_2020 &lt;- practice_session_scraper(2020, 1) fp1_2019 &lt;- practice_session_scraper(2019, 1) fp1_2018 &lt;- practice_session_scraper(2018, 1) fp1_2017 &lt;- practice_session_scraper(2017, 1) fp1_2016 &lt;- practice_session_scraper(2016, 1) fp1_2015 &lt;- practice_session_scraper(2015, 1) fp1_2014 &lt;- practice_session_scraper(2014, 1) To scrape FP2 data for the same time period, use this code: # Scrape FP2 data fp2_2023 &lt;- practice_session_scraper(2023, 2) fp2_2022 &lt;- practice_session_scraper(2022, 2) fp2_2021 &lt;- practice_session_scraper(2021, 2) fp2_2020 &lt;- practice_session_scraper(2020, 2) fp2_2019 &lt;- practice_session_scraper(2019, 2) fp2_2018 &lt;- practice_session_scraper(2018, 2) fp2_2017 &lt;- practice_session_scraper(2017, 2) fp2_2016 &lt;- practice_session_scraper(2016, 2) fp2_2015 &lt;- practice_session_scraper(2015, 2) fp2_2014 &lt;- practice_session_scraper(2014, 2) And finally, to scrape FP3 data, use this: # Scrape FP3 data fp3_2023 &lt;- practice_session_scraper(2023, 3) fp3_2022 &lt;- practice_session_scraper(2022, 3) fp3_2021 &lt;- practice_session_scraper(2021, 3) fp3_2020 &lt;- practice_session_scraper(2020, 3) fp3_2019 &lt;- practice_session_scraper(2019, 3) fp3_2018 &lt;- practice_session_scraper(2018, 3) fp3_2017 &lt;- practice_session_scraper(2017, 3) fp3_2016 &lt;- practice_session_scraper(2016, 3) fp3_2015 &lt;- practice_session_scraper(2015, 3) fp3_2014 &lt;- practice_session_scraper(2014, 3) Now that the data is scraped from www.formula1.com, I need to combine the dataframes together into one dataframe. I will use the rbind() function to row-bind data for each practice session. How to bind data in R The cbind() and rbind() functions combine by columns or rows, respectively. For more information on these two functions, visit this link: https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/cbind #Combine all practice data practice_times &lt;- rbind(fp3_2023, fp3_2022, fp3_2021, fp3_2020, fp3_2019, fp3_2018, fp3_2017, fp3_2016, fp3_2015, fp3_2014) %&gt;% left_join(rbind(fp2_2023, fp2_2022, fp2_2021, fp2_2020, fp2_2019, fp2_2018, fp2_2017, fp2_2016, fp2_2015, fp2_2014) %&gt;% dplyr::select(&#39;Driver&#39;, &#39;Race&#39;, &#39;Year&#39;, &#39;Time&#39;, &#39;Time_secs&#39;), by = c(&#39;Driver&#39;, &#39;Race&#39;, &#39;Year&#39;), suffix = c(&quot;_3&quot;, &quot;_2&quot;)) %&gt;% left_join(rbind(fp1_2023, fp1_2022, fp1_2021, fp1_2020, fp1_2019, fp1_2018, fp1_2017, fp1_2016, fp1_2015, fp1_2014) %&gt;% dplyr::select(&#39;Driver&#39;, &#39;Race&#39;, &#39;Year&#39;, &#39;Time&#39;, &#39;Time_secs&#39;), by = c(&#39;Driver&#39;, &#39;Race&#39;, &#39;Year&#39;)) %&gt;% rename(Time_1 = Time, Time_secs_1 = Time_secs) %&gt;% relocate(Time_3, .after = Year) While learning about the sport, one of my early questions was: Do cars get faster with each practice session? To begin trying to answer this question, I will make use of the practice session data. For this example, I will focus on 2023 practice data, and use left_join() from the dplyr package to merge all practice sessions together for the 2023 season. # Pull 2023 data # Scrape FP1 data fp1_2023 &lt;- practice_session_scraper(2023, 1) # Scrape FP2 data fp2_2023 &lt;- practice_session_scraper(2023, 2) # Scrape FP2 data fp3_2023 &lt;- practice_session_scraper(2023, 3) # Merge Practices times for 2022 practice_times_2023 &lt;- fp3_2023 %&gt;% left_join(fp2_2023 %&gt;% dplyr::select(&#39;Driver&#39;, &#39;Race&#39;, &#39;Year&#39;, &#39;Time&#39;, &#39;Time_secs&#39;), by = c(&#39;Driver&#39;, &#39;Race&#39;, &#39;Year&#39;), suffix = c(&quot;_3&quot;, &quot;_2&quot;)) %&gt;% left_join(fp1_2023 %&gt;% dplyr::select(&#39;Driver&#39;, &#39;Race&#39;, &#39;Year&#39;, &#39;Time&#39;, &#39;Time_secs&#39;), by = c(&#39;Driver&#39;, &#39;Race&#39;, &#39;Year&#39;)) %&gt;% rename(Time_1 = Time, Time_secs_1 = Time_secs) %&gt;% relocate(Time_3, .after = Year) How to join data in R Joining data is pretty simple using functions from the dplyr package. In dplyr, you will be mutating joins, which adds columns from y to x, matching observations based on the keys. There are four mutating joins: inner_join() left_join() right_join() full_join() For more information on mutating joins in dplyr, visit this link: https://dplyr.tidyverse.org/reference/mutate-joins.html Here’s another tip on reordering columns: How to relocate a column of a dataframe in R Notice that in the final line of the above code, I used the relocate() function to reorder a column. This is not necessary, but can help keep your dataframe more organized. As an example, you could use the following code to move column A after column C in a dataframe: df %&gt;% relocate(a, .after = c) For more information on the relocate() function in dplyr, visit this link: https://dplyr.tidyverse.org/reference/relocate.html I used the following code to plot the average times by practice session in 2023. practice_times_2023 %&gt;% dplyr::select(Time_secs_3, Time_secs_2, Time_secs_1) %&gt;% pivot_longer(c(Time_secs_3, Time_secs_2, Time_secs_1), names_to = &#39;practice_session&#39;, values_to = &#39;time&#39;) %&gt;% mutate(practice_session = str_remove(practice_session, &#39;Time_secs_&#39;)) %&gt;% ggplot(aes(practice_session, time)) + stat_summary(fun.y = mean, geom = &quot;point&quot;, pch = 21, col = &#39;black&#39;, fill = &#39;red&#39;, alpha = 0.5, size = 3) + theme_bw() + labs(y = &#39;Best Lap Time (secs)&#39;, x = &#39;Practice Session&#39;, title = &#39;Average Best Lap Time by Practice Session&#39;, subtitle = &#39;All Grands Prix: 2023&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) How to create a categorical scatter plot with a computed mean in R If there are many observations per group, you can use the stat_summary() function to calculate a grouped mean of the variable plotted along the y-axis. Specifically, the argument fun.y = mean specifies that you want to calculate the mean of the y-axis variable. If you wish to only plot the mean, include the geom = \"point\" argument. For more information on stat_summary() in ggplot2, visit this link: https://ggplot2.tidyverse.org/reference/stat_summary.html When pooling all teams, drivers, and circuits together, it looks like the average time during P1 is slower than both P2 and P3. P3’s average time is slightly slower than P2. But, these times will obviously vary considerably by year, Grand Prix, team, and driver. How much variability surrounds the average times in the figure above? I’ll re-plot that data but also include a standard deviation bar that describes the mean ± 1 standard deviation (SD). practice_times_2023 %&gt;% dplyr::select(Time_secs_3, Time_secs_2, Time_secs_1) %&gt;% pivot_longer(c(Time_secs_3, Time_secs_2, Time_secs_1), names_to = &#39;practice_session&#39;, values_to = &#39;time&#39;) %&gt;% mutate(practice_session = str_remove(practice_session, &#39;Time_secs_&#39;)) %&gt;% ggplot(aes(practice_session, time)) + stat_summary(fun.y = mean, fun.ymin = function(x) mean(x) - sd(x), fun.ymax = function(x) mean(x) + sd(x), geom = &quot;pointrange&quot;, pch = 21, col = &#39;black&#39;, fill = &#39;red&#39;, alpha = 0.5, size = 1) + theme_bw() + labs(y = &#39;Best Lap Time (secs)&#39;, x = &#39;Practice Session&#39;, title = &#39;Average Best Lap Time \\u00b1 SD by Practice Session&#39;, subtitle = &#39;All Grands Prix: 2023&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) How to create a categorical scatter plot with a computed mean ± standard deviation in R Again, you can use the stat_summary() function to calculate a grouped mean ± standard deviation of the variable plotted along the y-axis. To plot a computed point-interval or point-range, specify geom = \"pointrange\". Additionally, you’ll need to include arguments for the calculated minimum and maximum of that range. In the example above, I plot the mean ± 1 standard deviation using the arguments fun.ymin = function(x) mean(x) - sd(x) and fun.ymax = function(x) mean(x) + sd(x). For more information on stat_summary() in ggplot2, visit this link: https://ggplot2.tidyverse.org/reference/stat_summary.html Despite the differences in average times between practice sessions, the variability of times within each practice sessions far exceeds the differences between the sessions. This seems obvious. Circuits are very different, not to mention all of the other sources of variability including yearly, team, and differences by driver. Approximately 68% of the data will fall within 1 standard deviation of the mean. So, the entire distribution of practice times extends beyond the intervals in the figure above. If we’d like to know how the entire distributions of times compare, we can include the individual times beneath the mean ± SD. practice_times_2023 %&gt;% dplyr::select(Time_secs_3, Time_secs_2, Time_secs_1) %&gt;% pivot_longer(c(Time_secs_3, Time_secs_2, Time_secs_1), names_to = &#39;practice_session&#39;, values_to = &#39;time&#39;) %&gt;% mutate(practice_session = str_remove(practice_session, &#39;Time_secs_&#39;)) %&gt;% ggplot(aes(practice_session, time)) + geom_point(position = position_jitter(w= 0.3, h = 0), alpha = 0.25, col = &#39;grey&#39;) + theme_bw() + stat_summary(fun.y = mean, fun.ymin = function(x) mean(x) - sd(x), fun.ymax = function(x) mean(x) + sd(x), geom = &quot;pointrange&quot;, col = &#39;red&#39;, linewidth = 2, size = 1, alpha = 0.5) + labs(y = &#39;Best Lap Time (secs)&#39;, x = &#39;Practice Session&#39;, title = &#39;Average Best Lap Time \\u00b1 SD by Practice Session&#39;, subtitle = &#39;includes each time for every Grand Prix: 2023&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) How to create a strip plot in R In a 2015 paper, Tracey Weissgerber proposed packing as much information as possible into a figure. One of her recommendations for plotting a continuous variable across groups is the strip plot. Here is a link to Tracy’s fantastic paper: https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002128 To create a strip plot in ggplot, I simply use geom_point() and include the position = position_jitter() argument. I’ll mention more about geom_point() shortly, but visit this link for more information on point geometries in ggplot2: https://ggplot2.tidyverse.org/reference/geom_point.html And, here’s a link to background information on jittering: https://ggplot2.tidyverse.org/reference/position_jitter.html Much of this variability can actually be explained away. The largest source of variability among these times is caused by differences in circuits. For instance, lap times at the United States Grand Prix (Circuit of the Americas) are much longer than those at the Austrian Grand Prix. In the figure below, I plot the average P1 lap time for each Grand Prix. Clearly, lap times vary considerably by circuit! practice_times_2023 %&gt;% group_by(Race) %&gt;% summarize(mean = mean(Time_secs_1, na.rm = T)) %&gt;% ggplot(aes(mean, y = fct_reorder(Race, mean))) + geom_point() + theme_bw() + labs(x = &#39;Average Best Lap Time (secs)&#39;, y = &#39;Grand Prix&#39;, title = &#39;Average Best Lap Time&#39;, subtitle = &#39;Practice 1 - All Grands Prix: 2023&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) How to create a categorical scatter plot in R I used the simple point geometry (geom_point()) in ggplot2 to create the plot above. geom_point() is most commonly used to create scatterplots that display the relationship between two continuous variables. However, you can also use them to plot a continuous variables across groups. For more information on point geometries in ggplot2, visit this link: https://ggplot2.tidyverse.org/reference/geom_point.html We can pretty this figure up a bit by adding alternating row shading. ggplot does not have a way to do this directly, so I will use the geom_hline() function to do this. practice_times_2023 %&gt;% group_by(Race) %&gt;% summarize(mean = mean(Time_secs_1, na.rm = T)) %&gt;% ggplot(aes(mean, y = fct_reorder(Race, mean))) + geom_hline(yintercept = c(&#39;belgium&#39;, &#39;azerbaijan&#39;, &#39;united-states&#39;, &#39;singapore&#39;, &#39;miami&#39;, &#39;great-britain&#39;, &#39;abu-dhabi&#39;, &#39;canada&#39;, &#39;australia&#39;, &#39;monaco&#39;, &#39;brazil&#39;), col = &#39;grey&#39;, size = 5, alpha = 0.2) + geom_point() + theme_bw() + labs(x = &#39;Average Best Lap Time (secs)&#39;, y = &#39;Grand Prix&#39;, title = &#39;Average Best Lap Time&#39;, subtitle = &#39;Practice 1 - All Grands Prix: 2023&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) Building on this idea, let’s explore how variability for a single Grand Prix (Bahrain) compare to the variability across all Grands Prix. Below, I’ll calculate the standard deviation for Bahrian only and then all Grands Prix pooled together. Notice that the standard deviation for the Bahrain Grand Prix is about an order of magnitude smaller. # All Grands Prix practice_times_2023 %&gt;% pivot_longer(c(Time_secs_3, Time_secs_2, Time_secs_1), names_to = &#39;practice_session&#39;, values_to = &#39;time&#39;) %&gt;% mutate(practice_session = str_remove(practice_session, &#39;Time_secs_&#39;)) %&gt;% group_by(practice_session) %&gt;% summarize(sd = round(sd(time, na.rm = T), 2)) %&gt;% gt() %&gt;% tab_options(column_labels.font.weight = &quot;bold&quot;) %&gt;% tab_style( style = cell_text(align = &quot;center&quot;), locations = cells_body(columns = c(practice_session, sd))) %&gt;% cols_align(align = &quot;center&quot;, columns = c(practice_session, sd)) %&gt;% tab_header( title = md(&quot;**data from all Grands Prix**&quot;), subtitle = md(&quot;2023&quot;) ) #fibxyizacv table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #fibxyizacv thead, #fibxyizacv tbody, #fibxyizacv tfoot, #fibxyizacv tr, #fibxyizacv td, #fibxyizacv th { border-style: none; } #fibxyizacv p { margin: 0; padding: 0; } #fibxyizacv .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #fibxyizacv .gt_caption { padding-top: 4px; padding-bottom: 4px; } #fibxyizacv .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #fibxyizacv .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #fibxyizacv .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #fibxyizacv .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #fibxyizacv .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #fibxyizacv .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: bold; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #fibxyizacv .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: bold; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #fibxyizacv .gt_column_spanner_outer:first-child { padding-left: 0; } #fibxyizacv .gt_column_spanner_outer:last-child { padding-right: 0; } #fibxyizacv .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #fibxyizacv .gt_spanner_row { border-bottom-style: hidden; } #fibxyizacv .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #fibxyizacv .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #fibxyizacv .gt_from_md > :first-child { margin-top: 0; } #fibxyizacv .gt_from_md > :last-child { margin-bottom: 0; } #fibxyizacv .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #fibxyizacv .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #fibxyizacv .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #fibxyizacv .gt_row_group_first td { border-top-width: 2px; } #fibxyizacv .gt_row_group_first th { border-top-width: 2px; } #fibxyizacv .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #fibxyizacv .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #fibxyizacv .gt_first_summary_row.thick { border-top-width: 2px; } #fibxyizacv .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #fibxyizacv .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #fibxyizacv .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #fibxyizacv .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #fibxyizacv .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #fibxyizacv .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #fibxyizacv .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #fibxyizacv .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #fibxyizacv .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #fibxyizacv .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #fibxyizacv .gt_left { text-align: left; } #fibxyizacv .gt_center { text-align: center; } #fibxyizacv .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #fibxyizacv .gt_font_normal { font-weight: normal; } #fibxyizacv .gt_font_bold { font-weight: bold; } #fibxyizacv .gt_font_italic { font-style: italic; } #fibxyizacv .gt_super { font-size: 65%; } #fibxyizacv .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #fibxyizacv .gt_asterisk { font-size: 100%; vertical-align: 0; } #fibxyizacv .gt_indent_1 { text-indent: 5px; } #fibxyizacv .gt_indent_2 { text-indent: 10px; } #fibxyizacv .gt_indent_3 { text-indent: 15px; } #fibxyizacv .gt_indent_4 { text-indent: 20px; } #fibxyizacv .gt_indent_5 { text-indent: 25px; } data from all Grands Prix 2023 practice_session sd 1 13.28 2 7.84 3 11.99 # Bahrain Only practice_times_2023 %&gt;% filter(Race %in% c(&#39;bahrain&#39;)) %&gt;% pivot_longer(c(Time_secs_3, Time_secs_2, Time_secs_1), names_to = &#39;practice_session&#39;, values_to = &#39;time&#39;) %&gt;% mutate(practice_session = str_remove(practice_session, &#39;Time_secs_&#39;)) %&gt;% group_by(practice_session) %&gt;% summarize(sd = round(sd(time, na.rm = T), 2)) %&gt;% gt() %&gt;% tab_options(column_labels.font.weight = &quot;bold&quot;) %&gt;% tab_style( style = cell_text(align = &quot;center&quot;), locations = cells_body(columns = c(practice_session, sd))) %&gt;% cols_align(align = &quot;center&quot;, columns = c(practice_session, sd)) %&gt;% tab_header( title = md(&quot;**data from the Bahrain Grand Prix only**&quot;), subtitle = md(&quot;2023&quot;) ) #mhilcynlvi table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #mhilcynlvi thead, #mhilcynlvi tbody, #mhilcynlvi tfoot, #mhilcynlvi tr, #mhilcynlvi td, #mhilcynlvi th { border-style: none; } #mhilcynlvi p { margin: 0; padding: 0; } #mhilcynlvi .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #mhilcynlvi .gt_caption { padding-top: 4px; padding-bottom: 4px; } #mhilcynlvi .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #mhilcynlvi .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #mhilcynlvi .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #mhilcynlvi .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #mhilcynlvi .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #mhilcynlvi .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: bold; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #mhilcynlvi .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: bold; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #mhilcynlvi .gt_column_spanner_outer:first-child { padding-left: 0; } #mhilcynlvi .gt_column_spanner_outer:last-child { padding-right: 0; } #mhilcynlvi .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #mhilcynlvi .gt_spanner_row { border-bottom-style: hidden; } #mhilcynlvi .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #mhilcynlvi .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #mhilcynlvi .gt_from_md > :first-child { margin-top: 0; } #mhilcynlvi .gt_from_md > :last-child { margin-bottom: 0; } #mhilcynlvi .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #mhilcynlvi .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #mhilcynlvi .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #mhilcynlvi .gt_row_group_first td { border-top-width: 2px; } #mhilcynlvi .gt_row_group_first th { border-top-width: 2px; } #mhilcynlvi .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #mhilcynlvi .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #mhilcynlvi .gt_first_summary_row.thick { border-top-width: 2px; } #mhilcynlvi .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #mhilcynlvi .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #mhilcynlvi .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #mhilcynlvi .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #mhilcynlvi .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #mhilcynlvi .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #mhilcynlvi .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #mhilcynlvi .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #mhilcynlvi .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #mhilcynlvi .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #mhilcynlvi .gt_left { text-align: left; } #mhilcynlvi .gt_center { text-align: center; } #mhilcynlvi .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #mhilcynlvi .gt_font_normal { font-weight: normal; } #mhilcynlvi .gt_font_bold { font-weight: bold; } #mhilcynlvi .gt_font_italic { font-style: italic; } #mhilcynlvi .gt_super { font-size: 65%; } #mhilcynlvi .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #mhilcynlvi .gt_asterisk { font-size: 100%; vertical-align: 0; } #mhilcynlvi .gt_indent_1 { text-indent: 5px; } #mhilcynlvi .gt_indent_2 { text-indent: 10px; } #mhilcynlvi .gt_indent_3 { text-indent: 15px; } #mhilcynlvi .gt_indent_4 { text-indent: 20px; } #mhilcynlvi .gt_indent_5 { text-indent: 25px; } data from the Bahrain Grand Prix only 2023 practice_session sd 1 0.89 2 0.53 3 0.50 How to add a header to a table in R It is very easy to add a header to the tables above. To add a title and/or subtitle, simply pass the title and subtitle arguments to the tab_header() function. The table header is positioned just above the column labels in the gt table. To bold or italicize the title or subtitle in the header, gt actually allows the flexibility to use Markdown or HTML formatting. To use Markdown or HTML formatting, wrap your title and/or subtitle text in md() or html() functions, respectively. For more information, visit this link: https://gt.rstudio.com/reference/tab_header.html We should keep this in mind as we continue exploring the data. Below, I’ll plot the average practice times for the Bahrain Grand Prix, along with all of the underlying times (grey points). practice_times %&gt;% filter(Race %in% c(&#39;bahrain&#39;)) %&gt;% pivot_longer(c(Time_secs_3, Time_secs_2, Time_secs_1), names_to = &#39;practice_session&#39;, values_to = &#39;time&#39;) %&gt;% mutate(practice_session = str_remove(practice_session, &#39;Time_secs_&#39;)) %&gt;% ggplot(aes(practice_session, time)) + geom_point(position = position_jitter(w= 0.3, h = 0), alpha = 0.5, col = &#39;grey&#39;) + theme_bw() + stat_summary(fun.y = mean, fun.ymin = function(x) mean(x) - sd(x), fun.ymax = function(x) mean(x) + sd(x), geom = &quot;pointrange&quot;, col = &#39;red&#39;, linewidth = 2, size = 1, alpha = 0.5) + labs(y = &#39;Best Lap Time (secs)&#39;, x = &#39;Practice Session&#39;, title = &#39;Average Best Lap Time \\u00b1 SD by Practice Session&#39;, subtitle = &#39;includes each time for the Bahrain Grand Prix: 2014 - 2023&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) Judging by this figure, P1 times appear to be slower than times during P2 and P3 in Bahrain. And while the average for P2 is slightly faster than P3, the fastest single lap times were set during P3. Interesting. Let’s dig depper into the Bahrain data! If we plot practice times by year, but group the times by practice session, we can see that times steadily improve each year until 2021, at which point they begin to get a bit slower. practice_times %&gt;% filter(Race %in% c(&#39;bahrain&#39;)) %&gt;% pivot_longer(c(Time_secs_3, Time_secs_2, Time_secs_1), names_to = &#39;practice_session&#39;, values_to = &#39;time&#39;) %&gt;% mutate(practice_session = str_remove(practice_session, &#39;Time_secs_&#39;), Year = factor(Year)) %&gt;% ggplot(aes(Year, time, group = practice_session, col = practice_session)) + theme_bw() + stat_summary(fun.y = mean, fun.ymin = function(x) mean(x) - sd(x), fun.ymax = function(x) mean(x) + sd(x), geom = &quot;pointrange&quot;, linewidth = 1, size = 0.5, alpha = 0.5, position = position_dodge(w = 0.25)) + stat_summary(fun.y = mean, geom = &quot;line&quot;, linewidth = 1, size = 0.5, alpha = 0.5, position = position_dodge(w = 0.25)) + labs(x = &#39;Year&#39;, y = &#39;Best Lap Time (secs)&#39;, col = &#39;Practice Session&#39;, title = &#39;Average Best Lap Time \\u00b1 SD by Year and Practice Session&#39;, subtitle = &#39;Bahrain Grand Prix: 2014 - 2023&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) Pretty cool! How to create a line plot with a computed mean ± standard deviation in R Like the previous examples with points and point-ranges, you can also use the stat_summary() function to create a line plot using grouped means ± standard deviation. Simply pass geom = \"line\" as an argument to the stat_summary() function. In the figure above, I wanted to plot grouped lines, so I also passed group = practice_session and col = practice_session as arguments to the original ggplot() call. In addition to these summary lines, I also included the point-ranges using the same approach as previous figures (i.e. a separate stat_summary() call using the geom = \"pointrange\" argument.). For more information on stat_summary() in ggplot2, visit this link: https://ggplot2.tidyverse.org/reference/stat_summary.html Let’s expand a bit, and look at a few more circuits. Knowing that times vary by Grand Prix and year, let’s take a deeper look at differences in times within a single year (2022) and for one continent (North America). Below, I’ll plot the average practice times across North American Grands Prix for 2022 only. practice_times %&gt;% filter(Year == 2022) %&gt;% pivot_longer(c(Time_secs_3, Time_secs_2, Time_secs_1), names_to = &#39;practice_session&#39;, values_to = &#39;time&#39;) %&gt;% mutate(practice_session = str_remove(practice_session, &#39;Time_secs_&#39;), Year = factor(Year)) %&gt;% filter(Race %in% c(&#39;canada&#39;, &#39;miami&#39;, &#39;united-states&#39;, &#39;mexico&#39;, &#39;brazil&#39;)) %&gt;% ggplot(aes(practice_session, time, group = Race, col = Race)) + theme_bw() + stat_summary(fun.y = mean, fun.ymin = function(x) mean(x) - sd(x), fun.ymax = function(x) mean(x) + sd(x), geom = &quot;pointrange&quot;, linewidth = 1, size = 1, alpha = 0.5) + stat_summary(fun.y = mean, geom = &quot;line&quot;, linewidth = 1, size = 1, alpha = 0.5) + labs(y = &#39;Best Lap Time (secs)&#39;, x = &#39;Practice Session&#39;, col = &#39;Grand Prix&#39;, title = &#39;Average Best Lap Time \\u00b1 SD by Practice Session and Grand Prix&#39;, subtitle = &#39;North America: 2022&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) + scale_color_viridis_d(option = &#39;plasma&#39;) How to use colorblind-friendly palettes in R As someone with color blindness, I find the use of colorblind-friendly palettes very helpful! I often rely on the viridis color scales for ggplot. The viridis scales provides various color mappings that are designed to be perceived by viewers with common forms of color blindness. For a continuous color mapping, use scale_color_viridis_c(). For a discrete color mapping, use scale_color_viridis_d(). For more information about viridis, visit this link: https://ggplot2.tidyverse.org/reference/scale_viridis.html For more context about perceptually uniform color scales, see this link: https://bids.github.io/colormap/. Interestingly, there doesn’t seem to be a definitive pattern in practice times across these four North American races in 2022. Of note, some of these times are impacted by rain (i.e. Canada). Let’s compare this to four European races, where there is a fiarly clear decrease in times from P1 to P3. practice_times %&gt;% filter(Year == 2022) %&gt;% pivot_longer(c(Time_secs_3, Time_secs_2, Time_secs_1), names_to = &#39;practice_session&#39;, values_to = &#39;time&#39;) %&gt;% mutate(practice_session = str_remove(practice_session, &#39;Time_secs_&#39;), Year = factor(Year)) %&gt;% filter(Race %in% c(&#39;france&#39;, &#39;monza&#39;, &#39;great-britain&#39;, &#39;netherlands&#39;)) %&gt;% ggplot(aes(practice_session, time, group = Race, col = Race)) + theme_bw() + stat_summary(fun.y = mean, fun.ymin = function(x) mean(x) - sd(x), fun.ymax = function(x) mean(x) + sd(x), geom = &quot;pointrange&quot;, linewidth = 1, size = 1, alpha = 0.5) + stat_summary(fun.y = mean, geom = &quot;line&quot;, linewidth = 1, size = 1, alpha = 0.5) + labs(y = &#39;Best Lap Time (secs)&#39;, x = &#39;Practice Session&#39;, col = &#39;Grand Prix&#39;, title = &#39;Average Best Lap Time \\u00b1 SD by Practice Session and Grand Prix&#39;, subtitle = &#39;European Grands Prix: 2022&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) + scale_color_viridis_d(option = &#39;plasma&#39;) In the next section, we will begin exploring practice data by driver. 2.4 Practice Times by Driver Perhaps the most fun way to visualize data is to plot the times by driver. In the series of figures below, I plot standardized practice times for each driver. Each driver’s times are subtracted from the average session time for a given Grand Prix. For example, Lewis Hamilton’s best FP1 time at the 2014 Australian Grand Prix will be subtracted from the average best FP1 time for the grid. Hamilton was very quick in 2014, so naturally, his standardized times are much faster than the average over the entire year. We will first focus on the 2014 season, which Mercedes dominated. The two Mercedes drivers, Lewis Hamilton and Nico Rosberg, are clearly much faster than all other drivers during P2 and P3. During this 2014 season, Australia happened to be the circuit where Mercedes had the largest standardized gap in P3. practice_times %&gt;% filter(Year == 2014) %&gt;% pivot_longer(c(Time_secs_3, Time_secs_2, Time_secs_1), names_to = &#39;practice_session&#39;, values_to = &#39;time&#39;) %&gt;% mutate(practice_session = str_remove(practice_session, &#39;Time_secs_&#39;), Year = factor(Year)) %&gt;% group_by(Race, practice_session) %&gt;% mutate(track_mean = mean(time, na.rm = T), Time_std_track = time - track_mean) %&gt;% ungroup() %&gt;% group_by(Driver) %&gt;% mutate(driver_n = n(), mean = mean(Time_std_track, na.rm = T)) %&gt;% ungroup() %&gt;% dplyr::filter(driver_n &gt; 15) %&gt;% mutate(practice_session = paste0(&#39;free practice &#39;, practice_session)) %&gt;% ggplot(aes(Time_std_track, y = fct_reorder(Driver, desc(mean)))) + geom_point(position = position_jitter(w = 0, h = 0.1), alpha = 0.5) + theme_bw() + labs(x = &#39;Standardized Practice Time (secs)&#39;, y = &#39;Driver&#39;, title = &#39;Standardized Practice Time&#39;, subtitle = &#39;2014&#39;, caption = &#39;*Times are subtracted from the average time for each Grand Prix&#39;) + geom_vline(xintercept = 0, col = &#39;red&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), plot.caption = element_text(hjust = 0)) + facet_wrap(~ practice_session) + xlim(-5, 5) + geom_hline(yintercept = seq(22, 1, -2), col = &#39;grey&#39;, size = 5, alpha = 0.2) Jittering data points in R I’ll often use jittering to slightly separate data points that would otherwise be plotted on top of each other. Interestingly (and counterintuitively!), using a jitter argument in ggplot to add small amounts of random noise to a plot can actually make the plot easier to interpret. To add jittering to a plot, simply pass position = position_jitter() as an argument inside geom_point(). In the example above, I specify that I wish to add a small degree of jittering vertically and no jittering horizontally using position = position_jitter(w = 0, h = 0.1). https://ggplot2.tidyverse.org/reference/position_jitter.html Hamilton and Rosberg again dominated practice sessions 2 and 3 during the 2015 season. During 2015, Malaysia happened to be a particularly strong circuit for Mercedes. practice_times %&gt;% filter(Year == 2015) %&gt;% pivot_longer(c(Time_secs_3, Time_secs_2, Time_secs_1), names_to = &#39;practice_session&#39;, values_to = &#39;time&#39;) %&gt;% mutate(practice_session = str_remove(practice_session, &#39;Time_secs_&#39;), Year = factor(Year)) %&gt;% group_by(Race, practice_session) %&gt;% mutate(track_mean = mean(time, na.rm = T), Time_std_track = time - track_mean) %&gt;% ungroup() %&gt;% group_by(Driver) %&gt;% mutate(driver_n = n(), mean = mean(Time_std_track, na.rm = T)) %&gt;% ungroup() %&gt;% dplyr::filter(driver_n &gt; 15) %&gt;% mutate(practice_session = paste0(&#39;free practice &#39;, practice_session)) %&gt;% ggplot(aes(Time_std_track, y = fct_reorder(Driver, desc(mean)))) + geom_point(position = position_jitter(w = 0, h = 0.1), alpha = 0.5) + theme_bw() + labs(x = &#39;Standardized Practice Time (secs)&#39;, y = &#39;Driver&#39;, title = &#39;Standardized Practice Time&#39;, subtitle = &#39;2015&#39;, caption = &#39;*Times are subtracted from the average time for each Grand Prix&#39;) + geom_vline(xintercept = 0, col = &#39;red&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), plot.caption = element_text(hjust = 0)) + facet_wrap(~ practice_session) + xlim(-5, 5) + geom_hline(yintercept = seq(22, 1, -2), col = &#39;grey&#39;, size = 5, alpha = 0.2) Adding a reference line in R In the plot above, the solid red vertical line serves a reference line for a standardized time of 0 (i.e. the average time at a Grand Prix). Sometimes, it’s helpful to add a reference line to a figure. Within ggplot2, there are three common geometries that are used to add references lines to a plot: geom_hline() adds a horizontal reference line. Specify the yintercept. geom_vline() adds a vertical reference line. Specify the xintercept. geom_abline() adds a diagonal reference line. Specify the slope. For more information see this link: https://ggplot2.tidyverse.org/reference/geom_abline.html The Mercedes domination of practice sessions continued through 2016. However, by then, Ferrari had started chipping away at the gap to Mercedes. practice_times %&gt;% filter(Year == 2016) %&gt;% pivot_longer(c(Time_secs_3, Time_secs_2, Time_secs_1), names_to = &#39;practice_session&#39;, values_to = &#39;time&#39;) %&gt;% mutate(practice_session = str_remove(practice_session, &#39;Time_secs_&#39;), Year = factor(Year)) %&gt;% group_by(Race, practice_session) %&gt;% mutate(track_mean = mean(time, na.rm = T), Time_std_track = time - track_mean) %&gt;% ungroup() %&gt;% group_by(Driver) %&gt;% mutate(driver_n = n(), mean = mean(Time_std_track, na.rm = T)) %&gt;% ungroup() %&gt;% dplyr::filter(driver_n &gt; 15) %&gt;% mutate(practice_session = paste0(&#39;free practice &#39;, practice_session)) %&gt;% ggplot(aes(Time_std_track, y = fct_reorder(Driver, desc(mean)))) + geom_point(position = position_jitter(w = 0, h = 0.1), alpha = 0.5) + theme_bw() + labs(x = &#39;Standardized Practice Time (secs)&#39;, y = &#39;Driver&#39;, title = &#39;Standardized Practice Time&#39;, subtitle = &#39;2016&#39;, caption = &#39;*Times are subtracted from the average time for each Grand Prix&#39;) + geom_vline(xintercept = 0, col = &#39;red&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), plot.caption = element_text(hjust = 0)) + facet_wrap(~ practice_session) + xlim(-5, 5) + geom_hline(yintercept = seq(23, 1, -2), col = &#39;grey&#39;, size = 5, alpha = 0.2) Faceting Plots In the previous few figures, you will notice that the data is split into panes, or facets. Faceting splits a larger graph into two or more smaller graphs that are organized within a grid. Each facet within the grid will display the same style of graph for a designated group of the dataset. In the example above, I use the facet_wrap() function to split the graph into facets representing each practice session using the following function and argument: facet_wrap(~ practice_session) For more information on the facet_wrap() function, see this link: https://ggplot2.tidyverse.org/reference/facet_wrap.html By 2017, Ferrari had seemingly closed the gap to Mercedes. Sebastian Vettel led the driver’s championship for the first 12 rounds of the season, and we can see that Vettel’s practice times are comparable to those posted by Hamilton. In 2017, the Ferrari SF70H was thought to be initially a more consistent car in race trim, and Vettel’s P2 times seem to support this claim. practice_times %&gt;% filter(Year == 2017) %&gt;% pivot_longer(c(Time_secs_3, Time_secs_2, Time_secs_1), names_to = &#39;practice_session&#39;, values_to = &#39;time&#39;) %&gt;% mutate(practice_session = str_remove(practice_session, &#39;Time_secs_&#39;), Year = factor(Year)) %&gt;% group_by(Race, practice_session) %&gt;% mutate(track_mean = mean(time, na.rm = T), Time_std_track = time - track_mean) %&gt;% ungroup() %&gt;% group_by(Driver) %&gt;% mutate(driver_n = n(), mean = mean(Time_std_track, na.rm = T)) %&gt;% ungroup() %&gt;% dplyr::filter(driver_n &gt; 15) %&gt;% mutate(practice_session = paste0(&#39;free practice &#39;, practice_session)) %&gt;% ggplot(aes(Time_std_track, y = fct_reorder(Driver, desc(mean)))) + geom_point(position = position_jitter(w = 0, h = 0.1), alpha = 0.5) + theme_bw() + labs(x = &#39;Standardized Practice Time (secs)&#39;, y = &#39;Driver&#39;, title = &#39;Standardized Practice Time&#39;, subtitle = &#39;2017&#39;, caption = &#39;*Times are subtracted from the average time for each Grand Prix&#39;) + geom_vline(xintercept = 0, col = &#39;red&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), plot.caption = element_text(hjust = 0)) + facet_wrap(~ practice_session) + xlim(-5, 5) + geom_hline(yintercept = seq(22, 1, -2), col = &#39;grey&#39;, size = 5, alpha = 0.2) Sorting an axis in ggplot You may have noticed that the last few plots have a y-axis that is sorted by the values along the x-axis. ggplot2 does not sort an axis in this manner by default. However, this can easily be accomplished using the fct_reorder() function. In the plots above, I sort the y-axis (Drivers) by their average standardized time in descending order. For more information on the fct_reorder() function, see this link: https://forcats.tidyverse.org/reference/fct_reorder.html In 2018, Vettel’s distribution of practice times were not quite as fast as Hamilton. Hamilton’s times during P2 seemed to stand apart from the field. Interestingly, Max Verstappen’s times during P2 were comparable to those posted by Mercedes’ other driver, Valtteri Bottas. practice_times %&gt;% filter(Year == 2018) %&gt;% pivot_longer(c(Time_secs_3, Time_secs_2, Time_secs_1), names_to = &#39;practice_session&#39;, values_to = &#39;time&#39;) %&gt;% mutate(practice_session = str_remove(practice_session, &#39;Time_secs_&#39;), Year = factor(Year)) %&gt;% group_by(Race, practice_session) %&gt;% mutate(track_mean = mean(time, na.rm = T), Time_std_track = time - track_mean) %&gt;% ungroup() %&gt;% group_by(Driver) %&gt;% mutate(driver_n = n(), mean = mean(Time_std_track, na.rm = T)) %&gt;% ungroup() %&gt;% dplyr::filter(driver_n &gt; 15) %&gt;% mutate(practice_session = paste0(&#39;free practice &#39;, practice_session)) %&gt;% ggplot(aes(Time_std_track, y = fct_reorder(Driver, desc(mean)))) + geom_point(position = position_jitter(w = 0, h = 0.1), alpha = 0.5) + theme_bw() + labs(x = &#39;Standardized Practice Time (secs)&#39;, y = &#39;Driver&#39;, title = &#39;Standardized Practice Time&#39;, subtitle = &#39;2018&#39;, caption = &#39;*Times are subtracted from the average time for each Grand Prix&#39;) + geom_vline(xintercept = 0, col = &#39;red&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), plot.caption = element_text(hjust = 0)) + facet_wrap(~ practice_session) + xlim(-5, 5) + geom_hline(yintercept = seq(22, 1, -2), col = &#39;grey&#39;, size = 5, alpha = 0.2) During the 2019 season, Ferrari scored six consecutive pole positions between the Belgian and Mexican Grands Prix. However, after the FIA issued a technical directive reminding competitors of the regulations regarding fuel sensors, Ferrari’s performance faded and they failed to score a pole position or race win for the remainder of the season. In the plot below, we can see that Charles LeClerc’s distribution is fairly wide, which is likely indicative of this shift in performance. practice_times %&gt;% filter(Year == 2019) %&gt;% pivot_longer(c(Time_secs_3, Time_secs_2, Time_secs_1), names_to = &#39;practice_session&#39;, values_to = &#39;time&#39;) %&gt;% mutate(practice_session = str_remove(practice_session, &#39;Time_secs_&#39;), Year = factor(Year)) %&gt;% group_by(Race, practice_session) %&gt;% mutate(track_mean = mean(time, na.rm = T), Time_std_track = time - track_mean) %&gt;% ungroup() %&gt;% group_by(Driver) %&gt;% mutate(driver_n = n(), mean = mean(Time_std_track, na.rm = T)) %&gt;% ungroup() %&gt;% dplyr::filter(driver_n &gt; 15) %&gt;% mutate(practice_session = paste0(&#39;free practice &#39;, practice_session)) %&gt;% ggplot(aes(Time_std_track, y = fct_reorder(Driver, desc(mean)))) + geom_point(alpha = 0.5, col = &#39;red&#39;) + gghighlight(Driver == &#39;LEC&#39;, calculate_per_facet = T) + theme_bw() + labs(x = &#39;Standardized Practice Time (secs)&#39;, y = &#39;Driver&#39;, title = &#39;Standardized Practice Time&#39;, subtitle = &#39;2019&#39;, caption = &#39;*Times are subtracted from the average time for each Grand Prix&#39;) + geom_vline(xintercept = 0, col = &#39;red&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), plot.caption = element_text(hjust = 0)) + facet_wrap(~ practice_session) + xlim(-5, 5) + geom_hline(yintercept = seq(22, 1, -2), col = &#39;grey&#39;, size = 5, alpha = 0.1) Highlighting data in ggplot In the plot above, I highlight Charles Leclerc’s data using the gghighlight() function from the gghighlight package. This function is version easy to use! Because this plot also utilizes facet_wrap() and I wanted to highlight each facet individually, I set the calculate_per_facet argument to TRUE. To highlight Charles Leclerc, I used the following line of code: gghighlight(Abbr == 'LEC', calculate_per_facet = T). For more information on the gghighlight package, visit this link: https://cran.r-project.org/web/packages/gghighlight/vignettes/gghighlight.html In 2020, Max Verstappen competed with the two Mercedes Drivers (Hamilton and Bottas) in P3 (qualifying simulations), but struggled in the longer runs of P2. As we can see in the plot below, Verstappen’s P3 pace was the closest to Mercedes, and far quicker than his teammate Alex Albon. practice_times %&gt;% filter(Year == 2020) %&gt;% pivot_longer(c(Time_secs_3, Time_secs_2, Time_secs_1), names_to = &#39;practice_session&#39;, values_to = &#39;time&#39;) %&gt;% mutate(practice_session = str_remove(practice_session, &#39;Time_secs_&#39;), Year = factor(Year)) %&gt;% group_by(Race, practice_session) %&gt;% mutate(track_mean = mean(time, na.rm = T), Time_std_track = time - track_mean) %&gt;% ungroup() %&gt;% group_by(Driver) %&gt;% mutate(driver_n = n(), mean = mean(Time_std_track, na.rm = T)) %&gt;% ungroup() %&gt;% dplyr::filter(driver_n &gt; 15) %&gt;% mutate(practice_session = paste0(&#39;free practice &#39;, practice_session)) %&gt;% ggplot(aes(Time_std_track, y = fct_reorder(Driver, desc(mean)))) + geom_point(alpha = 0.5, col = &#39;navy&#39;) + gghighlight(Driver == &#39;VER&#39;, calculate_per_facet = T) + theme_bw() + labs(x = &#39;Standardized Practice Time (secs)&#39;, y = &#39;Driver&#39;, title = &#39;Standardized Practice Time&#39;, subtitle = &#39;2020&#39;, caption = &#39;*Times are subtracted from the average time for each Grand Prix&#39;) + geom_vline(xintercept = 0, col = &#39;red&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), plot.caption = element_text(hjust = 0)) + facet_wrap(~ practice_session) + xlim(-5, 5) + geom_hline(yintercept = seq(22, 1, -2), col = &#39;grey&#39;, size = 5, alpha = 0.1) For the first time since 2014, Mercedes faced a legitimate challenge to their dominance in 2021. While Ferrari had their moments during the turbo-hybrid era, Red Bull and Max Verstappen competed with Mercedes during every session of the 2021 season. practice_times %&gt;% filter(Year == 2021) %&gt;% pivot_longer(c(Time_secs_3, Time_secs_2, Time_secs_1), names_to = &#39;practice_session&#39;, values_to = &#39;time&#39;) %&gt;% mutate(practice_session = str_remove(practice_session, &#39;Time_secs_&#39;), Year = factor(Year)) %&gt;% group_by(Race, practice_session) %&gt;% mutate(track_mean = mean(time, na.rm = T), Time_std_track = time - track_mean) %&gt;% ungroup() %&gt;% group_by(Driver) %&gt;% mutate(driver_n = n(), mean = mean(Time_std_track, na.rm = T)) %&gt;% ungroup() %&gt;% dplyr::filter(driver_n &gt; 15) %&gt;% mutate(practice_session = paste0(&#39;free practice &#39;, practice_session)) %&gt;% ggplot(aes(Time_std_track, y = fct_reorder(Driver, desc(mean)), col = Driver)) + geom_point(alpha = 0.5) + gghighlight(Driver %in% c(&#39;VER&#39;, &#39;HAM&#39;, &#39;BOT&#39;), calculate_per_facet = T) + theme_bw() + labs(x = &#39;Standardized Practice Time (secs)&#39;, y = &#39;Driver&#39;, title = &#39;Standardized Practice Time&#39;, subtitle = &#39;2021&#39;, caption = &#39;*Times are subtracted from the average time for each Grand Prix&#39;) + geom_vline(xintercept = 0, col = &#39;red&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), plot.caption = element_text(hjust = 0)) + facet_wrap(~ practice_session) + xlim(-5, 5) + scale_color_manual(&#39;&#39;, values = c(&#39;seagreen&#39;, &#39;seagreen&#39;, &#39;navy&#39;)) + geom_hline(yintercept = seq(22, 1, -2), col = &#39;grey&#39;, size = 5, alpha = 0.1) Custom discrete color scales in ggplot In the plot above, I created a custom color scale that corresponds to two teams: Mercedes and Red Bull. The scale_color_manual() function allows you to specify your own set of mappings from levels/groups in the data to color values. For more information on this, visit this link: https://ggplot2.tidyverse.org/reference/scale_manual.html The plot below shows the head-to-head practice battles for Verstappen vs Hamilton in 2021. This figure seems to suggest that Hamilton had the edge in race pace (P2), while Verstappen had the advantage in qualifying simulations (P3). practice_times %&gt;% filter(Year == 2021) %&gt;% mutate(round = match(Race, unique(Race))) %&gt;% ungroup() %&gt;% pivot_longer(c(Time_secs_3, Time_secs_2, Time_secs_1), names_to = &#39;practice_session&#39;, values_to = &#39;time&#39;) %&gt;% mutate(practice_session = str_remove(practice_session, &#39;Time_secs_&#39;), Year = factor(Year)) %&gt;% group_by(Race, practice_session) %&gt;% mutate(track_mean = mean(time, na.rm = T), Time_std_track = time - track_mean) %&gt;% ungroup() %&gt;% group_by(Driver) %&gt;% mutate(driver_n = n(), mean = mean(Time_std_track, na.rm = T)) %&gt;% ungroup() %&gt;% dplyr::filter(driver_n &gt; 15) %&gt;% mutate(practice_session = paste0(&#39;free practice &#39;, practice_session)) %&gt;% filter(Driver %in% c(&#39;VER&#39;, &#39;HAM&#39;)) %&gt;% ggplot(aes(x = fct_reorder(Race, round), y = Time_std_track, group = Driver, col = Driver)) + geom_point() + geom_path() + theme_bw() + facet_wrap(~ practice_session) + scale_color_manual(&#39;&#39;, values = c(&#39;seagreen&#39;, &#39;navy&#39;)) + labs(y = &#39;Standardized Practice Time (secs)&#39;, x = &#39;Grand Prix&#39;, title = title_color_coder(&quot;&quot;, &quot;Verstappen&quot;, &#39;navy&#39;, &quot; vs &quot;, &quot;Hamilton&quot;, &#39;seagreen&#39;,&quot; in 2021 Practice Sessions&quot;), subtitle = &#39;Standardized Practice Time&#39;, caption = &#39;*Times are subtracted from the average time for each Grand Prix&#39;) + geom_vline(xintercept = seq(22, 1, -2), col = &#39;grey&#39;, size = 5, alpha = 0.2) + theme(strip.text.x = element_text(size = 12), axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1, size = 12), axis.title.x = element_text(size = 15), axis.title.y = element_text(size = 15), plot.subtitle = element_text(hjust = 0.5, size = 15), plot.caption = element_text(hjust = 0, size = 12), legend.text = element_text(size = 13), plot.title = ggtext::element_markdown(hjust = 0.5, size = 20)) ## render the provided text as markdown/html Color-coding the title in ggplot The ggtext package provides Markdown and HTML rendering for ggplot2. Creating color-coded titles in ggplot can get tedious if there are many different colors being used, so I created a simple function (title_color_coder()) to add color to titles or subtitles in a ggplot. Be sure to add the following function to ggplot code: theme(plot.title = ggtext::element_markdown()) For more information on the ggtext package, visit this link: https://cran.r-project.org/web/packages/ggtext/readme/README.html The 2022 Formula 1 season brought an overhaul of the technical regulations. The 2022 technical regulations reintroduced the use of ground effect for the first time since 1983. Additionally, teams now had to abide by a financial cost cap. These changes upset the dominance of Mercedes, and seemingly ushered in a new era of dominance by Red Bull. While Ferrari often had the fastest qualifying pace, Red Bull enjoyed a considerable advantage in race pace. practice_times %&gt;% filter(Year == 2022) %&gt;% pivot_longer(c(Time_secs_3, Time_secs_2, Time_secs_1), names_to = &#39;practice_session&#39;, values_to = &#39;time&#39;) %&gt;% mutate(practice_session = str_remove(practice_session, &#39;Time_secs_&#39;), Year = factor(Year)) %&gt;% group_by(Race, practice_session) %&gt;% mutate(track_mean = mean(time, na.rm = T), Time_std_track = time - track_mean) %&gt;% ungroup() %&gt;% group_by(Driver) %&gt;% mutate(driver_n = n(), mean = mean(Time_std_track, na.rm = T)) %&gt;% ungroup() %&gt;% dplyr::filter(driver_n &gt; 15) %&gt;% mutate(practice_session = paste0(&#39;free practice &#39;, practice_session)) %&gt;% ggplot(aes(Time_std_track, y = fct_reorder(Driver, desc(mean)), col = Driver)) + geom_point(alpha = 0.5) + gghighlight(Driver %in% c(&#39;VER&#39;, &#39;LEC&#39;, &#39;SAI&#39;, &#39;PER&#39;), calculate_per_facet = T) + theme_bw() + labs(x = &#39;Standardized Practice Time (secs)&#39;, y = &#39;Driver&#39;, title = &#39;Standardized Practice Time&#39;, subtitle = &#39;2022&#39;, caption = &#39;*Times are subtracted from the average time for each Grand Prix&#39;) + geom_vline(xintercept = 0, col = &#39;red&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), plot.caption = element_text(hjust = 0)) + facet_wrap(~ practice_session) + xlim(-5, 5) + scale_color_manual(&#39;&#39;, values = c(&#39;red&#39;, &#39;navy&#39;, &#39;red&#39;, &#39;navy&#39;)) + geom_hline(yintercept = seq(22, 1, -2), col = &#39;grey&#39;, size = 5, alpha = 0.1) How to rotate axis text in ggplot At times, you may wish to rotate the text of labels along the x- or y-axis. To rotate the x-axis labels in the plot above, I simply added the following function to the ggplot: theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) The rich got richer and the fast got faster in 2023. Max Verstappen was even more dominant in his third title-winning season. Compared to the previous year, Max was relatively even quicker across all practice sessions during 2023. practice_times %&gt;% filter(Year == 2023) %&gt;% pivot_longer(c(Time_secs_3, Time_secs_2, Time_secs_1), names_to = &#39;practice_session&#39;, values_to = &#39;time&#39;) %&gt;% mutate(practice_session = str_remove(practice_session, &#39;Time_secs_&#39;), Year = factor(Year)) %&gt;% group_by(Race, practice_session) %&gt;% mutate(track_mean = mean(time, na.rm = T), Time_std_track = time - track_mean) %&gt;% ungroup() %&gt;% group_by(Driver) %&gt;% mutate(driver_n = n(), mean = mean(Time_std_track, na.rm = T)) %&gt;% ungroup() %&gt;% dplyr::filter(driver_n &gt; 15) %&gt;% mutate(practice_session = paste0(&#39;free practice &#39;, practice_session)) %&gt;% ggplot(aes(Time_std_track, y = fct_reorder(Driver, desc(mean)), col = Driver)) + geom_point(alpha = 0.5) + gghighlight(Driver %in% c(&#39;VER&#39;, &#39;LEC&#39;, &#39;SAI&#39;, &#39;PER&#39;), calculate_per_facet = T) + theme_bw() + labs(x = &#39;Standardized Practice Time (secs)&#39;, y = &#39;Driver&#39;, title = &#39;Standardized Practice Time&#39;, subtitle = &#39;2023&#39;, caption = &#39;*Times are subtracted from the average time for each Grand Prix&#39;) + geom_vline(xintercept = 0, col = &#39;red&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), plot.caption = element_text(hjust = 0)) + facet_wrap(~ practice_session) + xlim(-5, 5) + scale_color_manual(&#39;&#39;, values = c(&#39;red&#39;, &#39;navy&#39;, &#39;red&#39;, &#39;navy&#39;)) + geom_hline(yintercept = seq(23, 1, -2), col = &#39;grey&#39;, size = 5, alpha = 0.1) 2.5 Next Chapter In the next chapter, we will move onto qualifying! "],["qualifying.html", "Chapter 3 Qualifying 3.1 Distribution of Qualifying Times by Session 3.2 Distribution of Qualifying Times by Year 3.3 Qualifying Times by Grand Prix 3.4 Car Progression Over Time 3.5 Next Chapter", " Chapter 3 Qualifying Qualifying is one of the most important components of a Formula 1 Grand Prix weekend. Qualifying position determines the position where that driver starts the Grand Prix. Qualifying is actually composed of three different sessions: Q1, Q2, and Q3. All drivers take part in Q1, but only the top 15 fastest times are allowed to continue on to Q2. The fastest 10 times in Q2 make it to Q3. Q1 lasts 18 minutes, while Q2 lasts 15 minutes, and Q3 lasts 12 minutes. If a driver violates a rule or regulation, the penalties are issued following qualifying. Qualifying data is readily available online. To scrape qualifying data drectly from www.formula1.com, you can use the qualifying_scraper() function from the drs package. library(tidyverse) library(drs) # Scrape qualifying data qualifying_2023 &lt;- qualifying_scraper(2023) qualifying_2022 &lt;- qualifying_scraper(2022) qualifying_2021 &lt;- qualifying_scraper(2021) qualifying_2020 &lt;- qualifying_scraper(2020) qualifying_2019 &lt;- qualifying_scraper(2019) qualifying_2018 &lt;- qualifying_scraper(2018) qualifying_2017 &lt;- qualifying_scraper(2017) qualifying_2016 &lt;- qualifying_scraper(2016) qualifying_2015 &lt;- qualifying_scraper(2015) qualifying_2014 &lt;- qualifying_scraper(2014) # Combine all qualifying data qualifying_allyears &lt;- rbind(qualifying_2023, qualifying_2022, qualifying_2021, qualifying_2020, qualifying_2019, qualifying_2018, qualifying_2017, qualifying_2016, qualifying_2015, qualifying_2014) 3.1 Distribution of Qualifying Times by Session Note: Throughout this chapter, I’ll introduce various visualization types. However, I knowingly include some visualizations that are not ideal for a given example. I did this for two reasons: (1) I wanted to include as much code as possible, in hopes that you may be able to use it for your own data, and (2) I wanted to include examples where we are forced to “try again”. One of the more basic ways to visualize qualifying times is to use a histogram. We can use histograms to plot the entire distribution of qualifying times from 2014 to 2023. In the figure below, I lump all qualifying times together (all sessions, drivers, teams, years, and circuits are included in the same distribution). qualifying_allyears %&gt;% dplyr::select(Q1_secs, Q2_secs, Q3_secs) %&gt;% pivot_longer(everything(), names_to = &#39;qualifying_session&#39;, values_to = &#39;time&#39;) %&gt;% ggplot(aes(time)) + geom_histogram(alpha = 0.5) + theme_bw() + labs(x = &#39;Qualifying Time (secs)&#39;, title = &#39;Distribution of all qualifying times&#39;, subtitle = &#39;2014 - 2023&#39;)+ theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), legend.position = &quot;none&quot;) How to create a histogram in R The histogram geometry, geom_histogram(), can be used to plot the distribution of a single continuous variable. geom_histogram() counts the number of observations in a given gib, and plots along the x-axis. For more information on using geom_histogram() in ggplot2, visit this link: https://ggplot2.tidyverse.org/reference/geom_histogram.html The plot above is not all that informative (at least in my opinion). Most people are probably not that interested in the distribution of all qualifying times, but rather the distribution by session, year, or some other grouping variable. Here’s a look at the distribution of qualifying times by session. Obviously, Q3 times should be faster than Q2 which is faster than Q1. qualifying_allyears %&gt;% pivot_longer(c(&#39;Q1_secs&#39;, &#39;Q2_secs&#39;, &#39;Q3_secs&#39;), names_to = &#39;session&#39;, values_to = &#39;time&#39;) %&gt;% mutate(Year = factor(Year), session = str_remove(session, &#39;_secs&#39;)) %&gt;% ggplot() + geom_histogram(aes(x = time, fill = session), alpha = 0.5, show.legend = T, position=&quot;identity&quot;) + theme_bw() + labs(x = &#39;Qualifying Time (secs)&#39;, title = &#39;Distribution of all qualifying times&#39;, subtitle = &#39;2014 - 2023&#39;)+ theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) + scale_fill_viridis_d() How to remove matched patterns in R In the code chunk above, I use the str_remove() function to remove text from the session variable name. At times, you will be left with sloppy variable names (like I did here). I wanted to remove the “_secs” suffix from the session factor names. str_remove() makes this incredibly easy to do. For more information on using str_remove(), visit this link: https://stringr.tidyverse.org/reference/str_remove.html There are more participants in the slower sessions, so the height of the distributions in the above figure are going to be different. We can standardize the heights of the distributions by using the following argument: geom_histogram(aes(x = time, y = ..density.., fill = session) qualifying_allyears %&gt;% pivot_longer(c(&#39;Q1_secs&#39;, &#39;Q2_secs&#39;, &#39;Q3_secs&#39;), names_to = &#39;session&#39;, values_to = &#39;time&#39;) %&gt;% mutate(Year = factor(Year), session = str_remove(session, &#39;_secs&#39;)) %&gt;% ggplot() + geom_histogram(aes(x = time, y = ..density.., fill = session), alpha = 0.5, show.legend = T, position=&quot;identity&quot;) + theme_bw() + labs(x = &#39;Qualifying Time (secs)&#39;, title = &#39;Distribution of all qualifying times&#39;, subtitle = &#39;2014 - 2023&#39;)+ theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) + scale_fill_viridis_d() Even with this adjustment, the plot is still not very visually informative. As an alternative, we can split the plot by session: qualifying_allyears %&gt;% pivot_longer(c(&#39;Q1_secs&#39;, &#39;Q2_secs&#39;, &#39;Q3_secs&#39;), names_to = &#39;session&#39;, values_to = &#39;time&#39;) %&gt;% mutate(Year = factor(Year), session = str_remove(session, &#39;_secs&#39;)) %&gt;% ggplot() + geom_histogram(aes(x = time, y = ..density.., fill = session), alpha = 0.5, show.legend = T, position=&quot;identity&quot;) + theme_bw() + labs(x = &#39;Qualifying Time (secs)&#39;, title = &#39;Distribution of all qualifying times&#39;, subtitle = &#39;2014 - 2023&#39;)+ theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) + scale_fill_viridis_d() + facet_wrap(~ session, ncol = 1) A little better, but… not great. We should revisit the best way to visualize times by session later. But for now, we will shift our focus to yearly differences. 3.2 Distribution of Qualifying Times by Year Visualizing the distribution of times by year is somewhat interesting, however. It is very easy to add a new dimension (year) to this histogram: qualifying_allyears %&gt;% pivot_longer(c(&#39;Q1_secs&#39;, &#39;Q2_secs&#39;, &#39;Q3_secs&#39;), names_to = &#39;session&#39;, values_to = &#39;time&#39;) %&gt;% mutate(Year = factor(Year)) %&gt;% ggplot() + geom_histogram(aes(x = time, fill = Year), alpha = 0.5, show.legend = T, position=&quot;identity&quot;) + theme_bw() + labs(x = &#39;Qualifying Time (secs)&#39;, title = &#39;Distribution of all qualifying times&#39;, subtitle = &#39;2014 - 2023&#39;)+ theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) + scale_fill_viridis_d() T his includes all qualifying sessions together. To clear things up a bit, I will group the data by year while retaining different plots for each session: qualifying_allyears %&gt;% pivot_longer(c(&#39;Q1_secs&#39;, &#39;Q2_secs&#39;, &#39;Q3_secs&#39;), names_to = &#39;session&#39;, values_to = &#39;time&#39;) %&gt;% mutate(Year = factor(Year), session = str_remove(session, &#39;_secs&#39;)) %&gt;% ggplot() + geom_histogram(aes(x = time, fill = Year), alpha = 0.5, show.legend = T, position=&quot;identity&quot;) + theme_bw() + labs(x = &#39;Qualifying Time (secs)&#39;, title = &#39;Distribution of all qualifying times&#39;, subtitle = &#39;2014 - 2023&#39;)+ theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) + scale_fill_viridis_d() + facet_wrap(~ session, scales = &#39;free_y&#39;) This plot is still very tough to read! Luckily, ggplot is very flexible, so there’s a few different ways to try and remedy this: Option 1: Use facet_wrap() to split the distributions into facets: qualifying_allyears %&gt;% pivot_longer(c(&#39;Q1_secs&#39;, &#39;Q2_secs&#39;, &#39;Q3_secs&#39;), names_to = &#39;session&#39;, values_to = &#39;time&#39;) %&gt;% mutate(Year = factor(Year), session = str_remove(session, &#39;_secs&#39;)) %&gt;% ggplot() + geom_histogram(aes(x = time, fill = session), alpha = 0.5, show.legend = T, position=&quot;identity&quot;) + theme_bw() + labs(x = &#39;Qualifying Time (secs)&#39;, title = &#39;Distribution of all qualifying times&#39;, subtitle = &#39;2014 - 2023&#39;, fill = &#39;&#39;)+ theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), legend.position = &#39;bottom&#39;) + scale_fill_viridis_d() + facet_wrap(~ Year, ncol = 5) Option 2: Use geom_density(), in lieu of geom_histogram(). ggplot’s density geometry is a smooth alternative to the histogram geometry. qualifying_allyears %&gt;% pivot_longer(c(&#39;Q1_secs&#39;, &#39;Q2_secs&#39;, &#39;Q3_secs&#39;), names_to = &#39;session&#39;, values_to = &#39;time&#39;) %&gt;% mutate(Year = factor(Year), session = str_remove(session, &#39;_secs&#39;)) %&gt;% ggplot() + geom_density(aes(x = time, fill = session), alpha = 0.5, show.legend = T, position=&quot;identity&quot;) + theme_bw() + labs(x = &#39;Qualifying Time (secs)&#39;, title = &#39;Distribution of all qualifying times&#39;, subtitle = &#39;2014 - 2023&#39;)+ theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), legend.position = &#39;bottom&#39;) + scale_fill_viridis_d() How to create a kernel density plot in R An alternative to using the histogram geometry, geom_histogram(), is the smoothed kernel density geometry, geom_density(). geom_density() calculates and plots a smoothed version of a histogram, commonly referred to as the kernel density estimate. For more information on using geom_density() in ggplot2, visit this link: https://ggplot2.tidyverse.org/reference/geom_density.html These distributions include all qualifying sessions from 2014 to 2023, and the multi-modal nature of the distributions reflect this. Or Option 3: Use geom_density() and facet_wrap() together. qualifying_allyears %&gt;% pivot_longer(c(&#39;Q1_secs&#39;, &#39;Q2_secs&#39;, &#39;Q3_secs&#39;), names_to = &#39;session&#39;, values_to = &#39;time&#39;) %&gt;% mutate(Year = factor(Year), session = str_remove(session, &#39;_secs&#39;)) %&gt;% ggplot() + geom_density(aes(x = time, fill = session), alpha = 0.5, show.legend = T, position=&quot;identity&quot;) + theme_bw() + labs(x = &#39;Qualifying Time (secs)&#39;, title = &#39;Distribution of all qualifying times&#39;, subtitle = &#39;2014 - 2023&#39;)+ theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), legend.position = &#39;bottom&#39;) + scale_fill_viridis_d() + facet_wrap(~ Year, ncol = 5) Option 4: Ridgelines One of my favorite ways to visualize shifts in a distribution over time is with density ridgelines. Density ridgelines are a way to visualize shifts in distributions across another variable. The name ridgeline was given because these plots give the impression of a mountain ridge. The density ridgeline is an alternative to standard density plots. The R package ggridges provides a very useful function called geom_density_ridges_gradient() that makes creating these plots very easy. I’ll use this function to create a ridgeline plot of our qualifying data. library(ggridges) qualifying_allyears %&gt;% pivot_longer(c(&#39;Q1_secs&#39;, &#39;Q2_secs&#39;, &#39;Q3_secs&#39;), names_to = &#39;session&#39;, values_to = &#39;time&#39;) %&gt;% mutate(Year = factor(Year), session = str_remove(session, &#39;_secs&#39;)) %&gt;% ggplot() + geom_density_ridges_gradient(aes(x = time, y = Year, fill = stat(x)), alpha = 0.5, show.legend = T, position=&quot;identity&quot;) + theme_bw() + labs(x = &#39;Qualifying Time (secs)&#39;, title = &#39;Distribution of qualifying times&#39;, subtitle = &#39;All Grands Prix: 2014 - 2023&#39;)+ theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), legend.position = &#39;none&#39;) + scale_fill_viridis_c() Interesting! Let’s build on this idea and start ooking into differences by Grand Prix. 3.3 Qualifying Times by Grand Prix Perhaps, a more useful way to use histograms is to plot the distribution for one Grand Prix at a time. For instance, here’s the distribution of qualifying times at the Bahrain Grand Prix only: qualifying_allyears %&gt;% pivot_longer(c(&#39;Q1_secs&#39;, &#39;Q2_secs&#39;, &#39;Q3_secs&#39;), names_to = &#39;session&#39;, values_to = &#39;time&#39;) %&gt;% mutate(Year = factor(Year), session = str_remove(session, &#39;_secs&#39;)) %&gt;% filter(Race == &#39;bahrain&#39;) %&gt;% ggplot() + geom_density(aes(x = time, fill = Year), alpha = 0.5, show.legend = T, position=&quot;identity&quot;) + theme_bw() + labs(x = &#39;Qualifying Time (secs)&#39;, title = &#39;Distribution of qualifying times at the Bahrain Grand Prix&#39;, subtitle = &#39;2014 - 2023&#39;)+ theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) + scale_fill_viridis_d() I prefer to use ggridges for this type of task. So, I’ll create a ridgeline plot for the Bahrain Grand Prix here: qualifying_allyears %&gt;% pivot_longer(c(&#39;Q1_secs&#39;, &#39;Q2_secs&#39;, &#39;Q3_secs&#39;), names_to = &#39;session&#39;, values_to = &#39;time&#39;) %&gt;% mutate(Year = factor(Year), session = str_remove(session, &#39;_secs&#39;)) %&gt;% filter(Race == &#39;bahrain&#39;) %&gt;% ggplot() + geom_density_ridges_gradient(aes(x = time, y = Year, fill = stat(x)), alpha = 0.5, show.legend = T, position=&quot;identity&quot;) + theme_bw() + labs(x = &#39;Qualifying Time (secs)&#39;, title = &#39;Distribution of qualifying times at the Bahrain Grand Prix&#39;, subtitle = &#39;2014 - 2023&#39;)+ theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), legend.position = &#39;none&#39;) + scale_fill_viridis_c() Much better! How to create a ridgeline plot in R Density ridgelines are a way to visualize shifts in distributions across another variable. The name ridgeline was given because these plots give the impression of a mountain ridge. The ggridges package provides two main geometries for creating ridgeline plots: geom_ridgeline() geom_density_ridges() geom_ridgeline() takes height values directly to draw ridglines, while geom_density_ridges() first estimates the data densities before drawing the ridgelines. In my opinion, geom_density_ridges() is easier to use. For more information on ggridges, visit this link: https://cran.r-project.org/web/packages/ggridges/vignettes/introduction.html One challenge with the qualifying data is that the fastest drivers are counted three times and the slowest only once. All drivers will typically have a Q1 time, but the fastest cars are probably driving more conservatively. So, how do we make comparisons between drivers? I prefer taking each driver’s fastest time in their last session. So, if a driver made it to Q2 but not Q3, we should use his Q2 time. To use this data, we will need to further tidy up the data that we currently have using the following code: qualifying_allyears &lt;- qualifying_allyears %&gt;% mutate(Q_secs = case_when( !is.na(Q3_secs) ~ Q3_secs, is.na(Q3_secs) &amp; !is.na(Q2_secs) ~ Q2_secs, is.na(Q3_secs) &amp; is.na(Q2_secs) &amp; !is.na(Q1_secs) ~ Q1_secs)) Now, I will use geom_histogram() / geom_density() to compare two Grands Prix: Bahrain vs Monaco. # histogram qualifying_allyears %&gt;% mutate(Year = factor(Year)) %&gt;% filter(Race %in% c(&#39;bahrain&#39;, &#39;monaco&#39;)) %&gt;% ggplot() + geom_histogram(aes(x = Q_secs, fill = Race), alpha = 0.5, show.legend = T, position=&quot;identity&quot;) + theme_bw() + labs(fill = &#39;Grand Prix&#39;, x = &#39;Qualifying Time (secs)&#39;, title = &#39;Distribution of qualifying times&#39;, subtitle = &#39;Bahrain vs Monaco&#39;)+ theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) #density qualifying_allyears %&gt;% mutate(Year = factor(Year)) %&gt;% filter(Race %in% c(&#39;bahrain&#39;, &#39;monaco&#39;)) %&gt;% ggplot() + geom_density(aes(x = Q_secs, fill = Race), alpha = 0.5, show.legend = T, position=&quot;identity&quot;) + theme_bw() + labs(fill = &#39;Grand Prix&#39;, x = &#39;Qualifying Time (secs)&#39;, title = &#39;Distribution of qualifying times&#39;, subtitle = &#39;Bahrain vs Monaco&#39;)+ theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) Clearly times in Bahrain are different than those at Monaco. In fact, all circuits are going to vary to some degree. The following figure shows the average qualifying times by Grand Prix. qualifying_allyears %&gt;% group_by(Race) %&gt;% summarize(mean = mean(Q_secs, na.rm = T), sd = sd(Q_secs, na.rm = T)) %&gt;% ggplot(aes(x = mean, y = fct_reorder(Race, mean))) + geom_point(position = position_jitter(h = 0, w = 0.3), alpha = 0.5) + geom_hline(yintercept = seq(37, 1, -2), col = &#39;grey&#39;, size = 3, alpha = 0.15) + theme_bw() + labs(x = &#39;Average Qualifying Time (secs)&#39;, y = &#39;Grand Prix&#39;, title = &#39;Average Qualifying Times&#39;, subtitle = &#39;2014 - 2023&#39;)+ theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), legend.position = &quot;none&quot;) A lot can happen during a qualifying session causing variability among the cars. There is a spectrum of performance across cars, drivers, sessions, and year. Therefore, it’s a good idea to include a measure of variance in this plot. So, we can add standard error bars to the plot we just made. qualifying_allyears %&gt;% group_by(Race) %&gt;% summarize(mean = mean(Q_secs, na.rm = T), sd = sd(Q_secs, na.rm = T)) %&gt;% mutate(lower = mean - sd, upper = mean + sd) %&gt;% ggplot(aes(x = mean, xmin = lower, xmax = upper, y = fct_reorder(Race, mean))) + geom_pointrange(alpha = 0.5) + geom_hline(yintercept = seq(37, 1, -2), col = &#39;grey&#39;, size = 3, alpha = 0.15) + theme_bw() + labs(x = &#39;Average Qualifying Time \\u00b1 SD (secs)&#39;, y = &#39;Grand Prix&#39;, title = &#39;Average Qualifying Times&#39;, subtitle = &#39;2014 - 2023&#39;)+ theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), legend.position = &quot;none&quot;) Plotting a point-range in ggplot Similar to earlier plots using stat_summary(), we can manually construct a point-range in ggplot using the geom_pointrange(). In the example above, the continuous variable is plotted along the x-axis. I wanted to plot the mean ± 1 standard deviation, so I pass arguments for x, xmin and xmax that define the mean, minimum, and maximum values in this geometry. For more information on using geom_pointrange() in ggplot2, visit this link: https://ggplot2.tidyverse.org/reference/geom_linerange.html Previously, we demonstrated that practice times vary by year. So, qualifying times must follow a similar pattern. Using the code below, we can plot the average qualifying times by year. qualifying_allyears %&gt;% mutate(Year = factor(Year)) %&gt;% group_by(Year) %&gt;% summarize(mean = mean(Q_secs, na.rm = T), sd = sd(Q_secs, na.rm = T)) %&gt;% mutate(lower = mean - sd, upper = mean + sd) %&gt;% ggplot(aes(y = mean, x = Year)) + geom_point(alpha = 0.5, size = 3) + theme_bw() + labs(x = &#39;Year&#39;, y = &#39;Average Qualifying Time (secs)&#39;, title = &#39;Average Qualifying Times&#39;, subtitle = &#39;2014 - 2023&#39;)+ theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), legend.position = &quot;none&quot;) This plot computes an average for all cars and circuits. This collection of qualifying times will undoubtedly include lots of variability! Using geom_pointrange(), we can try to capture that variability in the plot below: qualifying_allyears %&gt;% mutate(Year = factor(Year)) %&gt;% group_by(Year) %&gt;% summarize(mean = mean(Q_secs, na.rm = T), sd = sd(Q_secs, na.rm = T)) %&gt;% mutate(lower = mean - sd, upper = mean + sd) %&gt;% ggplot(aes(y = mean, ymin = lower, ymax = upper, x = Year)) + geom_pointrange(alpha = 0.5) + theme_bw() + labs(x = &#39;Year&#39;, y = &#39;Average Qualifying Time \\u00b1 SD (secs)&#39;, title = &#39;Average Qualifying Times&#39;, subtitle = &#39;2014 - 2023&#39;)+ theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), legend.position = &quot;none&quot;) To chip away at this variability, we can replicate this plot for just a handful of circuits (i.e. Australia, Monaco, Brazil, Austria, Canada, and the United States). Below, I use facet_wrap() to split the figure and assign each circuit its own facet. Notice how much tighter the standard deviations have become… much better! qualifying_allyears %&gt;% filter(Race %in% c(&#39;australia&#39;, &#39;monaco&#39;, &#39;brazil&#39;, &#39;austria&#39;, &#39;canada&#39;, &#39;united-states&#39;)) %&gt;% mutate(Year = factor(Year)) %&gt;% group_by(Year, Race) %&gt;% summarize(mean = mean(Q_secs, na.rm = T), sd = sd(Q_secs, na.rm = T)) %&gt;% mutate(lower = mean - sd, upper = mean + sd) %&gt;% ungroup() %&gt;% ggplot(aes(y = mean, ymin = lower, ymax = upper, x = Year)) + geom_pointrange(alpha = 0.5) + theme_bw() + labs(x = &#39;Year&#39;, y = &#39;Average Qualifying Time \\u00b1 SD (secs)&#39;, title = &#39;Average Qualifying Times&#39;, subtitle = &#39;2014 - 2023&#39;)+ theme(plot.title = element_text(hjust = 0.5, size = 18), plot.subtitle = element_text(hjust = 0.5, size = 15), legend.position = &quot;none&quot;, axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1, size = 10), axis.text.y = element_text(size = 10), strip.text = element_text(size = 12), axis.title.x = element_text(size = 12), axis.title.y = element_text(size = 12)) + facet_wrap(~ Race, scales = &#39;free_y&#39;) In the next section, we will dive a bit deeper into qualifying time progression. 3.4 Car Progression Over Time How does car development progress over time? Below, we can look at how the qualifying pace changes over time for the first race of most years: the Australian Grand Prix. qualifying_allyears %&gt;% mutate(Year = factor(Year)) %&gt;% filter(Race == &#39;australia&#39;) %&gt;% ggplot(aes(Year, Q_secs)) + geom_point(position = position_jitter(h = 0, w = 0.3), alpha = 0.5) + theme_bw() + labs(x = &#39;Year&#39;, y = &#39;Best Qualifying Time (secs)&#39;, title = &#39;Australian Grand Prix Qualifying&#39;, subtitle = &#39;2014 - 2023&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) During 2014 qualifying in Australia, rain arrived during Q2. I will color these points in blue below. qualifying_allyears %&gt;% mutate(Year = factor(Year)) %&gt;% filter(Race == &#39;australia&#39;) %&gt;% mutate(rain = ifelse(Year == &#39;2014&#39; &amp; Q_secs &gt; 100, &#39;rain during Q2 &amp; Q3&#39;, &#39; &#39;)) %&gt;% ggplot() + geom_point(aes(Year, Q_secs), position = position_jitter(seed= 123, h = 0, w = 0.3), alpha = 0.5) + geom_point(aes(Year, Q_secs, col = rain), position = position_jitter(seed = 123, h = 0, w = 0.3), alpha = 0.5) + theme_bw() + labs(x = &#39;Year&#39;, y = &#39;Best Qualifying Time (secs)&#39;, title = &#39;Australian Grand Prix Qualifying&#39;, subtitle = &#39;2014 - 2023&#39;) + scale_colour_manual(&quot;&quot;, values = c(&#39;rain during Q2 &amp; Q3&#39; = &#39;cornflowerblue&#39;, &#39; &#39; = &#39;transparent&#39;)) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) Reproducible jittering in ggplot In the plot above, I had to get a bit creative. My goal was to highlight the times in 2014 by using a blue color. And, I only wanted a single label for those times. My solution was to plot all points as a first layer, and then re-plot the data using a conditional coloring if the data was labeled with rain. The first layer was randomly jittered, so I needed to preserve that randomness for the second layer. To accomplish this, I passed an argument for a random seed (i.e. seed = 123) to ensure that the jitter was reproducible. For more information on jittering in ggplot2, visit this link: https://ggplot2.tidyverse.org/reference/position_jitter.html These times set during a rainy qualifying session would complicate our understanding of a car’s progression over time. To avoid their influence, we can filter the rain-influenced times from Q2 and Q3 in 2014, and re-plot below. I’ll add a best-fit linear regression line that describes the improvement of qualifying times from 2014 to 2022, on average. qualifying_allyears %&gt;% filter(Race == &#39;australia&#39;) %&gt;% mutate(rain = ifelse(Year == 2014 &amp; Q_secs &gt; 100, &#39;rain during Q2&#39;, &#39; &#39;)) %&gt;% filter(rain == &#39; &#39;) %&gt;% ggplot(aes(Year, Q_secs)) + geom_point(position = position_jitter(seed= 123, h = 0, w = 0.3), alpha = 0.5) + stat_smooth(method = &#39;lm&#39;, se = F, size = 0.3, alpha = 0.5) + theme_bw() + labs(x = &#39;Year&#39;, y = &#39;Best Qualifying Time (secs)&#39;, title = &#39;Australian Grand Prix Qualifying&#39;, subtitle = &#39;2014 - 2023&#39;) + scale_x_continuous(breaks = c(2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022)) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) Adding a best-fit line in ggplot The stat_smooth() function in ggplot2 can help aid the eye in observing patterns in the data. If no method is declared in the function call, a loess (locally estimated scatter plot smoothing) function is fit to the data. In the example above, I add method = \"lm\" to the function call, which adds a linear model to the data. By default, stat_smooth() also adds a 95% confidence interval around the fit. In the example above, I disabled the confidence interval by adding se = FALSE to the call. For more information on best-fit lines, or smoothed conditional means, in ggplot2, visit this link: https://ggplot2.tidyverse.org/reference/geom_smooth.html While the stat_smooth() function allows us to easily add a linear regression line to the scatter plot, it doesn’t quantify the coefficient estimates. In the next chapter, we will explore using linear regression models to estimate the relationship between practice times, qualifying times, and more! 3.5 Next Chapter In the next chapter, I will explore the relationship between practice pace and qualifying pace. "],["modeling-practice-and-qualifying-times.html", "Chapter 4 Modeling Practice and Qualifying Times 4.1 Models 4.2 Simple Linear Regression Model: Practice vs Qualifying Times 4.3 Simple Linear Regression Model: Qualifying time vs Year 4.4 Interaction Model 4.5 Next Chapter", " Chapter 4 Modeling Practice and Qualifying Times As mentioned in previous chapters, some runs during practice sessions are used to simulate qualifying or race pace. For instance, FP1 is typically used to test the car and ensure it is working as expected, while FP2 and FP3 are often used to test the car’s performance on long runs and the car’s speed over laps, respectively. Knowing this, I would expect practice times to correlate to qualifying times. This chapter will take a deep-dive into that relationship and try to better understand it. Below, we can plot the best time set during FP1 versus the best time during Q1. For the most part, there is a near 1:1 ratio. However, there are several outlying clusters of times that don’t follow a 1:1 ratio. practice_and_qualifying %&gt;% ggplot(aes(Time_secs_1, Q1_secs)) + geom_point(alpha = 0.2) + theme_bw() + labs(x = &#39;Best Time during FP1 (secs)&#39;, y = &#39;Best Time during Q1 (secs)&#39;) One of these outlying clusters is data collected from the Malaysian Grand Prix in 2017. Free Practice 1 of the Malaysian Grand Prix in 2017 was delayed for half an hour due to heavy rain at the Sepang International Circuit. Max Verstappen eventually finished the wet FP1 session with a fastest time of 1:48.962. Qualifying was not impacted by rain, and Lewis Hamilton took pole with a time of 1:30.076. In the figure below, we can clearly see this relationship displayed by the flatter slope for the Malaysian Grand Prix in 2017. practice_and_qualifying %&gt;% mutate(malaysia_2017 = ifelse(Race == &#39;malaysia&#39; &amp; Year == 2017, &#39;Malaysia 2017&#39;, &#39;No&#39;)) %&gt;% ggplot(aes(Time_secs_1, Q1_secs)) + geom_point(aes(col = malaysia_2017), alpha = 0.5, show.legend = T) + theme_bw() + labs(x = &#39;Best Time during FP1 (secs)&#39;, y = &#39;Best Time during Q1 (secs)&#39;) + gghighlight(malaysia_2017 == &#39;Malaysia 2017&#39;, use_direct_label = FALSE) + scale_color_manual(&#39;&#39;, values = c(&#39;Malaysia 2017&#39; = &#39;black&#39;)) How to create a custom color scale in ggplot At times, creating custom color scales can be very convenient. The simplest way to create a custom color scale is by using scale_color_manual(). In the figure above, I used the following function to color data for Malaysia 2017 black: scale_color_manual('', values = c('Malaysia 2017' = 'black')) For more information on using scale_color_manual() in ggplot2, visit this link: https://ggplot2.tidyverse.org/reference/scale_manual.html I want to estimate the relationship between FP1 times and Q1 times, but these outlying clusters are compromising the relationship. In the Malaysia example above, rain was the root cause for a distinctly different slope (FP1 vs Q1). I want to remove these abnormal sessions, but it would be quite tedious to research all FP1 or Q1 sessions that were impacted by rain. As an alternative, I will utilize exploratory data analysis (EDA) and data wrangling to filter out abnormal times. For the vast majority of Grands Prix, the slope of this relationship (FP1 times vs Q1 times) is slightly larger than 1. I’ll use a histogram below to plot the distribution of ratios of all FP1:Q1 times. practice_and_qualifying %&gt;% mutate(ratio = Time_secs_1 / Q1_secs) %&gt;% ggplot(aes(ratio)) + geom_histogram(bins = 100, alpha = 0.5) + theme_bw() + labs(x = &#39;Ratio of FP1:Q1 Times&#39;, title = &#39;Distribution of all Ratios of FP1:Q1 Times&#39;, subtitle = &#39;2014 - 2022&#39;) + scale_x_continuous(limits = c(0.7, 1.3), breaks = c(0.7, 0.8, 0.9, 1, 1.1, 1.2, 1.3)) There are three separate peaks in this distribution: slightly larger than 1.0 just below 0.90 at approximately 1.15 Knowing this information, I will try to filter out any ratios lower than 0.95 or larger than 1.10. practice_and_qualifying %&gt;% mutate(ratio = Time_secs_1 / Q1_secs) %&gt;% filter(ratio &gt;= 0.95 &amp; ratio &lt;= 1.10) %&gt;% ggplot(aes(Time_secs_1, Q1_secs)) + geom_point(alpha = 0.2) + theme_bw() + labs(x = &#39;Best Time during FP1 (secs)&#39;, y = &#39;Best Time during Q1 (secs)&#39;) That looks much better! There’s still a few funky outliers, but I feel comfortable that this distribution represents the typical Formula 1 weekend. While this graph is some evidence to suggest that FP1 times are correlated to Q1 times, I actually want to quantify this relationship. So, next, I will use a simple linear regression model to estimate the relationship between FP1 and Q1 times. 4.1 Models Why build a model? At the most basic level, we are looking for relationships between variables in our dataset. Models are mathematical mappings between variables… in a rather structured way. People generally build models for: making inferences (i.e. using a model to answer research questions) making predictions or both inference and prediction An example of an inferential model could be a logistic regression model used to estimate the effect that smoking has on someone’s likelihood of developing cancer. An example of predictive model is an election forecaster. But, inferential models can still be used to make predictions, and sometimes the predictive models can make inferences. In this chapter, I want to use an inferential models to make general claims about the relationship between practice times and qualifying times. In the next section, I’ll introduce regression models and demonstrate how to use them to learn about Formula 1. 4.2 Simple Linear Regression Model: Practice vs Qualifying Times Dr. Andrew Gelman summarizes regression much more succinctly than I ever could: Regression is a method that allows researchers to summarize how predictions or average values of an outcome vary across individuals defined by a set of predictors. If you’ve ever created a scatterplot in Excel, the best-fit line is based on a linear regression between the variable on the x-axis and the variable on the y-axis. However, linear regression can be far more useful than a simple method to find the best-fit line. Throughout the rest of this chapter, I will introduce ways to summarize and communicate results from linear regression models. In this book, I will only briefly cover particularly important aspects of models. So, if you are in need of an introduction or refresher on regression, I highly recommend checking out the following introductory book on regression: More information about linear regression in R My favorite introductory book on regression in R is Regression and Other Stories by Andrew Gelman, Jennifer Hill, and Aki Vehtari. This book focuses on the practical application of regression models in R, and includes tons of very helpful code. While the authors build bayesian models, I think it is helpful for anyone looking to learn about regression. Here’s a link to the free book and lots of example code: https://avehtari.github.io/ROS-Examples/index.html The most simple linear regression model uses a single predictor variable. In our model, the single predictor is FP1 times. We will use linear regression model to fit a linear relationship (straight line) between the predictor variable (FP1 times) and the response variable (Q1 times). This model provides estimates of the intercept, slope, and error. The formula for this simple linear regression model is: \\[ Q1 \\ time = Intercept + Slope * FP1 \\ time + error \\] I will fit the model to the filtered dataset I created earlier (i.e. with ratios between 0.95 and 1.10). Below, I’ll re-write this data to a new dataframe. fp1_v_q1_clean &lt;- practice_and_qualifying %&gt;% mutate(ratio = Time_secs_1 / Q1_secs) %&gt;% filter(ratio &gt;= 0.95 &amp; ratio &lt;= 1.10) To fit a linear regression model to this data, I will use the lm() function. # Fir the linear regression model fp1_v_q1.model &lt;- lm(Q1_secs ~ Time_secs_1, data = fp1_v_q1_clean) How to fit a simple linear regression model in R To fit a simple linear regression model in R, we can use the lm() function. The lm() function takes uses the following notation: lm(response variable ~ predictor variable(s), data source) For more information on using lm(), visit this link: https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/lm I will now use the summary() function to summarize the output from the linear model. summary(fp1_v_q1.model) ## ## Call: ## lm(formula = Q1_secs ~ Time_secs_1, data = fp1_v_q1_clean) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.9836 -0.5859 0.0210 0.6408 8.3919 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.610673 0.163619 9.844 &lt;2e-16 *** ## Time_secs_1 0.958841 0.001828 524.432 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.295 on 3200 degrees of freedom ## Multiple R-squared: 0.9885, Adjusted R-squared: 0.9885 ## F-statistic: 2.75e+05 on 1 and 3200 DF, p-value: &lt; 2.2e-16 The summary output from this model contains a lot of important information about this model fit. The following list gives a bit more information about the most noteworthy items in this summary output: The model formula is found under Call. The quantiles of the residuals are listed below the model formula. the coefficient values are the model estimates for intercept and slope. Each coefficient also has a standard error, t-values, and statistical significance. The residual standard error degrees of freedom R-squared F-statistic p-value In this particular model, the Intercept is somewhat meaningless. The intercept describes the value of the response (Q1 times) when the predictor variable (FP1 time) is = 0. However, in this example, there is no situation where FP1 is = 0. We can interpret the slope as: A driver’s Q1 time is expected to be about 0.036 seconds faster than their best time during Free Practice 1, on average. I arrived at 0.036 seconds by subtracting the slope from 1. The R2 is another important metric from this output. The R2 value, also known as the coefficient of determination, is the proportion of the variance in Q1 times that are explained by FP1 time. In this case, the R2 = 0.989, which can be interpreted as ~ 98.9% of the variance in a driver’s Q1 time can be explained by their FP1 time. The F-statistic is of little use to us in this case. The F-statistic is the test statistic for the F-test, which evaluates whether this particular model provides a better fit to our data than a model that uses no predictor variables. If the p-value is less than 0.05 (which it is), we conclude that this model is better than a model using no predictor variables. But, we already knew this linear relationship was legit. And, with really large datasets like this one, statistical significance can be a dubious concept. For our purposes, we should be most concerned with the R2 and the slope of this model. The R2 explains the amount of variance that our model explains, while the slope describes how FP1 times and Q1 times related. We can apply this same approach to a different comparison: FP3 vs fastest qualifying time. Because FP3 is used for qualifying simulations, it may be more representative of a car’s true qualifying pace. Using the fastest time in qualifying is likely a better measure because front-running teams likely take less risks during Q1. fp3_v_q_clean &lt;- practice_and_qualifying %&gt;% mutate(ratio = Time_secs_3 / Q_secs) %&gt;% filter(ratio &gt;= 0.95 &amp; ratio &lt;= 1.10) # Fir the linear regression model fp3_v_q.model &lt;- lm(Q_secs ~ Time_secs_3, data = fp3_v_q_clean) summary(fp3_v_q.model) ## ## Call: ## lm(formula = Q_secs ~ Time_secs_3, data = fp3_v_q_clean) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.1310 -0.3914 0.0386 0.5222 6.5447 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.210783 0.139942 8.652 &lt;2e-16 *** ## Time_secs_3 0.973464 0.001576 617.569 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.194 on 3584 degrees of freedom ## Multiple R-squared: 0.9907, Adjusted R-squared: 0.9907 ## F-statistic: 3.814e+05 on 1 and 3584 DF, p-value: &lt; 2.2e-16 practice_and_qualifying %&gt;% mutate(ratio = Q_secs / Time_secs_3) %&gt;% filter(ratio &gt;= 0.95 &amp; ratio &lt;= 1.10) %&gt;% ggplot(aes(Time_secs_3, Q_secs)) + geom_point(alpha = 0.2) + theme_bw() + labs(x = &#39;Best Time during FP3 (secs)&#39;, y = &#39;Best Time during Qualifying (secs)&#39;) In this FP3 model, we can interpret the slope as: A driver’s best time in qualifying is expected to be about 0.012 seconds faster than their best time during Free Practice 3, on average. Again, I arrived at 0.012 seconds by subtracting the slope from 1. This FP3 model explains fractionally more variance than the previous FP1 model (the R2 = 0.992 vs 0.989). So, FP3 times explain about 99.2% of the variance in a driver’s best qualifying time. Pretty interesting! Next, we’ll use a linear regression model to estimate the progression of qualifying times over time (i.e. by year). 4.3 Simple Linear Regression Model: Qualifying time vs Year In the last chapter, I created this figure to show the progression of qualifying times at the Australian Grand Prix. practice_and_qualifying %&gt;% filter(Race == &#39;australia&#39;) %&gt;% mutate(rain = ifelse(Year == 2014 &amp; Q_secs &gt; 100, &#39;rain during Q2&#39;, &#39; &#39;)) %&gt;% filter(rain == &#39; &#39;) %&gt;% ggplot(aes(Year, Q_secs)) + geom_point(position = position_jitter(seed= 123, h = 0, w = 0.3), alpha = 0.5) + stat_smooth(method = &#39;lm&#39;, se = F, size = 0.3, alpha = 0.5) + theme_bw() + labs(x = &#39;Year&#39;, y = &#39;Best Qualifying Time (secs)&#39;, title = &#39;Australian Grand Prix Qualifying&#39;, subtitle = &#39;2014 - 2023&#39;) + scale_x_continuous(breaks = c(2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023)) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) I used the stat_smooth() function to fit a best-fit line through the data. But, this was just a visual aid and I didn’t actually quantify the relationship between year and qualifying time. To quantify this relationship, a simple linear regression model can be used. The formula for this model is simply: \\[Qualifying \\ time \\sim intercept \\ + \\ Year \\ + \\ \\epsilon\\] In the code below, I will: filter the data to include only the Australian Grand Prix remove sessions that were impacted by rain. I then fit a simple linear regression model using the lm() function. # filter times to include only clean Australian data aus_quali_times &lt;- practice_and_qualifying %&gt;% filter(Race == &#39;australia&#39;) %&gt;% mutate(rain = ifelse(Year == 2014 &amp; Q_secs &gt; 100, &#39;rain during Q2&#39;, &#39; &#39;)) %&gt;% filter(rain == &#39; &#39;, !is.na(Q_secs)) # fit linear model aus.lm &lt;- lm(Q_secs ~ Year, aus_quali_times) Below is a summary of the model output. summary(aus.lm) ## ## Call: ## lm(formula = Q_secs ~ Year, data = aus_quali_times) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.5408 -1.0523 -0.0226 0.7590 7.0584 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2913.43233 106.09863 27.46 &lt;2e-16 *** ## Year -1.40194 0.05257 -26.67 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.767 on 141 degrees of freedom ## Multiple R-squared: 0.8346, Adjusted R-squared: 0.8334 ## F-statistic: 711.3 on 1 and 141 DF, p-value: &lt; 2.2e-16 Let’s try to interpret some of this output. This model’s fitted line equation is simply: \\[ time_Q \\sim 2913 - 1.40 * year + \\epsilon\\] Like before, I am most interested in the coefficient estimates, residual standard error, and R2. intercept: For this particular model, the intercept reported in this ouput isn’t immediately useful. It tells us the estimated mean qualifying time for the year zero. A year zero does not exist in the Anno Domini calendar year system (the year 1 BC is followed directly by year AD 1), so not helpful! However, we can rescale it to the year 2014 and interpret the value like this: The average qualifying time at the Australian Grand Prix during the initial year of the hybrid era (2014) was 93.4 seconds (2913 + (-1.40 x 2014). Slope: Since the beginning of the hybrid era (2014), qualifying times in Australia decrease by 1.40 seconds per year, on average. The R2 value (coefficient of determination) is the proportion of the variance in Australian qualifying times that can be explained by yearly progression. In this case, the R2 = 0.83, which can be interpreted as: 83% of the variance in qualifying times can be explained by the year. An approximate interpretation of the residual standard error/deviation (RSE) is: A year’s qualifying times will deviate from the linear regression model fit line by 1.77 seconds, on average. In the following section, I will discuss uncertainty in our model. 4.3.1 Uncertainty in model estimates In the last section, I described the slope point estimate as: Since the beginning of the hybrid era (2014), qualifying times in Australia decrease by 1.40 seconds per year, on average. But, how sure am I of this number? A confidence interval allows us to also describe the uncertainty in that estimate. Below, I’ll calculate the 95% confidence interval around the slope estimate, and interpret it. confint(aus.lm) ## 2.5 % 97.5 % ## (Intercept) 2703.682609 3123.182046 ## Year -1.505856 -1.298015 95% Confidence Interval around the Slope: Since the beginning of the hybrid era (2014), qualifying times in Australia decrease by 1.40 seconds per year, on average. We have 95% confidence that the true average decrease in Australian qualifying times per year is between 1.30 and 1.51 seconds, on average. Let’s revisit the earlier plot of Australian Grand Prix qualifying times, with a best-fit line included. This line describes the linear regression model’s estimate of average qualifying time for each year of the race. Like we did with our slope estimate, we should describe our uncertainty around this line using a confidence and prediction interval. The confidence interval is uncertainty surrounding a mean response, and the prediction interval is uncertainty surrounding a prediction of a single future observation. The interpretation of the 95% confidence interval of a predicted value is: “95% of intervals of this form will contain the expected value of average qualifying time given a particular year.” The interpretation of the 95% prediction interval of a predicted value is: “95% of intervals of this form will contain the true qualifying time for this particular year.” Below, I will plot the 95% confidence interval around the fit. library(broom) # calculate the 95% confidence interval for the fit line aus_aug &lt;- augment(aus.lm, data = aus_quali_times, interval = &quot;confidence&quot;, se_fit = T) aus_aug %&gt;% ggplot(aes(Year, Q_secs)) + geom_point(position = position_jitter(seed= 123, h = 0, w = 0.3), alpha = 0.5) + stat_smooth(method = &#39;lm&#39;, se = F, size = 0.3, alpha = 0.5) + theme_bw() + labs(x = &#39;Year&#39;, y = &#39;Best Qualifying Time (secs)&#39;, title = &#39;Australian Grand Prix Qualifying&#39;, subtitle = &#39;Linear Model, 95% CI (2014 - 2023)&#39;) + scale_x_continuous(breaks = c(2014, 2015, 2016, 2017, 2018, 2019, 2022, 2023)) + geom_ribbon(aes(ymin = .lower, ymax = .upper), fill = &#39;grey&#39;, alpha = 0.5) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) The augment() function from the broom package Augment accepts a model object and a dataset and adds information about each observation in the dataset. One of many conveniences provided by augment, is the ability to attach 95% confidence or prediction intervals to a dataframe. In the example above, I use the following code to attach a predicted value and the 95% confidence interval to the Australia qualifying dataset: augment(aus.lm, data = aus_quali_times, interval = \"confidence\", se_fit = T) For more information on using augment(), visit this link: https://broom.tidymodels.org/reference/augment.lm.html And here, I will also include the 95% prediction interval. # calculate the 95% prediction interval for the fit line aus_aug_pr &lt;- augment(aus.lm, data = aus_quali_times, interval = &quot;prediction&quot;, se_fit = T) %&gt;% rename(&quot;lower_PI&quot; = &quot;.lower&quot;, &quot;upper_PI&quot; = &quot;.upper&quot;) # combine the intervals aus_aug &lt;- aus_aug %&gt;% bind_cols(aus_aug_pr %&gt;% dplyr::select(lower_PI, upper_PI)) aus_aug %&gt;% ggplot(aes(Year, Q_secs)) + geom_point(position = position_jitter(seed= 123, h = 0, w = 0.3), alpha = 0.5) + stat_smooth(method = &#39;lm&#39;, se = F, size = 0.3, alpha = 0.5) + theme_bw() + labs(x = &#39;Year&#39;, y = &#39;Best Qualifying Time (secs)&#39;, title = &#39;Australian Grand Prix Qualifying&#39;, subtitle = &#39;Linear Model, 95% CI, and 95% PI (2014 - 2023)&#39;) + scale_x_continuous(breaks = c(2014, 2015, 2016, 2017, 2018, 2019, 2022, 2023)) + geom_ribbon(aes(ymin = lower_PI, ymax = upper_PI), fill = &#39;grey&#39;, alpha = 0.25) + geom_ribbon(aes(ymin = .lower, ymax = .upper), fill = &#39;grey&#39;, alpha = 0.5) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) Creating a prediction interval using the augment() function It’s just as easy to create a 95% prediction interval using augment. In the example above, I use the following code to attach the 95% prediction interval to the Australia qualifying dataset: augment(aus.lm, data = aus_quali_times, interval = \"prediction\", se_fit = T) %&gt;% rename(\"lower_PI\" = \".lower\", \"upper_PI\" = \".upper\") To avoid confusion, I renamed the lower and upper bounds of this prediction interval lower_PI and upper_PI respectively. For more information on using augment(), visit this link: https://broom.tidymodels.org/reference/augment.lm.html As another reminder: The confidence interval is uncertainty surrounding a mean response, and the prediction interval is uncertainty surrounding a prediction of a future observation. Or, in our racing example above: A confidence interval around the fit describes the uncertainty in predicting an average time for a given year, while the prediction interval describes the uncertainty in predicting any single time for a particular year. In the next section, I will build on these ideas and add complexity to this simple linear regression model. 4.4 Interaction Model I would like to now expand my question a bit. How does qualifying time progression compare between the Australian Grand Prix and another race? To begin answering this question, I can take a similar modeling approach that I used for Australia, but apply it to another race. Let’s take a look at the Monaco Grand Prix. I’ll run through the same modeling steps below. practice_and_qualifying %&gt;% filter(Race == &#39;monaco&#39;) %&gt;% ggplot(aes(Year, Q_secs)) + geom_point(position = position_jitter(h = 0, w = 0.3), alpha = 0.5) + theme_bw() + labs(x = &#39;Year&#39;, y = &#39;Best Qualifying Time (secs)&#39;, title = &#39;Monaco Grand Prix Qualifying&#39;, subtitle = &#39;2014 - 2023&#39;) + scale_x_continuous(breaks = c(2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023)) Interesting! We did not have data for Australia in 2020 and 2021 (due to the pandemic). In 2022, the new ground-effects regulations were introduced. Because we were missing data for Australia in 2020 and 2021, our model assumed a constant decrease in qualifying times. Compare this result to Monaco, where we did have data for 2021. Progression does appear to steadily decrease through 2021, but times then increase in 2022 as the new regulations are introduced. practice_and_qualifying %&gt;% filter(Race %in% c(&#39;australia&#39;, &#39;monaco&#39;)) %&gt;% ggplot(aes(Year, Q_secs, col = Race)) + geom_point(position = position_jitter(h = 0, w = 0.3), alpha = 0.5) + theme_bw() + labs(x = &#39;Year&#39;, y = &#39;Best Qualifying Time (secs)&#39;, title = title_color_coder(&quot;&quot;, &quot;Australia&quot;, &#39;#F8766D&#39;, &quot; vs &quot;, &quot;Monaco&quot;, &#39;#00BFC4&#39; ,&quot; Qualifying Time Progression&quot;), subtitle = &#39;2014 - 2023&#39;, col = &#39;&#39;) + # use the title_color_coder() function facet_wrap(~ Race, scales= &#39;free_y&#39;) + scale_x_continuous(breaks = c(2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023)) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), legend.position = &quot;none&quot;) + theme(strip.text.x = element_text(size = 12), axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1, size = 12), axis.title.x = element_text(size = 15), axis.title.y = element_text(size = 15), plot.subtitle = element_text(hjust = 0.5, size = 15), plot.caption = element_text(hjust = 0, size = 12), plot.title = ggtext::element_markdown(hjust = 0.5, size = 20)) ## render the provided text as markdown/html To quantitatively compare these two progression trends, we can build linear models for each race and compare the output. We should probably remove 2014 data (because Australia times were compromised by the rain), and 2021 data (because the Australian Grand Prix wasn’t held). And, it is probably wise to limit the data to one set of regulations (pre-2022). Below, I’ll re-plot the scatterplot with best-fit linear model lines for each race under these updated conditions. practice_and_qualifying %&gt;% filter(Race %in% c(&#39;australia&#39;, &#39;monaco&#39;), Year &gt; 2014 &amp; Year &lt; 2020) %&gt;% ggplot(aes(Year, Q_secs, col = Race)) + geom_point(position = position_jitter(h = 0, w = 0.3), alpha = 0.5) + stat_smooth(method = &#39;lm&#39;, se = F, alpha = 0.5, size = 0.3) + theme_bw() + labs(x = &#39;Year&#39;, y = &#39;Best Qualifying Time (secs)&#39;, title = title_color_coder(&quot;&quot;, &quot;Australia&quot;, &#39;#F8766D&#39;, &quot; vs &quot;, &quot;Monaco&quot;, &#39;#00BFC4&#39; ,&quot; Qualifying Time Progression&quot;), subtitle = &#39;2015 - 2019&#39;, col = &#39;&#39;) + # use the title_color_coder() function facet_wrap(~ Race, scales= &#39;free_y&#39;) + scale_x_continuous(breaks = c(2014, 2015, 2016, 2017, 2018, 2019)) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), legend.position = &quot;none&quot;) + theme(strip.text.x = element_text(size = 12), axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1, size = 12), axis.title.x = element_text(size = 15), axis.title.y = element_text(size = 15), plot.subtitle = element_text(hjust = 0.5, size = 15), plot.caption = element_text(hjust = 0, size = 12), plot.title = ggtext::element_markdown(hjust = 0.5, size = 20)) ## render the provided text as markdown/html With the naked eye, it looks like Australia’s slope is steeper, suggesting that times improved at a quicker rate relative to Monaco. But, we should calculate the slope estimates and the 95% confidence intervals around the slope estimate to quantitatively compare. # filter times to include only clean Australian data aus_quali_times &lt;- practice_and_qualifying %&gt;% filter(Race == &#39;australia&#39;) %&gt;% filter(!is.na(Q_secs), Year &gt; 2014 &amp; Year &lt; 2020) # fit linear model for Australia aus.lm &lt;- lm(Q_secs ~ Year, aus_quali_times) # 95% CI slope aus_coef_ci &lt;- confint(aus.lm) # filter times to include only clean Australian data mon_quali_times &lt;- practice_and_qualifying %&gt;% filter(Race == &#39;monaco&#39;) %&gt;% filter(!is.na(Q_secs), Year &gt; 2014 &amp; Year &lt; 2020) # fit linear model for Monaco mon.lm &lt;- lm(Q_secs ~ Year, data = mon_quali_times) # 95% CI slope mon_coef_ci &lt;- confint(mon.lm) I will plot the slopes ± 95% confidence intervals here: aus_v_mon &lt;- cbind(data.frame(australia = cbind(est = coef(aus.lm), aus_coef_ci)[2,]), data.frame(monaco = cbind(est = coef(mon.lm), mon_coef_ci)[2,])) %&gt;% t() %&gt;% as.data.frame() %&gt;% rownames_to_column(var = &#39;race&#39;) aus_v_mon %&gt;% ggplot() + geom_pointrange(aes(x = race, y = est, ymin = `2.5 %`, ymax = `97.5 %`, col = race), linewidth = 3, alpha = 0.5, size = 1) + theme_bw() + scale_color_manual(&#39;&#39;, values = c(&quot;australia&quot; = &#39;#F8766D&#39;, &quot;monaco&quot; = &#39;#00BFC4&#39;)) + labs(x = &#39;Grand Prix&#39;, y = &#39;Coefficient Estimate \\u00b1 95% CI&#39;, title = title_color_coder(&quot;&quot;, &quot;Australia&quot;, &#39;#F8766D&#39;, &quot; vs &quot;, &quot;Monaco&quot;, &#39;#00BFC4&#39; ,&quot; Qualifying Time Progression&quot;), subtitle = &#39;2015 - 2019&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), legend.position = &quot;none&quot;) + theme(plot.title = ggtext::element_markdown()) ## render the provided text as markdown/html So, while my initial guess that qualifying times decreased at a more rapid rate in Australia, the 95% confidence intervals for the slope estimates largely overlap. This suggests that there’s little to no statistical evidence that these slopes are in fact different. But, a more proper way to make this comparison is by using an interaction model. It can answer the question that we had (i.e. Was qualifying time progression different between the two races?), and it can do a lot more. Below, I’ll fit an interaction model to the 2015 - 2019 data for the Australian and Monaco Grands Prix. # filter data aus_mon_data &lt;- practice_and_qualifying %&gt;% filter(Race %in% c(&#39;australia&#39;, &#39;monaco&#39;), !is.na(Q_secs), Year &gt; 2014 &amp; Year &lt; 2020) %&gt;% mutate(Race = factor(Race)) # fit an interaction model aus.mon.int.lm &lt;- lm(Q_secs ~ Year:Race, data = aus_mon_data) # print the model output summary(aus.mon.int.lm) ## ## Call: ## lm(formula = Q_secs ~ Year:Race, data = aus_mon_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.0609 -0.9699 -0.1678 0.7731 9.2456 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3236.76129 166.49551 19.44 &lt;2e-16 *** ## Year:Raceaustralia -1.56248 0.08255 -18.93 &lt;2e-16 *** ## Year:Racemonaco -1.56793 0.08255 -18.99 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.638 on 197 degrees of freedom ## Multiple R-squared: 0.9292, Adjusted R-squared: 0.9285 ## F-statistic: 1294 on 2 and 197 DF, p-value: &lt; 2.2e-16 At first, the output may seem a bit confusing, but it only requires a small adjustment. This model produces coefficient estimates for a race-specific intercept and slope, relative to the ‘base-level’. In this case, Australia is the base level. Luckily, we can actually use functions from the emmeans package to easily estimate the coefficient and confidence intervals by race. emmeans stands for estimated marginal means, which are means for treatment levels that are adjusted for means of other factors in the model. In this example, we are interested in the influence of year on qualifying times, which is also impacted by the circuit. I will use the handy emtrends() function to tidy up this model’s results. emtrends(aus.mon.int.lm, pairwise ~ Race, var = &#39;Year&#39;) %&gt;% as.data.frame() %&gt;% filter(Race %in% c(&#39;australia&#39;, &#39;monaco&#39;)) %&gt;% ggplot() + geom_pointrange(aes(x = Race, y = Year.trend, ymin = lower.CL, ymax = upper.CL, col = Race), linewidth = 3, alpha = 0.5, size = 1) + theme_bw() + scale_color_manual(&#39;&#39;, values = c(&quot;australia&quot; = &#39;#F8766D&#39;, &quot;monaco&quot; = &#39;#00BFC4&#39;)) + labs(x = &#39;Grand Prix&#39;, y = &#39;Coefficient Estimate \\u00b1 95% CI&#39;, title = title_color_coder(&quot;&quot;, &quot;Australia&quot;, &#39;#F8766D&#39;, &quot; vs &quot;, &quot;Monaco&quot;, &#39;#00BFC4&#39; ,&quot; Qualifying Time Progression&quot;), subtitle = &#39;Interaction Model: 2015 - 2019&#39;) + theme(legend.position = &quot;none&quot;) + theme( plot.subtitle = element_text(hjust = 0.5), plot.title = ggtext::element_markdown(hjust = 0.5)) ## render the provided text as markdown/html the emmeans package Estimated marginal means (EMMs), a.k.a. least-squares means, are predictions on a reference grid of predictor settings, or marginal averages. To estimate the marginal means of a model, simply run emmeans(model, pairwise ~ treatment). However, more thought should go into the use of this function. This is particularly true for more complicated models. The emtrends() function is useful when a fitted model involves a numerical predictor x interacting with another predictor a (typically a factor). This si the case with our interaction model where we have year (numeric) interacting with race (factor). For more information on using the emmeans package, visit these links: https://cran.r-project.org/web/packages/emmeans/vignettes/comparisons.html https://rdrr.io/cran/emmeans/f/vignettes/FAQs.Rmd https://rdrr.io/cran/emmeans/man/emtrends.html While the estimates for the slope remain the same to those estimated with individual models, the 95% confidence intervals are narrower for this interaction model. Nonetheless, it doesn’t change our opinion that the rate of qualifying time improvement is not statistically different between these two races during this time period (2015 - 2019). As we did for a single race earlier, we can use the augment() function to estimate the 95% confidence and 95% prediction intervals for this interaction model. # calculate the 95% confidence interval for the fit line aus_v_mon_aug &lt;- augment(aus.mon.int.lm, data = aus_mon_data, interval = &quot;confidence&quot;, se_fit = T) # calculate the 95% prediction interval for the fit line aus_mon_aug_pr &lt;- augment(aus.mon.int.lm, data = aus_mon_data, interval = &quot;prediction&quot;, se_fit = T) %&gt;% rename(&quot;lower_PI&quot; = &quot;.lower&quot;, &quot;upper_PI&quot; = &quot;.upper&quot;) # combine the intervals aus_v_mon_aug &lt;- aus_v_mon_aug %&gt;% bind_cols(aus_mon_aug_pr %&gt;% dplyr::select(lower_PI, upper_PI)) aus_v_mon_aug %&gt;% ggplot(aes(Year, Q_secs, col = Race), show.legend = F) + geom_point(position = position_jitter(seed= 123, h = 0, w = 0.3), alpha = 0.5) + stat_smooth(method = &#39;lm&#39;, se = F, size = 0.3, alpha = 0.5) + theme_bw() + labs(x = &#39;Year&#39;, y = &#39;Best Qualifying Time (secs)&#39;, title = title_color_coder(&quot;&quot;, &quot;Australia&quot;, &#39;#F8766D&#39;, &quot; vs &quot;, &quot;Monaco&quot;, &#39;#00BFC4&#39; ,&quot; Qualifying Time Progression&quot;), subtitle = &#39;Linear Model, 95% CI, and 95% PI (2015 - 2019)&#39;) + scale_x_continuous(breaks = c(2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023)) + geom_ribbon(aes(ymin = lower_PI, ymax = upper_PI), fill = &#39;grey&#39;, alpha = 0.25) + geom_ribbon(aes(ymin = .lower, ymax = .upper), fill = &#39;grey&#39;, alpha = 0.5) + theme(legend.position = &quot;none&quot;) + theme(strip.text.x = element_text(size = 12), axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1, size = 12), axis.title.x = element_text(size = 15), axis.title.y = element_text(size = 15), plot.subtitle = element_text(hjust = 0.5, size = 15), plot.caption = element_text(hjust = 0, size = 12), plot.title = ggtext::element_markdown(hjust = 0.5, size = 20)) + ## render the provided text as markdown/html facet_wrap(~ Race) This visual helps highlight that these slopes are not statistically different from each other. As a reminder, each Grand Prix’s line describes the linear regression model’s estimate of average qualifying time for each year of the race. The inner band represents the 95% confidence interval of the prediction (confidence in the average response), and the outer band represents the 95% prediction interval of the prediction (confidence in a single response). And for yet another reminder…. The confidence interval is uncertainty surrounding a mean response, and the prediction interval is uncertainty surrounding a prediction of a future observation. The confidence interval width is driven by our uncertainty in the coefficient estimates. The prediction interval width is driven by both the uncertainty in the coefficient estimates and unmodeled variance. So, even if we had a very large sample size and were very confident in our coefficient estimates, our predicted values would still vary by the residual standard error of the model (i.e. the RSE in the summary() output). Residual Standard Error (RSE) The Residual Standard Error (RSE) is the standard deviation of the residuals for a model. It is commonly used as a measure of how well a regression model fits a dataset. In the interaction model above, the RSE is 1.587, meaning that the actual qualifying time with deviate from the true regression line by about 1.587 seconds, on average. So, a lose interpretation of RSE is: RSE, is the average amount that the response will deviate from the true regression line. 4.5 Next Chapter In the next chapter, we will look at driver performance. "],["drivers.html", "Chapter 5 Drivers 5.1 Race Performance 5.2 Qualifying Pace", " Chapter 5 Drivers What separates the great drivers from the merely good drivers? There are only 20 Formula 1 seats available in the world, and only the best even get considered for the job. A driver needs talent, money, an FIA super license, and a lot of luck to ever start a Grand Prix. And despite the incredibly tough filtering mechanisms for talent in F1, there remain a few outliers a few drivers that stand apart. In his autobiography Total Competition, the legendary team principal Ross Brawn described how Michael Schumacher, perhaps the greatest driver of all time, simply had more capacity available because of his talent. Brawn felt that Michael possessed so much talent for driving that he was free to think through more things while driving. Team principals are obviously in a great position to judge the talent of drivers, and each year, www.formula1.com publishes the results from a team principal survey of the top drivers in the sport. Here’s what the results from the past three years looks like (including only drivers that drove in 2023): # Create a dataframe with yearly rankings team_principal_survey &lt;- data.frame(Driver = c(&#39;VER&#39;, &#39;ALO&#39;, &#39;NOR&#39;, &#39;LEC&#39;, &#39;HAM&#39;, &#39;SAI&#39;, &#39;PIA&#39;, &#39;ALB&#39;, &#39;RUS&#39;, &#39;PER&#39;)) %&gt;% mutate(`2023` = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10), `2022` = c(1, 8, 6, 2, 4, 7, NA, NA, 3, 5), `2021` = c(1, 6, 3, 5, 2, 4, NA, NA, 8, NA)) # Plot the results team_principal_survey %&gt;% pivot_longer(-Driver, names_to = &#39;Year&#39;, values_to = &#39;Rank&#39;) %&gt;% ggplot(aes(Year, Rank, col = Driver, group = Driver)) + geom_line(size = 2) + geom_point(size = 3) + theme_bw() + scale_y_reverse(labels = seq(1, 10, 1), breaks = seq(1, 10, 1)) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), legend.position = &#39;none&#39;) + labs(title = &#39;Top Drivers&#39;, subtitle = &#39;based on team principal rankings&#39;) + geom_text_repel(data = ~ subset(.x, Year == 2023), aes(label = Driver), nudge_x = 0.1, nudge_y = 0, show.legend = F) + scale_color_manual(values = c(&quot;red&quot;, &quot;red&quot;, &quot;cyan3&quot;, &quot;cyan3&quot;, &quot;blue&quot;, &quot;darkslateblue&quot;, &quot;darkred&quot;, &quot;darkseagreen4&quot;, &quot;darkseagreen4&quot;, &quot;darkorange&quot;, &quot;darkorange&quot;, &quot;cornflowerblue&quot;, &quot;navy&quot;, &quot;navy&quot;), breaks = c(&quot;LEC&quot;, &quot;SAI&quot;, &quot;HAM&quot;, &quot;RUS&quot;, &quot;OCO&quot;, &quot;GAS&quot;, &quot;BOT&quot;, &quot;ALO&quot;, &quot;VET&quot;, &quot;NOR&quot;, &quot;PIA&quot;, &quot;ALB&quot;, &quot;PER&quot;, &quot;VER&quot; )) Clearly team principals think highly of the reigning world champion Max Verstappen. But, how do the team principals determine these rankings? I’m not entirely sure. Driver skill is a surprisingly difficult trait to describe and understand. What are the essential skills of a driver? And how do you measure those skills? Well, most agree that the best drivers share the following traits: consistency adaptability qualifying pace race pace tire management an understanding of the car’s limits and how to extract all of the performance from a car performance in wet conditions Are these traits measurable? Not really (at least for the common fan). While I wish there was some data available that described these driver traits, I am afraid we are limited to merely analyzing driver performance. In this chapter, I’ll use the drs package to pull data from www.formula1.com and visualize driver performance during qualifying and the race with ggplot. The most obvious and simplest place to start is to plot race results by driver and circuit. Later on in the chapter, we can also look at each driver’s qualifying pace compared to (A) the rest of the grid and (B) their teammate. The best drivers in history consistently out-qualify other team’s top drivers, and more importantly, their own teammates. 5.1 Race Performance I find heat maps an enjoyable way to visualize race results. So, throughout this section, I’ll create heat maps for each year from 2014 to 2023. I will start with 2023, whcih was effectively the Max Verstappen show. Max won his third world title and Red Bull collected their 2nd consecutive (6th total) constructors championship. From the very first race, Red Bull appeared to be nearly unbeatable. And ultimately, Max completed arguably the most dominant season of all time by winning 19 of 22 races and finishing on the podium in all but one Grand Prix. In 2023, Max had a winning percentage of 86.36%, eclipsing the previous long-standing record set by Alberto Ascari in 1952. library(drs) library(ggthemes) # Pull race results races2023 &lt;- race_result_scraper(2023) races2023 %&gt;% mutate( n = 1:n(), Race = fct_reorder(Race, desc(n))) %&gt;% group_by(Race) %&gt;% mutate(Race_number = cur_group_id()) %&gt;% group_by(Driver) %&gt;% mutate(sum_pts = sum(Points, na.rm = T)) %&gt;% ungroup() %&gt;% ggplot(aes(fct_reorder(Driver, desc(sum_pts)), Race, fill = Points)) + geom_tile(color=&quot;white&quot;, size=0.1, alpha = 0.75) + theme_bw() + labs(title = &#39;Points scored during the 2023 season&#39;, y = &#39;Race&#39;, x = &#39;Driver&#39;) + theme_tufte(base_family=&quot;Helvetica&quot;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), legend.position=&quot;bottom&quot;, axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + scale_fill_viridis_c(option = &#39;magma&#39;, guide = &#39;legend&#39;, breaks = c(0,1,2,4,6,8,10,12,15,18,25,26)) The 2022 Formula 1 season was also dominated by Red Bull. Max Verstappen secured the Driver’s Championship by the 18th race of the season, the Japanese Grand Prix. However, Red Bull’s 2022 season opened with some uncertainty after experiencing 3 retirements in first two races. Both Max Verstappen and Sergio Pérez retired from the Bahrain Grand Prix in the closing laps with fuel issues. In the Australian Grand Prix, Verstappen again retired with fuel issues. Ultimately, Max finished 2022 with 15 wins and 454 points, the most in F1 history (at the time). Charles LeClerc took 2nd (308 points) and Sergio Perez took 3rd (305 points) in the Driver’s Championship. # Pull race results races2022 &lt;- race_result_scraper(2022) races2022 %&gt;% mutate( n = 1:n(), Race = fct_reorder(Race, desc(n))) %&gt;% group_by(Race) %&gt;% mutate(Race_number = cur_group_id()) %&gt;% group_by(Driver) %&gt;% mutate(sum_pts = sum(Points, na.rm = T)) %&gt;% ungroup() %&gt;% ggplot(aes(fct_reorder(Driver, desc(sum_pts)), Race, fill = Points)) + geom_tile(color=&quot;white&quot;, size=0.1, alpha = 0.75) + theme_bw() + labs(title = &#39;Points scored during the 2022 season&#39;, y = &#39;Race&#39;, x = &#39;Driver&#39;) + theme_tufte(base_family=&quot;Helvetica&quot;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), legend.position=&quot;bottom&quot;, axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + scale_fill_viridis_c(option = &#39;magma&#39;, guide = &#39;legend&#39;, breaks = c(0,1,2,4,6,8,10,12,15,18,25,26)) How to create a heat map in ggplot To create a heap map in ggplot2, use the geom_tile() geometry. You will need to pass categorical variables to x and y arguments and the continuous color fill variable to the fill argument. For more information on using geom_tile() in ggplot2, visit this link: https://plotly.com/ggplot2/geom_tile/ The 2021 Formula 1 season was a modern classic! There was a year-long battle between Red Bull and Mercedes (the reigning 7-time World Constructor’s Champions). Max Verstappen and Lewis Hamilton (the reigning and 7-time World Driver’s Champion) combined to win 18 out of 22 Grands Prix in 2021 (Verstappen with 10 wins, and Hamilton with 8 wins). Both drivers entered the final race of the season (the Abu Dhabi Grand Prix) tied on points. This final race, and the 2021 Driver’s Championship, was decided on the final lap of the season when Max Verstappen overtook Hamilton following a safety car restart. # Pull race results races2021 &lt;- race_result_scraper(2021) races2021 %&gt;% mutate( n = 1:n(), Race = fct_reorder(Race, desc(n))) %&gt;% group_by(Race) %&gt;% mutate(Race_number = cur_group_id()) %&gt;% group_by(Driver) %&gt;% mutate(sum_pts = sum(Points, na.rm = T)) %&gt;% ungroup() %&gt;% ggplot(aes(fct_reorder(Driver, desc(sum_pts)), Race, fill = Points)) + geom_tile(color=&quot;white&quot;, size=0.1, alpha = 0.75) + theme_bw() + labs(title = &#39;Points scored during the 2021 season&#39;, y = &#39;Race&#39;, x = &#39;Driver&#39;) + theme_tufte(base_family=&quot;Helvetica&quot;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), legend.position=&quot;bottom&quot;, axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + scale_fill_viridis_c(option = &#39;magma&#39;, guide = &#39;legend&#39;, breaks = c(0,1,2,4,6,8,10,12,15,18,25,26)) Using a pretty theme for a heat map in ggplot The theme_tufte() function in ggplot2 cleans up the formatting for a figure, by using no border, no axis lines, and no grids. For more information on using theme_tufte() in ggplot2, visit this link: https://rdrr.io/cran/ggthemes/man/theme_tufte.html For more information about Edward Tufte’s principles, see this reference: Tufte, Edward R. (2001) The Visual Display of Quantitative Information, Chapter 6. The 2020 season was shortened and delayed due to the COVID-19 pandemic. Originally planned for 22 Grands Prix, this season only contested 17 races and several venues cancelled. Hamilton and Valtteri Bottas finished 1st and 2nd in the Driver’s standings as Mercedes dominated the 2020 season. # Pull race results races2020 &lt;- race_result_scraper(2020) races2020 %&gt;% mutate( n = 1:n(), Race = fct_reorder(Race, desc(n))) %&gt;% group_by(Race) %&gt;% mutate(Race_number = cur_group_id()) %&gt;% group_by(Driver) %&gt;% mutate(sum_pts = sum(Points, na.rm = T)) %&gt;% ungroup() %&gt;% ggplot(aes(fct_reorder(Driver, desc(sum_pts)), Race, fill = Points)) + geom_tile(color=&quot;white&quot;, size=0.1, alpha = 0.75) + theme_bw() + labs(title = &#39;Points scored during the 2020 season&#39;, y = &#39;Race&#39;, x = &#39;Driver&#39;) + theme_tufte(base_family=&quot;Helvetica&quot;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), legend.position=&quot;bottom&quot;, axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + scale_fill_viridis_c(option = &#39;magma&#39;, guide = &#39;legend&#39;, breaks = c(0,1,2,4,6,8,10,12,15,18,25,26)) Lewis Hamilton won 11 races and secured his sixth Driver’s Championship in 2019, as Mercedes won a total of 15 out of 21 races. Valtteri Bottas won 4 races, while Verstappen and LeClerc won 3 and 2 races, respectively. # Pull race results races2019 &lt;- race_result_scraper(2019) races2019 %&gt;% mutate( n = 1:n(), Race = fct_reorder(Race, desc(n))) %&gt;% group_by(Race) %&gt;% mutate(Race_number = cur_group_id()) %&gt;% group_by(Driver) %&gt;% mutate(sum_pts = sum(Points, na.rm = T)) %&gt;% ungroup() %&gt;% ggplot(aes(fct_reorder(Driver, desc(sum_pts)), Race, fill = Points)) + geom_tile(color=&quot;white&quot;, size=0.1, alpha = 0.75) + theme_bw() + labs(title = &#39;Points scored during the 2019 season&#39;, y = &#39;Race&#39;, x = &#39;Driver&#39;) + theme_tufte(base_family=&quot;Helvetica&quot;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), legend.position=&quot;bottom&quot;, axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + scale_fill_viridis_c(option = &#39;magma&#39;, guide = &#39;legend&#39;, breaks = c(0,1,2,4,6,8,10,12,15,18,25,26)) Four-time World Champions Lewis Hamilton and Sebastian Vettel battled closely during 2018. This was the first season in which two four-time world champions battled for a fifth championship. The Championship lead swapped hands between Hamilton and Vettel five times during 2018, and was ultimately secured by Hamilton at the Mexican Grand Prix. # Pull race results races2018 &lt;- race_result_scraper(2018) races2018 %&gt;% mutate( n = 1:n(), Race = fct_reorder(Race, desc(n))) %&gt;% group_by(Race) %&gt;% mutate(Race_number = cur_group_id()) %&gt;% group_by(Driver) %&gt;% mutate(sum_pts = sum(Points, na.rm = T)) %&gt;% ungroup() %&gt;% ggplot(aes(fct_reorder(Driver, desc(sum_pts)), Race, fill = Points)) + geom_tile(color=&quot;white&quot;, size=0.1, alpha = 0.75) + theme_bw() + labs(title = &#39;Points scored during the 2018 season&#39;, y = &#39;Race&#39;, x = &#39;Driver&#39;) + theme_tufte(base_family=&quot;Helvetica&quot;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), legend.position=&quot;bottom&quot;, axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + scale_fill_viridis_c(option = &#39;magma&#39;, guide = &#39;legend&#39;, breaks = c(0,1,2,4,6,8,10,12,15,18,25,26)) 2017 marked the first time during Mercedes’ turbo-hybrid era reign that another team presented a legitimate challenge. Sebastian Vettel led the championship deep into this season, but Hamilton ultimately won his 4th title by 46 points. # Pull race results races2017 &lt;- race_result_scraper(2017) races2017 %&gt;% mutate( n = 1:n(), Race = fct_reorder(Race, desc(n))) %&gt;% group_by(Race) %&gt;% mutate(Race_number = cur_group_id()) %&gt;% group_by(Driver) %&gt;% mutate(sum_pts = sum(Points, na.rm = T)) %&gt;% ungroup() %&gt;% ggplot(aes(fct_reorder(Driver, desc(sum_pts)), Race, fill = Points)) + geom_tile(color=&quot;white&quot;, size=0.1, alpha = 0.75) + theme_bw() + labs(title = &#39;Points scored during the 2017 season&#39;, y = &#39;Race&#39;, x = &#39;Driver&#39;) + theme_tufte(base_family=&quot;Helvetica&quot;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), legend.position=&quot;bottom&quot;, axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + scale_fill_viridis_c(option = &#39;magma&#39;, guide = &#39;legend&#39;, breaks = c(0,1,2,4,6,8,10,12,15,18,25,26)) Nico Rosberg won his only World Driver’s Championship title in 2016. During an intense intra-team battle, Rosberg won the first four races while Hamilton won the final four. Rosberg secured his title over teammate Hamilton in the final race of the season. Shortly after winning the title, Rosberg announced his retirement from Formula 1. # Pull race results races2016 &lt;- race_result_scraper(2016) races2016 %&gt;% mutate( n = 1:n(), Race = fct_reorder(Race, desc(n))) %&gt;% group_by(Race) %&gt;% mutate(Race_number = cur_group_id()) %&gt;% group_by(Driver) %&gt;% mutate(sum_pts = sum(Points, na.rm = T)) %&gt;% ungroup() %&gt;% ggplot(aes(fct_reorder(Driver, desc(sum_pts)), Race, fill = Points)) + geom_tile(color=&quot;white&quot;, size=0.1, alpha = 0.75) + theme_bw() + labs(title = &#39;Points scored during the 2016 season&#39;, y = &#39;Race&#39;, x = &#39;Driver&#39;) + theme_tufte(base_family=&quot;Helvetica&quot;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), legend.position=&quot;bottom&quot;, axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + scale_fill_viridis_c(option = &#39;magma&#39;, guide = &#39;legend&#39;, breaks = c(0,1,2,4,6,8,10,12,15,18,25,26)) Lewis Hamilton defended his World Championship in 2015, securing the title with three races to go in the season. Nico Rosberg (Hamilton’s teammate) and Sebastian Vettel finished 2nd and 3rd, respectively. # Pull race results races2015 &lt;- race_result_scraper(2015) races2015 %&gt;% mutate( n = 1:n(), Race = fct_reorder(Race, desc(n))) %&gt;% group_by(Race) %&gt;% mutate(Race_number = cur_group_id()) %&gt;% group_by(Driver) %&gt;% mutate(sum_pts = sum(Points, na.rm = T)) %&gt;% ungroup() %&gt;% ggplot(aes(fct_reorder(Driver, desc(sum_pts)), Race, fill = Points)) + geom_tile(color=&quot;white&quot;, size=0.1, alpha = 0.75) + theme_bw() + labs(title = &#39;Points scored during the 2015 season&#39;, y = &#39;Race&#39;, x = &#39;Driver&#39;) + theme_tufte(base_family=&quot;Helvetica&quot;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), legend.position=&quot;bottom&quot;, axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + scale_fill_viridis_c(option = &#39;magma&#39;, guide = &#39;legend&#39;, breaks = c(0,1,2,4,6,8,10,12,15,18,25,26)) The 2014 season marked the introduction of a new engine formula. The previous era’s 2.4 liter V8 engine was replaced by a 1.6 liter turbocharged V6 engine with an energy recovery system. As a result, Mercedes experienced a great engine advantage in 2014, an edge that carried through the entirety of this turbo-hybrid era. Mercedes won their first Constructor’s Championship by nearly 300 points, and Hamilton won his second Driver’s Championship. # Pull race results races2014 &lt;- race_result_scraper(2014) races2014 %&gt;% mutate( n = 1:n(), Race = fct_reorder(Race, desc(n))) %&gt;% group_by(Race) %&gt;% mutate(Race_number = cur_group_id()) %&gt;% group_by(Driver) %&gt;% mutate(sum_pts = sum(Points, na.rm = T)) %&gt;% ungroup() %&gt;% mutate(Points = ifelse(Race == &#39;abu-dhabi&#39;, Points / 2, Points)) %&gt;% # convert the Abu Dhabi points ggplot(aes(fct_reorder(Driver, desc(sum_pts)), Race, fill = Points)) + geom_tile(color=&quot;white&quot;, size=0.1, alpha = 0.75) + theme_bw() + labs(title = &#39;Points scored during the 2014 season&#39;, y = &#39;Race&#39;, x = &#39;Driver&#39;) + theme_tufte(base_family=&quot;Helvetica&quot;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), legend.position=&quot;bottom&quot;, axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + scale_fill_viridis_c(option = &#39;magma&#39;, guide = &#39;legend&#39;, breaks = c(0,1,2,4,6,8,10,12,15,18,25,26)) Note: Double points were awarded at the 2014 Abu Dhabi Grand Prix. I manually converted these back to a 25 point scale to aid in the visualization. 5.2 Qualifying Pace Qualifying pace, or a driver’s flat-out speed over 1 lap, is another attribute that sets drivers apart. The driver-effect can sometimes be difficult to separate from the car-effect, however. For this reason, comparing teammates is one of best ways to evaluate a driver’s qualifying pace. As I mentioned earlier in the chapter, the best drivers of all time consistently out-qualify their teammates. Another way to evaluate qualifying pace is by looking at the season-long performance of drivers in qualifying. Despite the differences in car quality, some drivers are simply more capable of very fast qualifying times. Throughout this section, I will create figures that (A) display intra-team qualifying battles, and (B) plot a driver’s overall distribution of qualifying times for an entire season. I will start with the 2023 season, and work backward. 5.2.1 2023 Here’s a look at the intra-team qualifying battles for 2023. The Mercedes battle between Lewis Hamilton and George Hamilton was quite close, while the battle at Red Bull, Aston Martin, and Williams was not. Note: This figure actually required quite a bit of data wrangling. It was somewhat annoying! quali2023 &lt;- qualifying_scraper(2023) quali2023 %&gt;% ungroup() %&gt;% dplyr::select(Race, Position, Car, Driver) %&gt;% mutate(Position = as.integer(Position), Position = ifelse(is.na(Position), 21, Position)) %&gt;% group_by(Car, Race) %&gt;% arrange(Driver) %&gt;% mutate(driver_num = 1:n()) %&gt;% pivot_wider(names_from = &#39;driver_num&#39;, values_from = c(&#39;Driver&#39;, &#39;Position&#39;)) %&gt;% mutate(best_qualifier = ifelse(Position_1 &lt; Position_2, Driver_1, Driver_2)) %&gt;% ungroup() %&gt;% group_by(Car) %&gt;% count(best_qualifier) %&gt;% mutate(percentage = n / sum(n)) %&gt;% filter(!is.na(best_qualifier)) %&gt;% ggplot(aes(y = fct_reorder(best_qualifier, Car), x = percentage, fill = Car)) + geom_bar(stat = &#39;identity&#39;, position = &#39;dodge&#39;, alpha = 0.5, col = &#39;black&#39;) + theme_bw() + scale_x_continuous(labels = scales::percent_format()) + facet_wrap(~ factor(Car, levels = c(&quot;Ferrari&quot;, &quot;Mercedes&quot;, &quot;Haas Ferrari&quot;, &quot;Alpine Renault&quot;, &quot;AlphaTauri Honda RBPT&quot;, &quot;Alfa Romeo Ferrari&quot;, &quot;Aston Martin Aramco Mercedes&quot;, &quot;McLaren Mercedes&quot;, &quot;Williams Mercedes&quot;, &quot;Red Bull Racing Honda RBPT&quot;)), ncol = 1, strip.position=&quot;top&quot;, scales = &#39;free_y&#39;) + labs(y = &#39;Driver&#39;, x = &#39;Season-long Qualifying Win Proportion&#39;, title = &#39;Intra-team Qualifying Battles&#39;, subtitle = &#39;2023&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), plot.caption = element_text(hjust = 0)) + scale_fill_manual(values = c(&quot;red&quot;, &quot;cyan3&quot;, &quot;black&quot;, &quot;blue&quot;, &quot;darkslateblue&quot;, &quot;darkred&quot;, &quot;darkseagreen4&quot;, &quot;darkorange&quot;, &quot;cornflowerblue&quot;, &quot;navy&quot;), breaks = c( &quot;Ferrari&quot;, &quot;Mercedes&quot;, &quot;Haas Ferrari&quot;, &quot;Alpine Renault&quot;, &quot;AlphaTauri Honda RBPT&quot;, &quot;Alfa Romeo Ferrari&quot;, &quot;Aston Martin Aramco Mercedes&quot;, &quot;McLaren Mercedes&quot;, &quot;Williams Mercedes&quot;, &quot;Red Bull Racing Honda RBPT&quot;)) In the figure below, I plot each driver’s standardized qualifying time for 2023 (standardized to a given circuit). In 2023, Ferrari had a rapid qualifying pace, with both drivers consistently putting up very fast times. Charles LeClerc and Carlos Sainz took pole position 5 and 2 times, respectively. Despite this strong qualifying pace by Ferrari, Max Verstappen still managed to secure 12 pole positions for Red Bull and Sergio Perez collected another 2. qualifying_allyears %&gt;% filter(!is.na(Q_secs), Year == 2023) %&gt;% group_by(Race) %&gt;% mutate(track_mean = mean(Q_secs, na.rm = T), Time_std_track = Q_secs - track_mean) %&gt;% ungroup() %&gt;% group_by(Driver) %&gt;% mutate(driver_n = n(), mean = mean(Time_std_track, na.rm = T)) %&gt;% ungroup() %&gt;% dplyr::filter(driver_n &gt; 15) %&gt;% ggplot(aes(Time_std_track, y = fct_reorder(Driver, desc(mean)))) + geom_point(position = position_jitter(w = 0, h = 0.1), alpha = 0.5) + geom_hline(yintercept = seq(23, 1, -2), col = &#39;grey&#39;, size = 3, alpha = 0.15) + theme_bw() + labs(x = &#39;Standardized Qualifying Time (secs)&#39;, y = &#39;Driver&#39;, title = &#39;Standardized Qualifying Time&#39;, subtitle = &#39;2023&#39;, caption = &#39;*Times are subtracted from the average time for each Grand Prix&#39;) + geom_vline(xintercept = 0, col = &#39;red&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), plot.caption = element_text(hjust = 0)) + xlim(-5, 10) 5.2.2 2022 Charles Leclerc had the most most poles in 2022 (9), followed my Max Verstappen with 7. Some could argue that the Ferrari car was more suited for qualifying pace, but it’s difficult to definitely say the Ferrari was consistently the faster car over 1 lap. Leclerc and Verstappen both out-qualified their teammates by a wide margin (9-3 in favor of LeClerc and 7-1 in favor of Varstappen). Leclerc’s worst performance was 15th in Canada, while Verstappen’s worst qualifying was 10th in Hungary. Additionally, Verstappen made it to Q3 in every weekend of the season. While the vast majority of the drivers on the grid were not in serious contention for poles, some drivers clearly outperformed their teammates in 2022. For instance, Alex Albon out-qualified teammate Nicholas Latifi by a margin of 19-2. quali2022 &lt;- qualifying_scraper(2022) quali2022 %&gt;% ungroup() %&gt;% dplyr::select(Race, Position, Car, Driver) %&gt;% mutate(Position = as.integer(Position), Position = ifelse(is.na(Position), 21, Position)) %&gt;% group_by(Car, Race) %&gt;% arrange(Driver) %&gt;% mutate(driver_num = 1:n()) %&gt;% pivot_wider(names_from = &#39;driver_num&#39;, values_from = c(&#39;Driver&#39;, &#39;Position&#39;)) %&gt;% mutate(best_qualifier = ifelse(Position_1 &lt; Position_2, Driver_1, Driver_2)) %&gt;% ungroup() %&gt;% group_by(Car) %&gt;% count(best_qualifier) %&gt;% mutate(percentage = n / sum(n)) %&gt;% filter(!is.na(best_qualifier)) %&gt;% ggplot(aes(y = fct_reorder(best_qualifier, Car), x = percentage, fill = Car)) + geom_bar(stat = &#39;identity&#39;, position = &#39;dodge&#39;, alpha = 0.5, col = &#39;black&#39;) + theme_bw() + scale_x_continuous(labels = scales::percent_format()) + facet_wrap(~ factor(Car, levels = c(&quot;Ferrari&quot;, &quot;Mercedes&quot;, &quot;Haas Ferrari&quot;, &quot;Alpine Renault&quot;, &quot;AlphaTauri RBPT&quot;, &quot;Alfa Romeo Ferrari&quot;, &quot;Aston Martin Aramco Mercedes&quot;, &quot;McLaren Mercedes&quot;, &quot;Williams Mercedes&quot;, &quot;Red Bull Racing RBPT&quot;)), ncol = 1, strip.position=&quot;top&quot;, scales = &#39;free_y&#39;) + labs(y = &#39;Driver&#39;, x = &#39;Season-long Qualifying Win Proportion&#39;, title = &#39;Intra-team Qualifying Battles&#39;, subtitle = &#39;2022&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), plot.caption = element_text(hjust = 0)) + scale_fill_manual(values = c(&quot;red&quot;, &quot;cyan3&quot;, &quot;black&quot;, &quot;blue&quot;, &quot;darkslateblue&quot;, &quot;darkred&quot;, &quot;darkseagreen4&quot;, &quot;darkorange&quot;, &quot;cornflowerblue&quot;, &quot;navy&quot;), breaks = c( &quot;Ferrari&quot;, &quot;Mercedes&quot;, &quot;Haas Ferrari&quot;, &quot;Alpine Renault&quot;, &quot;AlphaTauri RBPT&quot;, &quot;Alfa Romeo Ferrari&quot;, &quot;Aston Martin Aramco Mercedes&quot;, &quot;McLaren Mercedes&quot;, &quot;Williams Mercedes&quot;, &quot;Red Bull Racing RBPT&quot;)) It’s pretty clear that both LeClerc and Verstappen stood apart from any other driver in 2022. qualifying_allyears %&gt;% filter(!is.na(Q_secs), Year == 2022) %&gt;% group_by(Race) %&gt;% mutate(track_mean = mean(Q_secs, na.rm = T), Time_std_track = Q_secs - track_mean) %&gt;% ungroup() %&gt;% group_by(Driver) %&gt;% mutate(driver_n = n(), mean = mean(Time_std_track, na.rm = T)) %&gt;% ungroup() %&gt;% dplyr::filter(driver_n &gt; 15) %&gt;% ggplot(aes(Time_std_track, y = fct_reorder(Driver, desc(mean)))) + geom_point(position = position_jitter(w = 0, h = 0.1), alpha = 0.5) + geom_hline(yintercept = seq(37, 1, -2), col = &#39;grey&#39;, size = 3, alpha = 0.15) + theme_bw() + labs(x = &#39;Standardized Qualifying Time (secs)&#39;, y = &#39;Driver&#39;, title = &#39;Standardized Qualifying Time&#39;, subtitle = &#39;2022&#39;, caption = &#39;*Times are subtracted from the average time for each Grand Prix&#39;) + geom_vline(xintercept = 0, col = &#39;red&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), plot.caption = element_text(hjust = 0)) + xlim(-5, 10) 5.2.3 2021 During 2021, some head-to-head battles were particularly lop-sided. Pierre Gasly out-qualified Yuki Tsunoda in 21 out of 22 races, while Verstappen out-qualified Perez in 20 out of 22. quali2021 &lt;- qualifying_scraper(2021) quali2021 %&gt;% ungroup() %&gt;% dplyr::select(Race, Position, Car, Driver) %&gt;% mutate(Position = as.integer(Position), Position = ifelse(is.na(Position), 21, Position)) %&gt;% group_by(Car, Race) %&gt;% arrange(Driver) %&gt;% mutate(driver_num = 1:n()) %&gt;% pivot_wider(names_from = &#39;driver_num&#39;, values_from = c(&#39;Driver&#39;, &#39;Position&#39;)) %&gt;% mutate(best_qualifier = ifelse(Position_1 &lt; Position_2, Driver_1, Driver_2)) %&gt;% ungroup() %&gt;% group_by(Car) %&gt;% count(best_qualifier) %&gt;% mutate(percentage = n / sum(n)) %&gt;% filter(!is.na(best_qualifier)) %&gt;% ggplot(aes(y = fct_reorder(best_qualifier, Car), x = percentage, fill = Car)) + geom_bar(stat = &#39;identity&#39;, position = &#39;dodge&#39;, alpha = 0.5, col = &#39;black&#39;) + theme_bw() + scale_x_continuous(labels = scales::percent_format()) + facet_wrap(~ factor(Car, levels = c(&quot;Ferrari&quot;, &quot;Mercedes&quot;, &quot;Haas Ferrari&quot;, &quot;Alpine Renault&quot;, &quot;AlphaTauri Honda&quot;, &quot;Alfa Romeo Racing Ferrari&quot;, &quot;Aston Martin Mercedes&quot;, &quot;McLaren Mercedes&quot;, &quot;Williams Mercedes&quot;, &quot;Red Bull Racing Honda&quot;)), ncol = 1, strip.position=&quot;top&quot;, scales = &#39;free_y&#39;) + labs(y = &#39;Driver&#39;, x = &#39;Season-long Qualifying Win Proportion&#39;, title = &#39;Intra-team Qualifying Battles&#39;, subtitle = &#39;2021&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), plot.caption = element_text(hjust = 0)) + scale_fill_manual(values = c(&quot;red&quot;, &quot;cyan3&quot;, &quot;black&quot;, &quot;blue&quot;, &quot;darkslateblue&quot;, &quot;darkred&quot;, &quot;darkseagreen4&quot;, &quot;darkorange&quot;, &quot;cornflowerblue&quot;, &quot;navy&quot;), breaks = c(&quot;Ferrari&quot;, &quot;Mercedes&quot;, &quot;Haas Ferrari&quot;, &quot;Alpine Renault&quot;, &quot;AlphaTauri Honda&quot;, &quot;Alfa Romeo Racing Ferrari&quot;, &quot;Aston Martin Mercedes&quot;, &quot;McLaren Mercedes&quot;, &quot;Williams Mercedes&quot;, &quot;Red Bull Racing Honda&quot;)) Max Verstappen (10) and Lewis Hamilton (5) took 15 of the 22 possible poles in 2021. In fact, Red Bull and Mercedes combined to take all but 3 of the 2021 pole positions. qualifying_allyears %&gt;% filter(!is.na(Q_secs), Year == 2021) %&gt;% group_by(Race) %&gt;% mutate(track_mean = mean(Q_secs, na.rm = T), Time_std_track = Q_secs - track_mean) %&gt;% ungroup() %&gt;% group_by(Driver) %&gt;% mutate(driver_n = n(), mean = mean(Time_std_track, na.rm = T)) %&gt;% ungroup() %&gt;% dplyr::filter(driver_n &gt; 15) %&gt;% ggplot(aes(Time_std_track, y = fct_reorder(Driver, desc(mean)))) + geom_point(position = position_jitter(w = 0, h = 0.1), alpha = 0.5) + geom_hline(yintercept = seq(37, 1, -2), col = &#39;grey&#39;, size = 3, alpha = 0.15) + theme_bw() + labs(x = &#39;Standardized Qualifying Time (secs)&#39;, y = &#39;Driver&#39;, title = &#39;Standardized Qualifying Time&#39;, subtitle = &#39;2021&#39;, caption = &#39;*Times are subtracted from the average time for each Grand Prix&#39;) + geom_vline(xintercept = 0, col = &#39;red&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), plot.caption = element_text(hjust = 0)) + xlim(-5, 10) 5.2.4 2020 Mercedes dominated the COVID-shortened 2020 season, winning all but two poles. Lance Stroll took his first career pole during a rainy qualifying session at the Turkish Grand Prix. In the final Grand Prix of the year, Verstappen qualified on pole with the only car not powered by a Mercedes engine in 2020. quali2020 &lt;- qualifying_scraper(2020) quali2020 %&gt;% ungroup() %&gt;% dplyr::select(Race, Position, Car, Driver) %&gt;% mutate(Position = as.integer(Position), Position = ifelse(is.na(Position), 21, Position)) %&gt;% group_by(Car, Race) %&gt;% arrange(Driver) %&gt;% mutate(driver_num = 1:n()) %&gt;% pivot_wider(names_from = &#39;driver_num&#39;, values_from = c(&#39;Driver&#39;, &#39;Position&#39;)) %&gt;% mutate(best_qualifier = ifelse(Position_1 &lt; Position_2, Driver_1, Driver_2)) %&gt;% ungroup() %&gt;% group_by(Car) %&gt;% count(best_qualifier) %&gt;% mutate(percentage = n / sum(n)) %&gt;% filter(!is.na(best_qualifier)) %&gt;% ggplot(aes(y = fct_reorder(best_qualifier, Car), x = percentage, fill = Car)) + geom_bar(stat = &#39;identity&#39;, position = &#39;dodge&#39;, alpha = 0.5, col = &#39;black&#39;) + theme_bw() + scale_x_continuous(labels = scales::percent_format()) + facet_wrap(~ factor(Car, levels = c(&quot;Ferrari&quot;, &quot;Mercedes&quot;, &quot;Haas Ferrari&quot;, &quot;Renault&quot;, &quot;AlphaTauri Honda&quot;, &quot;Alfa Romeo Racing Ferrari&quot;, &quot;Racing Point BWT Mercedes&quot;, &quot;McLaren Renault&quot;, &quot;Williams Mercedes&quot;, &quot;Red Bull Racing Honda&quot;)), ncol = 1, strip.position=&quot;top&quot;, scales = &#39;free_y&#39;) + labs(y = &#39;Driver&#39;, x = &#39;Season-long Qualifying Win Proportion&#39;, title = &#39;Intra-team Qualifying Battles&#39;, subtitle = &#39;2020&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), plot.caption = element_text(hjust = 0)) + scale_fill_manual(values = c(&quot;red&quot;, &quot;cyan3&quot;, &quot;black&quot;, &quot;yellow&quot;, &quot;darkslateblue&quot;, &quot;darkred&quot;, &quot;pink&quot;, &quot;darkorange&quot;, &quot;cornflowerblue&quot;, &quot;navy&quot;), breaks = c(&quot;Ferrari&quot;, &quot;Mercedes&quot;, &quot;Haas Ferrari&quot;, &quot;Renault&quot;, &quot;AlphaTauri Honda&quot;, &quot;Alfa Romeo Racing Ferrari&quot;, &quot;Racing Point BWT Mercedes&quot;, &quot;McLaren Renault&quot;, &quot;Williams Mercedes&quot;, &quot;Red Bull Racing Honda&quot;)) And, here’s a look at 2020’s standardized qualifying times. qualifying_allyears %&gt;% filter(!is.na(Q_secs), Year == 2020) %&gt;% group_by(Race) %&gt;% mutate(track_mean = mean(Q_secs, na.rm = T), Time_std_track = Q_secs - track_mean) %&gt;% ungroup() %&gt;% group_by(Driver) %&gt;% mutate(driver_n = n(), mean = mean(Time_std_track, na.rm = T)) %&gt;% ungroup() %&gt;% dplyr::filter(driver_n &gt; 15) %&gt;% ggplot(aes(Time_std_track, y = fct_reorder(Driver, desc(mean)))) + geom_point(position = position_jitter(w = 0, h = 0.1), alpha = 0.5) + geom_hline(yintercept = seq(37, 1, -2), col = &#39;grey&#39;, size = 3, alpha = 0.15) + theme_bw() + labs(x = &#39;Standardized Qualifying Time (secs)&#39;, y = &#39;Driver&#39;, title = &#39;Standardized Qualifying Time&#39;, subtitle = &#39;2020&#39;, caption = &#39;*Times are subtracted from the average time for each Grand Prix&#39;) + geom_vline(xintercept = 0, col = &#39;red&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), plot.caption = element_text(hjust = 0)) + xlim(-5, 10) 5.2.5 2019 Qualifying during 2019 was quite close between Mercedes (10 poles) and Ferrari (9 poles). Charles LeClerc qualified on pole 7 times, while Valterri Bottas and Lewis Hamilton both finished with 5 pole positions each. quali2019 &lt;- qualifying_scraper(2019) quali2019 %&gt;% ungroup() %&gt;% dplyr::select(Race, Position, Car, Driver) %&gt;% mutate(Position = as.integer(Position), Position = ifelse(is.na(Position), 21, Position)) %&gt;% group_by(Car, Race) %&gt;% arrange(Driver) %&gt;% mutate(driver_num = 1:n()) %&gt;% pivot_wider(names_from = &#39;driver_num&#39;, values_from = c(&#39;Driver&#39;, &#39;Position&#39;)) %&gt;% mutate(best_qualifier = ifelse(Position_1 &lt; Position_2, Driver_1, Driver_2)) %&gt;% ungroup() %&gt;% group_by(Car) %&gt;% count(best_qualifier) %&gt;% mutate(percentage = n / sum(n)) %&gt;% filter(!is.na(best_qualifier)) %&gt;% ggplot(aes(y = fct_reorder(best_qualifier, Car), x = percentage, fill = Car)) + geom_bar(stat = &#39;identity&#39;, position = &#39;dodge&#39;, alpha = 0.5, col = &#39;black&#39;) + theme_bw() + scale_x_continuous(labels = scales::percent_format()) + facet_wrap(~ factor(Car, levels = c(&quot;Ferrari&quot;, &quot;Mercedes&quot;, &quot;Haas Ferrari&quot;, &quot;Renault&quot;, &quot;Scuderia Toro Rosso Honda&quot;, &quot;Alfa Romeo Racing Ferrari&quot;, &quot;Racing Point BWT Mercedes&quot;, &quot;McLaren Renault&quot;, &quot;Williams Mercedes&quot;, &quot;Red Bull Racing Honda&quot;)), ncol = 1, strip.position=&quot;top&quot;, scales = &#39;free_y&#39;) + labs(y = &#39;Driver&#39;, x = &#39;Season-long Qualifying Win Proportion&#39;, title = &#39;Intra-team Qualifying Battles&#39;, subtitle = &#39;2019&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), plot.caption = element_text(hjust = 0)) + scale_fill_manual(values = c(&quot;red&quot;, &quot;cyan3&quot;, &quot;black&quot;, &quot;yellow&quot;, &quot;darkslateblue&quot;, &quot;darkred&quot;, &quot;pink&quot;, &quot;darkorange&quot;, &quot;cornflowerblue&quot;, &quot;navy&quot;), breaks = c(&quot;Ferrari&quot;, &quot;Mercedes&quot;, &quot;Haas Ferrari&quot;, &quot;Renault&quot;, &quot;AlphaTauri Honda&quot;, &quot;Alfa Romeo Racing Ferrari&quot;, &quot;Racing Point BWT Mercedes&quot;, &quot;McLaren Renault&quot;, &quot;Williams Mercedes&quot;, &quot;Red Bull Racing Honda&quot;)) When we look at the standardized qualifying times in 2019, five drivers are clearly faster than the rest of the grid: both Mercedes drivers, both Ferrari drivers, and Max Verstappen. qualifying_allyears %&gt;% filter(!is.na(Q_secs), Year == 2019) %&gt;% group_by(Race) %&gt;% mutate(track_mean = mean(Q_secs, na.rm = T), Time_std_track = Q_secs - track_mean) %&gt;% ungroup() %&gt;% group_by(Driver) %&gt;% mutate(driver_n = n(), mean = mean(Time_std_track, na.rm = T)) %&gt;% ungroup() %&gt;% dplyr::filter(driver_n &gt; 15) %&gt;% ggplot(aes(Time_std_track, y = fct_reorder(Driver, desc(mean)))) + geom_point(position = position_jitter(w = 0, h = 0.1), alpha = 0.5) + geom_hline(yintercept = seq(37, 1, -2), col = &#39;grey&#39;, size = 3, alpha = 0.15) + theme_bw() + labs(x = &#39;Standardized Qualifying Time (secs)&#39;, y = &#39;Driver&#39;, title = &#39;Standardized Qualifying Time&#39;, subtitle = &#39;2019&#39;, caption = &#39;*Times are subtracted from the average time for each Grand Prix&#39;) + geom_vline(xintercept = 0, col = &#39;red&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), plot.caption = element_text(hjust = 0)) + xlim(-5, 10) 5.2.6 2018 During the 2018 season, Lewis Hamilton and Sebastian Vettel battled closely for pole positions. Here’s a look at the intra-team battles. quali2018 &lt;- qualifying_scraper(2018) quali2018 %&gt;% ungroup() %&gt;% dplyr::select(Race, Position, Car, Driver) %&gt;% mutate(Position = as.integer(Position), Position = ifelse(is.na(Position), 21, Position)) %&gt;% group_by(Car, Race) %&gt;% arrange(Driver) %&gt;% mutate(driver_num = 1:n()) %&gt;% pivot_wider(names_from = &#39;driver_num&#39;, values_from = c(&#39;Driver&#39;, &#39;Position&#39;)) %&gt;% mutate(best_qualifier = ifelse(Position_1 &lt; Position_2, Driver_1, Driver_2)) %&gt;% ungroup() %&gt;% group_by(Car) %&gt;% count(best_qualifier) %&gt;% mutate(percentage = n / sum(n)) %&gt;% filter(!is.na(best_qualifier)) %&gt;% ggplot(aes(y = fct_reorder(best_qualifier, Car), x = percentage, fill = Car)) + geom_bar(stat = &#39;identity&#39;, position = &#39;dodge&#39;, alpha = 0.5, col = &#39;black&#39;) + theme_bw() + scale_x_continuous(labels = scales::percent_format()) + facet_wrap(~ factor(Car, levels = c(&quot;Ferrari&quot;, &quot;Mercedes&quot;, &quot;Haas Ferrari&quot;, &quot;Renault&quot;, &quot;Scuderia Toro Rosso Honda&quot;, &quot;Sauber Ferrari&quot;, &quot;Force India Mercedes&quot;, &quot;McLaren Renault&quot;, &quot;Williams Mercedes&quot;, &quot;Red Bull Racing TAG Heuer&quot;)), ncol = 1, strip.position=&quot;top&quot;, scales = &#39;free_y&#39;) + labs(y = &#39;Driver&#39;, x = &#39;Season-long Qualifying Win Proportion&#39;, title = &#39;Intra-team Qualifying Battles&#39;, subtitle = &#39;2018&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), plot.caption = element_text(hjust = 0)) + scale_fill_manual(values = c(&quot;red&quot;, &quot;cyan3&quot;, &quot;black&quot;, &quot;yellow&quot;, &quot;darkslateblue&quot;, &quot;darkred&quot;, &quot;pink&quot;, &quot;darkorange&quot;, &quot;cornflowerblue&quot;, &quot;navy&quot;), breaks = c(&quot;Ferrari&quot;, &quot;Mercedes&quot;, &quot;Haas Ferrari&quot;, &quot;Renault&quot;, &quot;Scuderia Toro Rosso Honda&quot;, &quot;Sauber Ferrari&quot;, &quot;Force India Mercedes&quot;, &quot;McLaren Renault&quot;, &quot;Williams Mercedes&quot;, &quot;Red Bull Racing TAG Heuer&quot;)) Every race during the 2018 season featured either Hamilton or Vettel on the front row of the grid. In fact, Hamilton and Vettel had average starting positions of 2.48 and 2.62, respectively. qualifying_allyears %&gt;% filter(!is.na(Q_secs), Year == 2018) %&gt;% group_by(Race) %&gt;% mutate(track_mean = mean(Q_secs, na.rm = T), Time_std_track = Q_secs - track_mean) %&gt;% ungroup() %&gt;% group_by(Driver) %&gt;% mutate(driver_n = n(), mean = mean(Time_std_track, na.rm = T)) %&gt;% ungroup() %&gt;% dplyr::filter(driver_n &gt; 15) %&gt;% ggplot(aes(Time_std_track, y = fct_reorder(Driver, desc(mean)))) + geom_point(position = position_jitter(w = 0, h = 0.1), alpha = 0.5) + geom_hline(yintercept = seq(37, 1, -2), col = &#39;grey&#39;, size = 3, alpha = 0.15) + theme_bw() + labs(x = &#39;Standardized Qualifying Time (secs)&#39;, y = &#39;Driver&#39;, title = &#39;Standardized Qualifying Time&#39;, subtitle = &#39;2018&#39;, caption = &#39;*Times are subtracted from the average time for each Grand Prix&#39;) + geom_vline(xintercept = 0, col = &#39;red&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), plot.caption = element_text(hjust = 0)) + xlim(-5, 10) 5.2.7 2017 During 2017, only two teams (Mercedes and Ferrari) qualified on pole. Impressively, Hamilton achieved pole position in over half the races (11/ 20). quali2017 &lt;- qualifying_scraper(2017) quali2017 %&gt;% ungroup() %&gt;% dplyr::select(Race, Position, Car, Driver) %&gt;% mutate(Position = as.integer(Position), Position = ifelse(is.na(Position), 21, Position)) %&gt;% group_by(Car, Race) %&gt;% arrange(Driver) %&gt;% mutate(driver_num = 1:n()) %&gt;% pivot_wider(names_from = &#39;driver_num&#39;, values_from = c(&#39;Driver&#39;, &#39;Position&#39;)) %&gt;% mutate(best_qualifier = ifelse(Position_1 &lt; Position_2, Driver_1, Driver_2)) %&gt;% ungroup() %&gt;% group_by(Car) %&gt;% count(best_qualifier) %&gt;% mutate(percentage = n / sum(n)) %&gt;% filter(!is.na(best_qualifier)) %&gt;% ggplot(aes(y = fct_reorder(best_qualifier, Car), x = percentage, fill = Car)) + geom_bar(stat = &#39;identity&#39;, position = &#39;dodge&#39;, alpha = 0.5, col = &#39;black&#39;) + theme_bw() + scale_x_continuous(labels = scales::percent_format()) + facet_wrap(~ factor(Car, levels = c(&quot;Ferrari&quot;, &quot;Mercedes&quot;, &quot;Haas Ferrari&quot;, &quot;Renault&quot;, &quot;Toro Rosso&quot;, &quot;Sauber Ferrari&quot;, &quot;Force India Mercedes&quot;, &quot;McLaren Honda&quot;, &quot;Williams Mercedes&quot;, &quot;Red Bull Racing TAG Heuer&quot;)), ncol = 1, strip.position=&quot;top&quot;, scales = &#39;free_y&#39;) + labs(y = &#39;Driver&#39;, x = &#39;Season-long Qualifying Win Proportion&#39;, title = &#39;Intra-team Qualifying Battles&#39;, subtitle = &#39;2017&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), plot.caption = element_text(hjust = 0)) + scale_fill_manual(values = c(&quot;red&quot;, &quot;cyan3&quot;, &quot;black&quot;, &quot;yellow&quot;, &quot;darkslateblue&quot;, &quot;darkred&quot;, &quot;pink&quot;, &quot;darkorange&quot;, &quot;cornflowerblue&quot;, &quot;navy&quot;), breaks = c(&quot;Ferrari&quot;, &quot;Mercedes&quot;, &quot;Haas Ferrari&quot;, &quot;Renault&quot;, &quot;Scuderia Toro Rosso Honda&quot;, &quot;Sauber Ferrari&quot;, &quot;Force India Mercedes&quot;, &quot;McLaren Honda&quot;, &quot;Williams Mercedes&quot;, &quot;Red Bull Racing TAG Heuer&quot;)) Interestingly, only Bottas and Kimi Raikkonen reached Q3 in every Grand Prix of the season. qualifying_allyears %&gt;% filter(!is.na(Q_secs), Year == 2017) %&gt;% group_by(Race) %&gt;% mutate(track_mean = mean(Q_secs, na.rm = T), Time_std_track = Q_secs - track_mean) %&gt;% ungroup() %&gt;% group_by(Driver) %&gt;% mutate(driver_n = n(), mean = mean(Time_std_track, na.rm = T)) %&gt;% ungroup() %&gt;% dplyr::filter(driver_n &gt; 15) %&gt;% ggplot(aes(Time_std_track, y = fct_reorder(Driver, desc(mean)))) + geom_point(position = position_jitter(w = 0, h = 0.1), alpha = 0.5) + geom_hline(yintercept = seq(37, 1, -2), col = &#39;grey&#39;, size = 3, alpha = 0.15) + theme_bw() + labs(x = &#39;Standardized Qualifying Time (secs)&#39;, y = &#39;Driver&#39;, title = &#39;Standardized Qualifying Time&#39;, subtitle = &#39;2017&#39;, caption = &#39;*Times are subtracted from the average time for each Grand Prix&#39;) + geom_vline(xintercept = 0, col = &#39;red&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), plot.caption = element_text(hjust = 0)) + xlim(-5, 10) 5.2.8 2016 2016 was an extremely tight competition in both races and qualifying for the two Mercedes drivers. Nico Rosberg qualified on the front row in every race of the season, while maintaining an average starting position of 1.62. However, Hamilton still managed to achieve pole more often: 12 poles vs 8 for Rosberg. quali2016 &lt;- qualifying_scraper(2016) quali2016 %&gt;% mutate(Car = str_replace(Car, &quot;-&quot;, &quot; &quot;)) %&gt;% ungroup() %&gt;% dplyr::select(Race, Position, Car, Driver) %&gt;% mutate(Position = as.integer(Position), Position = ifelse(is.na(Position), 21, Position)) %&gt;% group_by(Car, Race) %&gt;% arrange(Driver) %&gt;% mutate(driver_num = 1:n()) %&gt;% pivot_wider(names_from = &#39;driver_num&#39;, values_from = c(&#39;Driver&#39;, &#39;Position&#39;)) %&gt;% mutate(best_qualifier = ifelse(Position_1 &lt; Position_2, Driver_1, Driver_2)) %&gt;% ungroup() %&gt;% group_by(Car) %&gt;% count(best_qualifier) %&gt;% mutate(percentage = n / sum(n)) %&gt;% filter(!is.na(best_qualifier)) %&gt;% ggplot(aes(y = fct_reorder(best_qualifier, Car), x = percentage, fill = Car)) + geom_bar(stat = &#39;identity&#39;, position = &#39;dodge&#39;, alpha = 0.5, col = &#39;black&#39;) + theme_bw() + scale_x_continuous(labels = scales::percent_format()) + facet_wrap(~ factor(Car, levels = c(&quot;Ferrari&quot;, &quot;Mercedes&quot;, &quot;Haas Ferrari&quot;, &quot;Renault&quot;, &quot;Toro Rosso Ferrari&quot;, &quot;Sauber Ferrari&quot;, &quot;Force India Mercedes&quot;, &quot;McLaren Honda&quot;, &quot;Williams Mercedes&quot;, &quot;Red Bull Racing TAG Heuer&quot;, &quot;MRT Mercedes&quot;)), ncol = 1, strip.position=&quot;top&quot;, scales = &#39;free_y&#39;) + labs(y = &#39;Driver&#39;, x = &#39;Season-long Qualifying Win Proportion&#39;, title = &#39;Intra-team Qualifying Battles&#39;, subtitle = &#39;2016&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), plot.caption = element_text(hjust = 0)) + scale_fill_manual(values = c(&quot;red&quot;, &quot;cyan3&quot;, &quot;black&quot;, &quot;yellow&quot;, &quot;darkslateblue&quot;, &quot;darkred&quot;, &quot;pink&quot;, &quot;darkorange&quot;, &quot;cornflowerblue&quot;, &quot;navy&quot;, &quot;blue&quot;), breaks = c(&quot;Ferrari&quot;, &quot;Mercedes&quot;, &quot;Haas Ferrari&quot;, &quot;Renault&quot;, &quot;Toro Rosso Ferrari&quot;, &quot;Sauber Ferrari&quot;, &quot;Force India Mercedes&quot;, &quot;McLaren Honda&quot;, &quot;Williams Mercedes&quot;, &quot;Red Bull Racing TAG Heuer&quot;, &quot;MRT Mercedes&quot;)) The only non-Mercedes pole of 2016 was Daniel Ricciardo in Monaco. qualifying_allyears %&gt;% filter(!is.na(Q_secs), Year == 2016) %&gt;% group_by(Race) %&gt;% mutate(track_mean = mean(Q_secs, na.rm = T), Time_std_track = Q_secs - track_mean) %&gt;% ungroup() %&gt;% group_by(Driver) %&gt;% mutate(driver_n = n(), mean = mean(Time_std_track, na.rm = T)) %&gt;% ungroup() %&gt;% dplyr::filter(driver_n &gt; 15) %&gt;% ggplot(aes(Time_std_track, y = fct_reorder(Driver, desc(mean)))) + geom_point(position = position_jitter(w = 0, h = 0.1), alpha = 0.5) + geom_hline(yintercept = seq(37, 1, -2), col = &#39;grey&#39;, size = 3, alpha = 0.15) + theme_bw() + labs(x = &#39;Standardized Qualifying Time (secs)&#39;, y = &#39;Driver&#39;, title = &#39;Standardized Qualifying Time&#39;, subtitle = &#39;2016&#39;, caption = &#39;*Times are subtracted from the average time for each Grand Prix&#39;) + geom_vline(xintercept = 0, col = &#39;red&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), plot.caption = element_text(hjust = 0)) + xlim(-5, 10) 5.2.9 2015 Lewis Hamilton took 11 out of the first 12 poles of the 2015 season, while Rosberg won the final 6 poles. quali2015 &lt;- qualifying_scraper(2015) quali2015 %&gt;% mutate(Car = str_replace(Car, &quot;-&quot;, &quot; &quot;)) %&gt;% ungroup() %&gt;% dplyr::select(Race, Position, Car, Driver) %&gt;% mutate(Position = as.integer(Position), Position = ifelse(is.na(Position), 21, Position)) %&gt;% group_by(Car, Race) %&gt;% arrange(Driver) %&gt;% mutate(driver_num = 1:n()) %&gt;% pivot_wider(names_from = &#39;driver_num&#39;, values_from = c(&#39;Driver&#39;, &#39;Position&#39;)) %&gt;% mutate(best_qualifier = ifelse(Position_1 &lt; Position_2, Driver_1, Driver_2)) %&gt;% ungroup() %&gt;% group_by(Car) %&gt;% count(best_qualifier) %&gt;% mutate(percentage = n / sum(n)) %&gt;% filter(!is.na(best_qualifier)) %&gt;% ggplot(aes(y = fct_reorder(best_qualifier, Car), x = percentage, fill = Car)) + geom_bar(stat = &#39;identity&#39;, position = &#39;dodge&#39;, alpha = 0.5, col = &#39;black&#39;) + theme_bw() + scale_x_continuous(labels = scales::percent_format()) + facet_wrap(~ factor(Car, levels =c(&quot;Ferrari&quot;, &quot;Mercedes&quot;, &quot;Lotus Mercedes&quot;, &quot;Marussia Ferrari&quot;, &quot;STR Renault&quot;, &quot;Sauber Ferrari&quot;, &quot;Force India Mercedes&quot;, &quot;McLaren Honda&quot;, &quot;Williams Mercedes&quot;, &quot;Red Bull Racing Renault&quot;)), ncol = 1, strip.position=&quot;top&quot;, scales = &#39;free_y&#39;) + labs(y = &#39;Driver&#39;, x = &#39;Season-long Qualifying Win Proportion&#39;, title = &#39;Intra-team Qualifying Battles&#39;, subtitle = &#39;2015&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), plot.caption = element_text(hjust = 0)) + scale_fill_manual(values = c(&quot;red&quot;, &quot;cyan3&quot;, &quot;black&quot;, &quot;grey&quot;, &quot;darkslateblue&quot;, &quot;darkred&quot;, &quot;pink&quot;, &quot;darkorange&quot;, &quot;cornflowerblue&quot;, &quot;blue&quot;, &quot;navy&quot;), breaks = c(&quot;Ferrari&quot;, &quot;Mercedes&quot;, &quot;Lotus Mercedes&quot;, &quot;Marussia Ferrari&quot;, &quot;STR Renault&quot;, &quot;Sauber Ferrari&quot;, &quot;Force India Mercedes&quot;, &quot;McLaren Honda&quot;, &quot;Williams Mercedes&quot;, &quot;Red Bull Racing Renault&quot;)) Mercedes was absolutely dominant in qualifying and missed out on only 1 pole to Sebastian Vettel in Singapore. qualifying_allyears %&gt;% filter(!is.na(Q_secs), Year == 2015) %&gt;% group_by(Race) %&gt;% mutate(track_mean = mean(Q_secs, na.rm = T), Time_std_track = Q_secs - track_mean) %&gt;% ungroup() %&gt;% group_by(Driver) %&gt;% mutate(driver_n = n(), mean = mean(Time_std_track, na.rm = T)) %&gt;% ungroup() %&gt;% dplyr::filter(driver_n &gt; 15) %&gt;% ggplot(aes(Time_std_track, y = fct_reorder(Driver, desc(mean)))) + geom_point(position = position_jitter(w = 0, h = 0.1), alpha = 0.5) + geom_hline(yintercept = seq(37, 1, -2), col = &#39;grey&#39;, size = 3, alpha = 0.15) + theme_bw() + labs(x = &#39;Standardized Qualifying Time (secs)&#39;, y = &#39;Driver&#39;, title = &#39;Standardized Qualifying Time&#39;, subtitle = &#39;2015&#39;, caption = &#39;*Times are subtracted from the average time for each Grand Prix&#39;) + geom_vline(xintercept = 0, col = &#39;red&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), plot.caption = element_text(hjust = 0)) + xlim(-5, 10) 5.2.10 2014 The very consistent Nico Rosberg averaged a 1.68 starting position, won 11 poles, and out-qualified Lewis Hamilton 12:7 in the 2014 season. quali2014 &lt;- qualifying_scraper(2014) quali2014 %&gt;% mutate(Car = str_replace(Car, &quot;-&quot;, &quot; &quot;)) %&gt;% ungroup() %&gt;% dplyr::select(Race, Position, Car, Driver) %&gt;% mutate(Position = as.integer(Position), Position = ifelse(is.na(Position), 21, Position)) %&gt;% group_by(Car, Race) %&gt;% arrange(Driver) %&gt;% mutate(driver_num = 1:n()) %&gt;% pivot_wider(names_from = &#39;driver_num&#39;, values_from = c(&#39;Driver&#39;, &#39;Position&#39;)) %&gt;% mutate(best_qualifier = ifelse(Position_1 &lt; Position_2, Driver_1, Driver_2)) %&gt;% ungroup() %&gt;% group_by(Car) %&gt;% count(best_qualifier) %&gt;% mutate(percentage = n / sum(n)) %&gt;% filter(!is.na(best_qualifier)) %&gt;% ggplot(aes(y = fct_reorder(best_qualifier, Car), x = percentage, fill = Car)) + geom_bar(stat = &#39;identity&#39;, position = &#39;dodge&#39;, alpha = 0.5, col = &#39;black&#39;) + theme_bw() + scale_x_continuous(labels = scales::percent_format()) + facet_wrap(~ factor(Car, levels =c(&quot;Ferrari&quot;, &quot;Mercedes&quot;, &quot;Lotus Renault&quot;, &quot;Marussia Ferrari&quot;, &quot;STR Renault&quot;, &quot;Sauber Ferrari&quot;, &quot;Force India Mercedes&quot;, &quot;McLaren Mercedes&quot;, &quot;Williams Mercedes&quot;, &quot;Red Bull Racing Renault&quot;, &quot;Caterham Renault&quot;)), ncol = 1, strip.position=&quot;top&quot;, scales = &#39;free_y&#39;) + labs(y = &#39;Driver&#39;, x = &#39;Season-long Qualifying Win Proportion&#39;, title = &#39;Intra-team Qualifying Battles&#39;, subtitle = &#39;2015&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), plot.caption = element_text(hjust = 0)) + scale_fill_manual(values = c(&quot;red&quot;, &quot;cyan3&quot;, &quot;black&quot;, &quot;grey&quot;, &quot;darkslateblue&quot;, &quot;darkred&quot;, &quot;pink&quot;, &quot;darkorange&quot;, &quot;cornflowerblue&quot;, &quot;blue&quot;, &quot;navy&quot;, &quot;seagreen&quot;), breaks = c(&quot;Ferrari&quot;, &quot;Mercedes&quot;, &quot;Lotus Mercedes&quot;, &quot;Marussia Ferrari&quot;, &quot;STR Renault&quot;, &quot;Sauber Ferrari&quot;, &quot;Force India Mercedes&quot;, &quot;McLaren Honda&quot;, &quot;Williams Mercedes&quot;, &quot;Red Bull Racing Renault&quot;, &quot;Caterham Renault&quot;)) Mercedes power units claimed every pole of this inaugural season for the turbo-hybrid era. qualifying_allyears %&gt;% filter(!is.na(Q_secs), Year == 2014) %&gt;% group_by(Race) %&gt;% mutate(track_mean = mean(Q_secs, na.rm = T), Time_std_track = Q_secs - track_mean) %&gt;% ungroup() %&gt;% group_by(Driver) %&gt;% mutate(driver_n = n(), mean = mean(Time_std_track, na.rm = T)) %&gt;% ungroup() %&gt;% dplyr::filter(driver_n &gt; 15) %&gt;% ggplot(aes(Time_std_track, y = fct_reorder(Driver, desc(mean)))) + geom_point(position = position_jitter(w = 0, h = 0.1), alpha = 0.5) + geom_hline(yintercept = seq(37, 1, -2), col = &#39;grey&#39;, size = 3, alpha = 0.15) + theme_bw() + labs(x = &#39;Standardized Qualifying Time (secs)&#39;, y = &#39;Driver&#39;, title = &#39;Standardized Qualifying Time&#39;, subtitle = &#39;2014&#39;, caption = &#39;*Times are subtracted from the average time for each Grand Prix&#39;) + geom_vline(xintercept = 0, col = &#39;red&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), plot.caption = element_text(hjust = 0)) + xlim(-5, 10) 5.2.11 Next Chapter In the next chapter, we will explore the relationship between qualifying position and race placing. "],["modeling-race-results.html", "Chapter 6 Modeling Race Results 6.1 Starting Position vs Finish Position 6.2 Shortcomings of a simple linear regression model 6.3 Ordinal regression model 6.4 Explore Circuit Specific Models 6.5 Appendix: Probability distributions for every Grand Prix 6.6 Next Chapter", " Chapter 6 Modeling Race Results What determines success in a Grand Prix? Obviously, a multitude of factors do, but which ones specifically? And how much? These type of questions were essentially the motivation behind me writing this book. I was new to this sport, and had lots of questions. As a casual fan of Daily Fantasy Sports (DFS), I was curious to know what variables determine final placing in a race. Assuming something measurable does influence final placing, I wanted to build a model to describe the relationship (inferential model)and make predictions (predictive model). This chapter documents my modeling approach. 6.1 Starting Position vs Finish Position Let’s begin with a mostly uncontroversial statement: All other factors held constant, it is best to start the race in first place! How much does starting position influence a driver’s finishing place? Throughout this chapter, we will explore data and build models to answer this question. Using the drs package, we can pull all starting grid positions and Grand Prix finishing places for each race from 2014 to 2023. # Pull Race Data races2023 &lt;- race_result_scraper(2023) races2022 &lt;- race_result_scraper(2022) races2021 &lt;- race_result_scraper(2021) races2020 &lt;- race_result_scraper(2020) races2019 &lt;- race_result_scraper(2019) races2018 &lt;- race_result_scraper(2018) races2017 &lt;- race_result_scraper(2017) races2016 &lt;- race_result_scraper(2016) races2015 &lt;- race_result_scraper(2015) races2014 &lt;- race_result_scraper(2014) # Combine all race data races_allyears &lt;- rbind(races2023, races2022, races2021, races2020, races2019, races2018, races2017, races2016, races2015, races2014) # Pull Starting Grid Data grid2023 &lt;- starting_grid_scraper(2023) grid2022 &lt;- starting_grid_scraper(2022) grid2021 &lt;- starting_grid_scraper(2021) grid2020 &lt;- starting_grid_scraper(2020) grid2019 &lt;- starting_grid_scraper(2019) grid2018 &lt;- starting_grid_scraper(2018) grid2017 &lt;- starting_grid_scraper(2017) grid2016 &lt;- starting_grid_scraper(2016) grid2015 &lt;- starting_grid_scraper(2015) grid2014 &lt;- starting_grid_scraper(2014) # Combine all starting grid data grid_allyears &lt;- rbind(grid2023, grid2022, grid2021, grid2020, grid2019, grid2018, grid2017, grid2016, grid2015, grid2014) In the code above, I scrape results by year and bind all years together. Next, I’ll merge the starting grid positions with the race results for all Grands Prix from 2014 to 2023. ## Merge practice and qualifying data grids_and_races &lt;- races_allyears %&gt;% left_join(grid_allyears %&gt;% dplyr::select(Driver, Race, Circuit, Year, Position, Time_secs), by = c(&#39;Driver&#39;, &#39;Race&#39;, &#39;Circuit&#39;, &#39;Year&#39;), suffix = c(&quot;_race&quot;, &quot;_grid&quot;)) %&gt;% filter(!Position_race %in% c(&quot;NC&quot;, &quot;DQ&quot;), !is.na(Position_race), !is.na(Position_grid)) %&gt;% mutate(Position_race = as.numeric(Position_race), Position_grid = as.numeric(Position_grid)) As a sanity check, let’s take an initial look at our data. Does the data appear to support our hypothesis? Recall our hypothesis: It is best to start the race in 1st place. Or, we can use perhaps a more versatile version of our hypothesis: Starting grid position is positively correlated to finishing position. Below, I’ll plot starting grid position vs finishing position for each driver over this ten year period. I’ll include a best-fit line to highlight the overall relationship. Note: I jitter the data using position_jitter(). grids_and_races %&gt;% ggplot(aes(Position_grid, Position_race)) + geom_point(position = position_jitter(), alpha = 0.25) + geom_line(stat = &#39;smooth&#39;, se = F, method = &#39;lm&#39;, col = &#39;red&#39;, formula = y ~ 0 + x, size = 1, alpha = 0.75) + theme_bw() + labs(x = &#39;Starting Grid Position&#39;, y = &#39;Finishing Position&#39;, title = &#39;Starting Position vs Finish Position&#39;, subtitle = &#39;All Grands Prix: (2014 - 2023)&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) Clearly, there’s a linear relationship that supports our hypothesis. In chapter 4, we used a linear regression model to estimate the relationship between practice times and qualifying times. Linear regression is typically a fine choice to model a continuous response variable, so long as the assumptions of the model are met. There are four primary assumptions associated with simple linear regression (linear regression with only one predictor variable): Linearity: The relationship between the predictor variable and the mean of the response variable is linear. Homoscedasticity: The variance of a residual is the same for any value of the predictor variable. Independence: Observations are independent of each other. Normality: For any fixed value of the predictor variable, the response variable is normally distributed. In the next section, I’ll fit a simple linear regression model to this data. I will perform some diagnostic checks to ensure that we meet our assumptions. Spoiler alert: we don’t!. 6.2 Shortcomings of a simple linear regression model To start off, I’ll fit the linear regression with the following code: grid.lm &lt;- lm(Position_race ~ 0 + Position_grid, data = grids_and_races) We can use this model to make predictions for each starting position. The red line represents the predictions. Notice how the code below produces the same figure that we created above. # make predictions grid.lm.predictions &lt;- predict(grid.lm, grids_and_races) # Plot data with predictions grids_and_races %&gt;% bind_cols(pred = grid.lm.predictions) %&gt;% ggplot(aes(Position_grid, Position_race)) + geom_point(position = position_jitter(), alpha = 0.25) + geom_line(aes(y = pred), col = &#39;red&#39;, size = 1, alpha = 0.75) + theme_bw() + labs(x = &#39;Starting Grid Position&#39;, y = &#39;Finishing Position&#39;, title = &#39;Starting Position vs Finish Position&#39;, subtitle = &#39;All Grands Prix: (2014 - 2023)&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) Now, we can begin checking our assumptions for the simple linear regression model. 6.2.1 Linearity The relationship between the predictor variable and the mean of the response variable should be linear. Judging by the graph above, it certainly looks like we meet this assumption. We can also use a test to check this assumption. One such test is the Ramsey Regression Equation Specification Error Test (RESET). library(lmtest) resettest(grid.lm) ## ## RESET test ## ## data: grid.lm ## RESET = 264.3, df1 = 2, df2 = 3348, p-value &lt; 2.2e-16 The RESET performs a nested model comparison with the current model and the current model plus some polynomial terms. The results from this test on our model suggests that our model may be mis-specified. However, it is not recommended to begin thoughtlessly experimenting with additional terms. Let’s continue checking the other assumptions of the linear regression model. How to test for linearity in R The Ramsey Regression Equation Specification Error Test (RESET) can be used to detect specification errors in a linear model model. The RESET performs a nested model comparison between: the current model and, the current model plus some polynomial terms The resettest() returns the result of an F-test. A significant result is an indication that we need to further investigate the relationship between the predictors and the outcome. For more information on checking for linearity and addressing mis-specified models, visit this link: https://sscc.wisc.edu/sscc/pubs/RegDiag-R/linearity.html 6.2.2 Homoscedasticity The variance of a residual should be constant for any value of the predictor variable. To check this, we can first use a visualization. I’ll plot the residuals vs the starting grid positions for our model: # Bind the residuals and plot residuals vs predictor levels grids_and_races %&gt;% filter(!is.na(Position_grid), !is.na(Position_race)) %&gt;% bind_cols(res = grid.lm$residuals) %&gt;% ggplot(aes(Position_grid, res)) + geom_hline(yintercept = 0, col = &#39;red&#39;, alpha = 0.5) + geom_point(alpha = 0.25) + theme_bw() + labs(x = &#39;Starting Grid Position&#39;, y = &#39;Residual&#39;, title = &#39;Starting Position vs Residuals&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) This does not look good! We clearly have a pattern in our residual variance. Notice how the residuals steadily decline with starting grid position. Ideally, the residuals should be centered around 0 (the horizontal red line) with approximately equal variance (in both directions) across the plot. We can use a statistical test to quantify this violation. The Non-Constant Error Variance Test (ncvTest) is a useful test for the assumption of constant variance. Within the car package, there is a function called ncvTest() that will test whether the model demonstrates constant variance. I will use this function below: library(car) ncvTest(grid.lm) ## Non-constant Variance Score Test ## Variance formula: ~ fitted.values ## Chisquare = 26.98787, Df = 1, p = 2.0474e-07 How to test for Homoscedasticity in R There are a few options available, depending on your data and model. Two of the most common functions that I use are: The car package includes the ncvTest() function that performs a Non-Constant Error Variance Test. the lmtest packages includes the bptest() function that performs the Breusch-Pagan test against heteroskedasticity. The results from this test suggest a pretty severe violation of homoscedasticity. 6.2.3 Independence Observations should be independent of each other. The assumption of independence should be carefully investigated. However, merely using data visualizations or statistical tests is insufficient. A thorough discussion about statistical independence is outside of the scope of this book, but to learn more I recommend starting here: https://sscc.wisc.edu/sscc/pubs/RegDiag-R/independence.html 6.2.4 Normality For any fixed value of the predictor variable, the response variable should be normally distributed. One way to create a visual check for normality is by using a qq-plot. We can easily create a qq-plot in ggplot using the following code: ggplot() + geom_qq(aes(sample = rstandard(grid.lm))) + geom_abline(color = &quot;red&quot;) + coord_fixed() + theme_bw() Visually, there seems to be deviations at the upper and lower ends of our range. The ols_test_correlation() function from the olsrr package tests the correlation between the observed residuals and expected residuals under normality. I’ll run that test here: library(olsrr) ols_test_correlation(grid.lm) ## [1] 0.9860785 The olsrr package in R If you are new to R, the olsrr package can be very helpful. It is built with the aim of helping those users who are building models but are new to the R language. To learn more about the olsrr package, visit this link: https://cran.r-project.org/web/packages/olsrr/vignettes/residual_diagnostics.html That seems like a strong enough correlation, suggesting that the residuals are not normally distributed. But, let’s dive even deeper. If we look at the distribution of residuals at each starting position, we can see clear deviations from normality. I will plot the distribution of residuals for each starting grid position below. Each facet contains data for a given starting grid position. Notice that the distributional shapes at the front (P1, P2, P3) and back of the grid (P20, P21, P22) deviate considerably from a normal distribution. grids_and_races %&gt;% filter(!is.na(Position_grid), !is.na(Position_race)) %&gt;% bind_cols(res = grid.lm$residuals) %&gt;% ggplot(aes(res, y = ..density..)) + geom_vline(xintercept = 0, col = &#39;red&#39;, alpha = 0.5) + geom_histogram(alpha = 0.5) + theme_bw() + labs(x = &#39;Residual&#39;, y = &#39;&#39;, title = &#39;Distribution of Residuals at each Starting Grid Position&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) + facet_wrap(~ Position_grid, scales = &#39;free_y&#39;) To make this even easier to see, we can overlay a theoretical normal distribution over these plots. library(ggh4x) library(fitdistrplus) grids_and_races %&gt;% filter(!is.na(Position_grid), !is.na(Position_race)) %&gt;% bind_cols(res = grid.lm$residuals) %&gt;% ggplot(aes(res, y = ..density..)) + geom_vline(xintercept = 0, col = &#39;red&#39;, alpha = 0.5, size = 0.3) + geom_histogram(alpha = 0.5) + theme_bw() + labs(x = &#39;Residual&#39;, y = &#39;&#39;, title = &#39;Distribution of Residuals at each Starting Grid Position&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) + stat_theodensity(aes(y = after_stat(density)), distri = &#39;norm&#39;) + facet_wrap(~ Position_grid, scales = &#39;free_y&#39;) After seeing all of these figures and the results from the statistical tests, I feel strongly that our data does not meet the assumptions for the simple linear regression model. How to overlay a theoretical distribution in ggplot The ggh4x package has lots of fantastic functions for plotting. One of my favorites is the stat_theodensity() function. This function can be useful for comparing histograms or kernel density estimates against a theoretical distribution. For more information on the function, visit this link: https://www.rdocumentation.org/packages/ggh4x/versions/0.2.1/topics/stat_theodensity The ggh4x package proviedes various hacks for ggplot. For more information on the package, visit this link: https://cran.r-project.org/web/packages/ggh4x/readme/README.html If we can not use a linear regression model, what model should we use? When your data fails to meet the assumptions for a linear regression, it is common to consider a generalized linear model (GLM) as an alternative. Without going into too much detail, the choice of a particular GLM depends on the characteristics of the response variable. In our data, the response variable is Grand Prix final classification position (a.k.a. what place you finished the race in). This variable is not continuous, but is rather quite special. We could treat it as an integer, but it actually possesses a very special characteristic: it is an ordered factor, or ordinal variable. An ordinal variable is a categorical factor in which the levels of that factor demonstrate an ordering. For example, taco salsa at a restaurant may be categorized as mild, warm, and hot. Survey responses may include strongly disagree, disagree, neutral, agree, and strongly agree. This is no different in our data. P1 finishes ahead of P2, who finishes ahead of P3, and so on. This ordered nature of the response variable requires a different approach to modeling. I’ll discuss further in the next section! 6.3 Ordinal regression model Judging by our earlier plots at the beginning of this chapter, we can safely conclude that there is a pattern in the data; starting and finishing positions are positively correlated. However, we failed to meet the assumptions for a simple linear regression model. Luckily, there is a better suited model for this task: the Ordinal regression model. Ordinal regression can be used to estimate the association between a predictor variable (or several) and an ordinal outcome (finishing position). Rather than predicting the finishing position explicitly, this model estimates the odds (and probability) of finishing in a particular position. For this reason, we would need to slightly modify our research question to: What effect does starting grid position have on the probability of a finishing position in a Grand Prix? Note: the odds of an outcome are the ratio of the probability that the outcome occurs to the probability that the outcome does not occur. \\[ Odds = \\frac{Pr(outcome \\ occurs)}{Pr(outcome \\ does \\ not \\ occur)} \\] What is an Ordinal Regression model? An ordinal regression model estimates the probability of a given ordinal outcome, or a better outcome. Ordinal outcomes are ordered factors. In other words, an ordinal outcome is a categorical response variable that is ordered in some relevant way. For example, “Low/Medium/High” or “Bad/Good/Great” are both ordered factors with three levels. Finishing Place, “P1, P2, P3, etc.”, is also an ordered factor. We can use an ordinal regression model to estimate the probability of a driver finishing in a given position (or better). A helpful introduction to ordinal regression models can be found here: https://towardsdatascience.com/implementing-and-interpreting-ordinal-logistic-regression-1ee699274cf5 One of my favorite books that discusses ordinal regression models is Modern Applied Regressions by Jun Xu: https://a.co/d/dJgYJ5B Another great book that covers the topic is Applications of Regression for Categorical Outcomes Using R by David Melamed and Long Doan: https://www.routledge.com/Applications-of-Regression-for-Categorical-Outcomes-Using-R/Melamed-Doan/p/book/9781032509518 For a better understanding of ordinal regression models, I highly recommend the sources that I linked above. In my opinion, the best way to learn is through experimentation. So, I will go ahead and build an ordinal regression model, also known as a proportional odds model, to estimate the effect of starting grid position on the odds of a particular finishing position. This model will utilize all races across the ten year period (2014 to 2023). I removed DNFs (did not finish the race) from this data. In the code chunk below, I fit the model and print the summary output. library(VGAM) # fit the ordinal regression model overall_model &lt;- vglm(Position_race ~ Position_grid, family = cumulative(parallel = T, reverse = F), data = grids_and_races) # print summary of the model summary(overall_model) ## ## Call: ## vglm(formula = Position_race ~ Position_grid, family = cumulative(parallel = T, ## reverse = F), data = grids_and_races) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept):1 -0.644045 0.087340 -7.374 1.66e-13 *** ## (Intercept):2 0.329305 0.071981 4.575 4.76e-06 *** ## (Intercept):3 1.022282 0.069060 14.803 &lt; 2e-16 *** ## (Intercept):4 1.598608 0.070211 22.769 &lt; 2e-16 *** ## (Intercept):5 2.114212 0.073265 28.857 &lt; 2e-16 *** ## (Intercept):6 2.592038 0.077363 33.505 &lt; 2e-16 *** ## (Intercept):7 3.046305 0.082086 37.111 &lt; 2e-16 *** ## (Intercept):8 3.483734 0.087173 39.964 &lt; 2e-16 *** ## (Intercept):9 3.918157 0.092562 42.330 &lt; 2e-16 *** ## (Intercept):10 4.349819 0.098091 44.345 &lt; 2e-16 *** ## (Intercept):11 4.778810 0.103624 46.117 &lt; 2e-16 *** ## (Intercept):12 5.213809 0.109165 47.761 &lt; 2e-16 *** ## (Intercept):13 5.666237 0.114784 49.364 &lt; 2e-16 *** ## (Intercept):14 6.151682 0.120650 50.988 &lt; 2e-16 *** ## (Intercept):15 6.687755 0.127085 52.624 &lt; 2e-16 *** ## (Intercept):16 7.278569 0.134653 54.054 &lt; 2e-16 *** ## (Intercept):17 8.030367 0.146859 54.681 &lt; 2e-16 *** ## (Intercept):18 8.922927 0.170686 52.277 &lt; 2e-16 *** ## (Intercept):19 9.993626 0.228059 43.820 &lt; 2e-16 *** ## (Intercept):20 11.286906 0.377899 29.868 &lt; 2e-16 *** ## (Intercept):21 12.548708 0.677595 18.519 &lt; 2e-16 *** ## Position_grid -0.348591 0.007456 -46.753 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Number of linear predictors: 21 ## ## Residual deviance: 16944.08 on 70349 degrees of freedom ## ## Log-likelihood: -8472.041 on 70349 degrees of freedom ## ## Number of Fisher scoring iterations: 7 ## ## Warning: Hauck-Donner effect detected in the following estimate(s): ## &#39;(Intercept):17&#39;, &#39;(Intercept):18&#39;, &#39;(Intercept):19&#39;, &#39;(Intercept):20&#39;, &#39;(Intercept):21&#39; ## ## ## Exponentiated coefficients: ## Position_grid ## 0.7056819 How do I interpret these results? The interpretation can get a bit tedious. If you look at the bottom of the summary output, you will find an exponentiated coefficient for starting grid position (Position_grid = 0.7056819). We can interpret this value as: For each place improvement in starting grid position, the odds of that driver finishing the race in a better position (i.e. P1 vs P2 or P19 vs P20) is 0.29 (1 - 0.71) times higher (than the driver starting the race one position lower on the grid. The interpretation of the coefficient is not super intuitive. It makes sense, but it takes some effort to sort out. Another way to visualize this model’s results is by making predictions with the model. Below, I’ll simulate a starting grid, and predict the final placing for each starting grid position. These predictions are actually probabilities of each finishing position. For example, this model estimates the probability that the pole-sitter finishes the race in P1, P2, P3, etc. These probabilities will include a 95% confidence interval. # Create a placeholder dataframe with all possible starting positions starting_grid = data.frame(Position_grid = 1:20) # Create a placeholder dataframe with names for predictions and standard errors. placing_df &lt;- data.frame(prob = rep(&#39;prob&#39;, 21)) %&gt;% mutate(y = 1:n()) %&gt;% unite(prob, prob, y, remove = F) %&gt;% mutate(se = rep(&#39;se&#39;, 21)) %&gt;% unite(se, se, y, remove = F) # Make Predictions for each Finishing position (1:20), using each Starting Position (1:20) predictions &lt;- as.data.frame(predictvglm(overall_model, newdata = starting_grid, se.fit = T)$fitted.values) # rename the columns names(predictions) &lt;- placing_df$prob # Extract the standard error for each prediction standard_errors &lt;- as.data.frame(predictvglm(overall_model, newdata = starting_grid, se.fit = T)$se.fit) # rename the standard error names(standard_errors) &lt;- placing_df$se ## As an example, I&#39;ll select the predictions and standard error for: Probability(P1 finish | P1 starting grid) # Bind the predictions and standard errors together, and convert the odds to probabilities. starting_grid_coef_finish1 &lt;- starting_grid %&gt;% bind_cols(pr_1 = predictions$prob_1, se_1 = standard_errors$se_1) %&gt;% mutate(pr = exp(pr_1), lower = exp(pr_1 - se_1 * 1.96), upper = exp(pr_1 + se_1 * 1.96)) %&gt;% mutate( pr = pr / (1 + pr), lower = lower / (1 + lower), upper = upper / (1 + upper)) What is the probability of winning a race? Now that the predictions are made, I will use ggplot() to plot the probability of winning a race given each potential starting position, P(P1 finish | a starting grid position): starting_grid_coef_finish1 %&gt;% ggplot(aes(Position_grid, y = pr, ymin = lower, ymax = upper)) + geom_pointrange() + theme_bw() + labs( y = &#39;Probability of finishing P1&#39;, x = &#39;Starting Grid Position&#39;, title = &#39;Ordinal Regression Model Predictions&#39;, subtitle = &#39;All Grands Prix: (2014 - 2023)&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) If starting from P1, a driver has a 27% chance of winning the race, on average. If starting from P2, but still on the front row, a driver’s chance of winning drops to 21%. What is the probability of finishing on the podium? We can use the same approach to estimate the probability of a podium given a starting grid position. ## As an example, I&#39;ll select the predictions and standard error for: Probability(Podium | a starting grid) # Bind the predictions and standard errors together, and convert the odds to probabilities. starting_grid_coef_podium &lt;- starting_grid %&gt;% bind_cols(pr_1 = predictions$prob_3, se_1 = standard_errors$se_3) %&gt;% mutate(pr = exp(pr_1), lower = exp(pr_1 - se_1 * 1.96), upper = exp(pr_1 + se_1 * 1.96)) %&gt;% mutate( pr = pr / (1 + pr), lower = lower / (1 + lower), upper = upper / (1 + upper)) starting_grid_coef_podium %&gt;% ggplot(aes(Position_grid, y = pr, ymin = lower, ymax = upper)) + geom_pointrange() + theme_bw() + labs( y = &#39;Probability of a podium&#39;, x = &#39;Starting Grid Position&#39;, title = &#39;Ordinal Regression Model Predictions&#39;, subtitle = &#39;All Grands Prix: (2014 - 2023)&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) A driver starting from P1 has a 66% chance of finishing on the podium. I like those odds! A driver starting from P5 still has a 1 in 3 chance of finishing on the podium. In my opinion, probabilities are more intuitive than odds. For that reason, I will use probabilities throughout the rest of the chapter. 6.3.1 Prediction Grid What if we wanted to know the probability of all finishing positions given each starting position? We can use a similar approach. I will plot the model predictions in a heat map below. Each tile represents the: Probability(a finishing position | a starting grid position). To create this plot, I first need to re-shape the dataframe to a long format indexed by starting and finish position. I will also add a new column that contains the discrete probability of a given finish. Because the proportional odds model output is converted to a probabilistic statement like Probability(finishing P3 or better | a starting position), the discrete probability column will give a statement like Probability(finishing P3 | a starting position). preds_se_full &lt;- predictions %&gt;% mutate(starting = 1:n()) %&gt;% pivot_longer(- starting, names_to = &#39;placing&#39;, values_to = &#39;prob&#39;) %&gt;% bind_cols(standard_errors %&gt;% mutate(starting = 1:n()) %&gt;% pivot_longer(- starting, names_to = &#39;placing&#39;, values_to = &#39;se&#39;) %&gt;% dplyr::select(se)) %&gt;% mutate(placing = as.numeric(str_remove(placing, &#39;prob_&#39;))) %&gt;% mutate(prob = exp(prob)) %&gt;% mutate(prob = prob / (1 + prob)) %&gt;% group_by(starting) %&gt;% mutate(discrete_prob = ifelse(is.na(prob - lag(prob)), prob, (prob - lag(prob)))) %&gt;% ungroup() Now that the data is re-shaped, I’ll plot the discrete and cumulative probability heatmaps. # Plot Discrete Probability heatmap preds_se_full %&gt;% ggplot(aes(starting, placing, fill = discrete_prob)) + geom_tile(col = &#39;white&#39;, size = 0.1) + theme_tufte(base_family=&quot;Helvetica&quot;) + scale_fill_viridis_c(option = &#39;magma&#39;) + coord_equal() + theme(axis.ticks=element_blank()) + labs(x = &#39;Starting Grid&#39;, y = &#39;Final Placing&#39;, fill = &#39;Probability&#39;, title = expression(Discrete~Probability~of~finishing~P[n])) + scale_x_continuous(breaks = c(1:20,1)) + scale_y_continuous(breaks = c(1:21,1)) + theme(plot.title = element_text(hjust = 0.5)) # Plot Cumulative Probability heatmap preds_se_full %&gt;% ggplot(aes(starting, placing, fill = prob)) + geom_tile(col = &#39;white&#39;, size = 0.1) + theme_tufte(base_family=&quot;Helvetica&quot;) + scale_fill_viridis_c(option = &#39;magma&#39;) + coord_equal() + theme(axis.ticks=element_blank()) + labs(x = &#39;Starting Grid&#39;, y = &#39;Final Placing&#39;, fill = &#39;Probability&#39;, title = expression(Probability~of~finishing~P[n]~or~better)) + scale_x_continuous(breaks = c(1:20,1)) + scale_y_continuous(breaks = c(1:21,1)) + theme(plot.title = element_text(hjust = 0.5)) The cumulative probability plot is helpful, but the discrete probability plot is a bit tough to follow. Let’s try plotting this information a different way. Rather than using a heat map, I will combine the probabilities into distributions using the code below. preds_se_full %&gt;% mutate(starting_pos = as.factor(paste0(&#39;P&#39;,starting)), starting_pos = fct_reorder(starting_pos, starting)) %&gt;% ggplot(aes(placing, discrete_prob)) + geom_point() + geom_segment(aes(x = placing, xend = placing, y = 0, yend = discrete_prob), size = 2, alpha = 0.5, col = &#39;cornflowerblue&#39;) + theme_bw() + facet_wrap(~ starting, labeller = label_both) + labs(x = &#39;Final Race Classification&#39;, y = &#39;Probability&#39;, title = expression(Discrete~Probability~of~finishing~P[n]), subtitle = &#39;All races: 2014 - 2023&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) The plot above shows the probability of finishing at any position given where the driver starts the race. In my opinion, this is a more interpretable representation of the data. You can track the shift in distributions as you move down the grid. It is important to keep in mind what data this model is built on. These predictions are generated solely from starting position. But, we know there is much more involved in where you finish. Otherwise, why even run the race?! Nonetheless, this data can be used to answer our hypothesis directly and it establishes a nice baseline to build upon. In the next section, I will try expanding this model to include additional variables. 6.3.2 Yearly Differences in Ordinal Regression Results Does this relationship between starting and finishing position vary over time? In the section, I will explore yearly differences in ordinal regression results. I will start by looking at changes occurring in 2022. After a forty-year ban, ground effect returned to Formula 1 in 2022. The goal for a Formula 1 design team is to optimize ground effect so that the bottom of the car generates low pressure, which will suck the car to the track. These new regulations in 2022 aimed to enable cars to follow more closely and overtake more easily. Theoretically, this change should decrease the correlation between starting position and finishing position. Is this represented in the data? Let’s model it and see! I will fit an ordinal regression model for each year from 2014 to 2023. I will start with the pre-ground effect era. Here’s a model for each year from 2014 through 2021, which I will refer to as the turbo-hybrid era. model14 &lt;- vglm(Position_race ~ Position_grid, family = cumulative(parallel = T, reverse = F), data = grids_and_races %&gt;% filter(Year == 2014)) model15 &lt;- vglm(Position_race ~ Position_grid, family = cumulative(parallel = T, reverse = F), data = grids_and_races %&gt;% filter(Year == 2015)) model16 &lt;- vglm(Position_race ~ Position_grid, family = cumulative(parallel = T, reverse = F), data = grids_and_races %&gt;% filter(Year == 2016)) model17 &lt;- vglm(Position_race ~ Position_grid, family = cumulative(parallel = T, reverse = F), data = grids_and_races %&gt;% filter(Year == 2017)) model18 &lt;- vglm(Position_race ~ Position_grid, family = cumulative(parallel = T, reverse = F), data = grids_and_races %&gt;% filter(Year == 2018)) model19 &lt;- vglm(Position_race ~ Position_grid, family = cumulative(parallel = T, reverse = F), data = grids_and_races %&gt;% filter(Year == 2019)) model20 &lt;- vglm(Position_race ~ Position_grid, family = cumulative(parallel = T, reverse = F), data = grids_and_races %&gt;% filter(Year == 2020)) model21 &lt;- vglm(Position_race ~ Position_grid, family = cumulative(parallel = T, reverse = F), data = grids_and_races %&gt;% filter(Year == 2021)) #exponentiate coefficient turbo_hybrid_coefs &lt;- data.frame(Position_grid = rbind(summary(model14)@post$expcoeffs, summary(model15)@post$expcoeffs, summary(model16)@post$expcoeffs, summary(model17)@post$expcoeffs, summary(model18)@post$expcoeffs, summary(model19)@post$expcoeffs, summary(model20)@post$expcoeffs, summary(model21)@post$expcoeffs), Year = c(2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021), era = &#39;turbo-hybrid&#39;) Now, I’ll fit two models for the recent ground effect era (2022 and 2023). model22 &lt;- vglm(Position_race ~ Position_grid, family = cumulative(parallel = T, reverse = F), data = grids_and_races %&gt;% filter(Year == 2022)) model23 &lt;- vglm(Position_race ~ Position_grid, family = cumulative(parallel = T, reverse = F), data = grids_and_races %&gt;% filter(Year == 2023)) Here, I will tidy up the model coefficient estimates and bind the two eras together. #exponentiate coefficient ground_effects_coefs &lt;- data.frame(Position_grid = rbind(summary(model22)@post$expcoeffs, summary(model23)@post$expcoeffs), Year = c(2022, 2023), era = &#39;ground effects&#39;) # combine the coefficients for these two eras all_coefs &lt;- turbo_hybrid_coefs %&gt;% bind_rows(ground_effects_coefs) %&gt;% mutate(era = factor(era, levels = c(&#39;turbo-hybrid&#39;, &#39;ground effects&#39;))) Finally, I can plot model coefficient estimates by year. In this plot, I use a color fill to distinguish the pre-ground effect turbo-hybrid era (a pink-red color is used for years 2014 - 2021) from the ground effect era (a cyan color is used for 2022 and 2023). all_coefs %&gt;% ggplot(aes(Year, Position_grid)) + annotate(geom = &#39;rect&#39;, xmin = -Inf, xmax = 2021.5, ymin = -Inf, ymax = Inf, fill = &#39;#F8766D&#39;, alpha = 0.25) + annotate(geom = &#39;rect&#39;, xmin = 2021.5, xmax = Inf, ymin = -Inf, ymax = Inf, fill = &#39;#00BFC4&#39;, alpha = 0.25) + geom_vline(xintercept = 2021.5, col = &#39;grey&#39;) + geom_point(aes(fill = era), pch = 22) + geom_point() + geom_line() + theme_bw() + labs(fill = &#39;&#39;, x = &#39;Year&#39;, y = &#39;Coefficient Estimate for Starting Grid Position&#39;, title = &#39;Change in Ordinal Regression Model Coefficient Estimates&#39;) + scale_x_continuous(labels = seq(2014, 2023, 1), breaks = seq(2014, 2023, 1)) + theme(legend.position = &#39;bottom&#39;, plot.title = element_text(hjust = 0.5)) The plot above is not very easy to interpret. But, it is basically demonstrating that starting grid position matters slightly less in 2022 and 2023, relative to prior years. So, this does support the idea that the ground effect regulations increase a car’s ability to make up positions. However, the plot leaves much to be desired in terms of intuitiveness. As an alternative, we can plot predicted probabilities for each year. This will require more effort to tidy up the probabilities and standard errors, but it gives us a much more granular insight into yearly differences. In the following code chunks, I will construct a dataframe for each year’s model that contains a predicted probability for each starting and finishing position combination. These probabilities are then ordered by starting position and grouped by finishing position to construct a distribution. Here’s how I tidy up data for 2014 through 2021. ## 2014 preds_2014 &lt;- data.frame(predictvglm(model14, newdata = starting_grid, se.fit = T)$fitted.values) # rename the columns names(preds_2014) &lt;- placing_df$prob[-21] preds_2014 &lt;- preds_2014 %&gt;% mutate(Year = 2014) %&gt;% bind_cols(starting_grid) # Extract the standard error for each prediction standard_errors_2014 &lt;- as.data.frame(predictvglm(model14, newdata = starting_grid, se.fit = T)$se.fit) # rename the standard error names(standard_errors_2014) &lt;- placing_df$se[-21] ## 2015 preds_2015 &lt;- data.frame(predictvglm(model15, newdata = starting_grid, se.fit = T)$fitted.values) # rename the columns names(preds_2015) &lt;- placing_df$prob[1:19] preds_2015 &lt;- preds_2015 %&gt;% mutate(Year = 2015) %&gt;% bind_cols(starting_grid) # Extract the standard error for each prediction standard_errors_2015 &lt;- as.data.frame(predictvglm(model15, newdata = starting_grid, se.fit = T)$se.fit) # rename the standard error names(standard_errors_2015) &lt;- placing_df$se[1:19] ## 2016 preds_2016 &lt;- data.frame(predictvglm(model16, newdata = starting_grid, se.fit = T)$fitted.values) # rename the columns names(preds_2016) &lt;- placing_df$prob preds_2016 &lt;- preds_2016 %&gt;% mutate(Year = 2016) %&gt;% bind_cols(starting_grid) # Extract the standard error for each prediction standard_errors_2016 &lt;- as.data.frame(predictvglm(model16, newdata = starting_grid, se.fit = T)$se.fit) # rename the standard error names(standard_errors_2016) &lt;- placing_df$se ## 2017 preds_2017 &lt;- data.frame(predictvglm(model17, newdata = starting_grid, se.fit = T)$fitted.values) # rename the columns names(preds_2017) &lt;- placing_df$prob[1:17] preds_2017 &lt;- preds_2017 %&gt;% mutate(Year = 2017) %&gt;% bind_cols(starting_grid) # Extract the standard error for each prediction standard_errors_2017 &lt;- as.data.frame(predictvglm(model17, newdata = starting_grid, se.fit = T)$se.fit) %&gt;% mutate(Year = 2017) %&gt;% bind_cols(starting_grid) # rename the standard error names(standard_errors_2017) &lt;- placing_df$se[1:18] ## 2018 preds_2018 &lt;- data.frame(predictvglm(model18, newdata = starting_grid, se.fit = T)$fitted.values) # rename the columns names(preds_2018) &lt;- placing_df$prob[1:19] preds_2018 &lt;- preds_2018 %&gt;% mutate(Year = 2018) %&gt;% bind_cols(starting_grid) # Extract the standard error for each prediction standard_errors_2018 &lt;- as.data.frame(predictvglm(model18, newdata = starting_grid, se.fit = T)$se.fit) %&gt;% mutate(Year = 2018) %&gt;% bind_cols(starting_grid) # rename the standard error names(standard_errors_2018) &lt;- placing_df$se[1:20] ## 2019 preds_2019 &lt;- data.frame(predictvglm(model19, newdata = starting_grid, se.fit = T)$fitted.values) # rename the columns names(preds_2019) &lt;- placing_df$prob[1:19] preds_2019 &lt;- preds_2019 %&gt;% mutate(Year = 2019) %&gt;% bind_cols(starting_grid) # Extract the standard error for each prediction standard_errors_2019 &lt;- as.data.frame(predictvglm(model19, newdata = starting_grid, se.fit = T)$se.fit) %&gt;% mutate(Year = 2019) %&gt;% bind_cols(starting_grid) # rename the standard error names(standard_errors_2019) &lt;- placing_df$se[1:20] ## 2020 preds_2020 &lt;- data.frame(predictvglm(model20, newdata = starting_grid, se.fit = T)$fitted.values) # rename the columns names(preds_2020) &lt;- placing_df$prob[1:18] preds_2020 &lt;- preds_2020 %&gt;% mutate(Year = 2020) %&gt;% bind_cols(starting_grid) # Extract the standard error for each prediction standard_errors_2020 &lt;- as.data.frame(predictvglm(model20, newdata = starting_grid, se.fit = T)$se.fit) %&gt;% mutate(Year = 2020) %&gt;% bind_cols(starting_grid) # rename the standard error names(standard_errors_2020) &lt;- placing_df$se[1:19] ## 2021 preds_2021 &lt;- data.frame(predictvglm(model21, newdata = starting_grid, se.fit = T)$fitted.values) # rename the columns names(preds_2021) &lt;- placing_df$prob[1:19] preds_2021 &lt;- preds_2021 %&gt;% mutate(Year = 2021) %&gt;% bind_cols(starting_grid) # Extract the standard error for each prediction standard_errors_2021 &lt;- as.data.frame(predictvglm(model21, newdata = starting_grid, se.fit = T)$se.fit) %&gt;% mutate(Year = 2021) %&gt;% bind_cols(starting_grid) # rename the standard error names(standard_errors_2021) &lt;- placing_df$se[1:20] And, here’s tidied dataframes for 2022 and 2023. ## 2022 preds_2022 &lt;- data.frame(predictvglm(model22, newdata = starting_grid, se.fit = T)$fitted.values) # rename the columns names(preds_2022) &lt;- placing_df$prob[1:19] preds_2022 &lt;- preds_2022 %&gt;% mutate(Year = 2022) %&gt;% bind_cols(starting_grid) # Extract the standard error for each prediction standard_errors_2022 &lt;- as.data.frame(predictvglm(model22, newdata = starting_grid, se.fit = T)$se.fit) %&gt;% mutate(Year = 2022) %&gt;% bind_cols(starting_grid) # rename the standard error names(standard_errors_2022) &lt;- placing_df$se[1:20] ## 2023 preds_2023 &lt;- data.frame(predictvglm(model23, newdata = starting_grid, se.fit = T)$fitted.values) # rename the columns names(preds_2023) &lt;- placing_df$prob[1:19] preds_2023 &lt;- preds_2023 %&gt;% mutate(Year = 2023) %&gt;% bind_cols(starting_grid) # Extract the standard error for each prediction standard_errors_2023 &lt;- as.data.frame(predictvglm(model23, newdata = starting_grid, se.fit = T)$se.fit) %&gt;% mutate(Year = 2023) %&gt;% bind_cols(starting_grid) # rename the standard error names(standard_errors_2023) &lt;- placing_df$se[1:20] How does starting position influence a driver’s probability of winning a race over the last 10 years? To answer this question, we can plot a yearly probability curve for finishing in P1 (across all starting grid positions). In the code chunk below, I will bind together each year’s dataframe and plot the data. rbind(preds_2014[, c(&#39;prob_1&#39;, &#39;Year&#39;, &#39;Position_grid&#39;)], preds_2015[, c(&#39;prob_1&#39;, &#39;Year&#39;, &#39;Position_grid&#39;)], preds_2016[, c(&#39;prob_1&#39;, &#39;Year&#39;, &#39;Position_grid&#39;)], preds_2017[, c(&#39;prob_1&#39;, &#39;Year&#39;, &#39;Position_grid&#39;)], preds_2018[, c(&#39;prob_1&#39;, &#39;Year&#39;, &#39;Position_grid&#39;)], preds_2019[, c(&#39;prob_1&#39;, &#39;Year&#39;, &#39;Position_grid&#39;)], preds_2020[, c(&#39;prob_1&#39;, &#39;Year&#39;, &#39;Position_grid&#39;)], preds_2021[, c(&#39;prob_1&#39;, &#39;Year&#39;, &#39;Position_grid&#39;)], preds_2022[, c(&#39;prob_1&#39;, &#39;Year&#39;, &#39;Position_grid&#39;)], preds_2023[, c(&#39;prob_1&#39;, &#39;Year&#39;, &#39;Position_grid&#39;)]) %&gt;% mutate(pr = exp(prob_1), Year = factor(Year)) %&gt;% mutate( pr = pr / (1 + pr)) %&gt;% ggplot(aes(Position_grid, y = pr, group = Year, col = Year)) + geom_line(size = 1) + geom_point(size = 2, pch = 21, col = &#39;black&#39;, aes(fill = Year)) + theme_bw() + labs( y = &#39;Probability of finishing P1&#39;, x = &#39;Starting Grid Position&#39;, title = &#39;Model Results by Year&#39;) + scale_x_continuous(labels = seq(1, 20, 1), breaks = seq(1, 20, 1)) + theme(plot.title = element_text(hjust = 0.5)) This is tough to make sense of because the curves are so close to eachother. One possible solution is to use facet_zoom() to zoom in on podium places. library(ggforce) rbind(preds_2014[, c(&#39;prob_1&#39;, &#39;Year&#39;, &#39;Position_grid&#39;)], preds_2015[, c(&#39;prob_1&#39;, &#39;Year&#39;, &#39;Position_grid&#39;)], preds_2016[, c(&#39;prob_1&#39;, &#39;Year&#39;, &#39;Position_grid&#39;)], preds_2017[, c(&#39;prob_1&#39;, &#39;Year&#39;, &#39;Position_grid&#39;)], preds_2018[, c(&#39;prob_1&#39;, &#39;Year&#39;, &#39;Position_grid&#39;)], preds_2019[, c(&#39;prob_1&#39;, &#39;Year&#39;, &#39;Position_grid&#39;)], preds_2020[, c(&#39;prob_1&#39;, &#39;Year&#39;, &#39;Position_grid&#39;)], preds_2021[, c(&#39;prob_1&#39;, &#39;Year&#39;, &#39;Position_grid&#39;)], preds_2022[, c(&#39;prob_1&#39;, &#39;Year&#39;, &#39;Position_grid&#39;)], preds_2023[, c(&#39;prob_1&#39;, &#39;Year&#39;, &#39;Position_grid&#39;)]) %&gt;% mutate(pr = exp(prob_1), Year = factor(Year)) %&gt;% mutate( pr = pr / (1 + pr)) %&gt;% ggplot(aes(Position_grid, y = pr, group = Year, col = Year)) + geom_line(size = 1) + geom_point(size = 2, pch = 21, col = &#39;black&#39;, aes(fill = Year)) + theme_bw() + labs( y = &#39;Probability of finishing P1&#39;, x = &#39;Starting Grid Position&#39;, title = &#39;Model Results by Year&#39;, subtitle = &#39;2021 - 2023&#39;) + scale_x_continuous(labels = seq(1, 20, 1), breaks = seq(1, 20, 1)) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), legend.position = &#39;bottom&#39;) + facet_zoom(xlim = c(1,3)) Not much better! But, it was worth giving facet_zoom() a shot. Zooming in on data with ggplot The ggforce package has a helpful function called facet_zoom(). This facetting function provides allows you to zoom in on a subset of the data, while keeping the view of the full dataset as a separate panel. The zoomed-in area will be indicated on the full dataset panel for reference. For more information on facet_zoom(), check this out: https://ggforce.data-imaginist.com/reference/facet_zoom.html To learn about the ggforce package, see this link: https://ggforce.data-imaginist.com/index.html As an alternative, let’s filter the data by 2021, 2022, and 2023 only. rbind(preds_2021[, c(&#39;prob_1&#39;, &#39;Year&#39;, &#39;Position_grid&#39;)], preds_2022[, c(&#39;prob_1&#39;, &#39;Year&#39;, &#39;Position_grid&#39;)], preds_2023[, c(&#39;prob_1&#39;, &#39;Year&#39;, &#39;Position_grid&#39;)]) %&gt;% mutate(pr = exp(prob_1), Year = factor(Year)) %&gt;% mutate( pr = pr / (1 + pr)) %&gt;% ggplot(aes(Position_grid, y = pr, group = Year, col = Year)) + geom_line(size = 1) + geom_point(size = 2, pch = 21, col = &#39;black&#39;, aes(fill = Year)) + theme_bw() + labs( y = &#39;Probability of finishing P1&#39;, x = &#39;Starting Grid Position&#39;, title = &#39;Model Results by Year&#39;, subtitle = &#39;2021 - 2023&#39;) + scale_x_continuous(labels = seq(1, 20, 1), breaks = seq(1, 20, 1)) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), legend.position = &#39;bottom&#39;) + facet_zoom(xlim = c(1,3)) This is better. It looks like drivers in 2022 and 2023 had a better chance of winning a race from 5th or worse position on the starting grid. In 2021, drivers who started from the first two rows of the grid had a better chance of winning than they did in 2022. Are these differences due to the new regulations, or is this simply capturing the fact that Verstappen won from 7th, 10th, and 14th with the recent Red Bull car? Not sure! What about podiums? Because this model estimates the cumulative probability, the model is already estimating a probability of a podium (i.e. Probability(finish &lt;= P3 | a starting position). So, I will plot these below. rbind(preds_2021[, c(&#39;prob_3&#39;, &#39;Year&#39;, &#39;Position_grid&#39;)], preds_2022[, c(&#39;prob_3&#39;, &#39;Year&#39;, &#39;Position_grid&#39;)], preds_2023[, c(&#39;prob_3&#39;, &#39;Year&#39;, &#39;Position_grid&#39;)]) %&gt;% mutate(pr = exp(prob_3), Year = factor(Year)) %&gt;% mutate( pr = pr / (1 + pr)) %&gt;% ggplot(aes(Position_grid, y = pr, group = Year, col = Year)) + geom_line(size = 1) + geom_point(size = 2, pch = 21, col = &#39;black&#39;, aes(fill = Year)) + theme_bw() + labs( y = &#39;Probability of finishing on the podium&#39;, x = &#39;Starting Grid Position&#39;, title = &#39;Model Results by Year&#39;, subtitle = &#39;2021 - 2023&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), legend.position = &#39;bottom&#39;) The podium probabilities are even more compelling. Drivers starting from 5th or lower have an even greater chance of finishing on the podium in 2022 and 2023. In the next section, we will look into circuit-specific models. 6.4 Explore Circuit Specific Models It is generally accepted among racing teams, drivers, and enthusiasts that overtaking (or passing opponents) is easier at some circuits compared to others. Let’s explore whether there are indeed differences in our model coefficients across different circuits. Note: Throughout this section, I will provide minimal commentary about the plots. This section is primarily for referencing and gee-whiz purposes. 6.4.1 Bahrain The Bahrain Grand Prix is the first race of the season. What type of finish might we expect, if only using starting grid position as a predictor? Let’s fit the same model to one track. bahrain_model &lt;- vglm(Position_race ~ Position_grid, family = cumulative(parallel = T, reverse = F), data = grids_and_races %&gt;% filter(Race == &#39;bahrain&#39;)) Now, we can make predictions for the grid. # Make Predictions for each Finishing position (1:20), using each Starting Position (1:20) predictions_bahrain &lt;- as.data.frame(predictvglm(bahrain_model, newdata = starting_grid, se.fit = T)$fitted.values) # rename the columns names(predictions_bahrain) &lt;- placing_df$prob[1:18] # Extract the standard error for each prediction standard_errors_bahrain &lt;- as.data.frame(predictvglm(bahrain_model, newdata = starting_grid, se.fit = T)$se.fit) # rename the standard error names(standard_errors_bahrain) &lt;- placing_df$se[1:18] And, we can plot the data using this code: starting_grid %&gt;% bind_cols(pr_1 = predictions_bahrain$prob_1, se_1 = standard_errors_bahrain$se_1) %&gt;% mutate(pr = exp(pr_1), lower = exp(pr_1 - se_1 * 1.96), upper = exp(pr_1 + se_1 * 1.96)) %&gt;% mutate( pr = pr / (1 + pr), lower = lower / (1 + lower), upper = upper / (1 + upper)) %&gt;% ggplot(aes(Position_grid, y = pr, ymin = lower, ymax = upper)) + geom_pointrange() + theme_bw() + labs( y = &#39;Probability of finishing P1&#39;, x = &#39;Starting Grid Position&#39;, title = &#39;Bahrain Grand Prix&#39;, subtitle = &#39;Probability of a win&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), legend.position = &#39;bottom&#39;) What’s the probability of a podium? We can extract these probabilities (and standard errors), and plot with ggplot. starting_grid %&gt;% bind_cols(pr_3 = predictions_bahrain$prob_3, se_3 = standard_errors_bahrain$se_3) %&gt;% mutate(pr = exp(pr_3), lower = exp(pr_3 - se_3 * 1.96), upper = exp(pr_3 + se_3 * 1.96)) %&gt;% mutate( pr = pr / (1 + pr), lower = lower / (1 + lower), upper = upper / (1 + upper)) %&gt;% ggplot(aes(Position_grid, y = pr, ymin = lower, ymax = upper)) + geom_pointrange() + theme_bw() + labs( y = &#39;Probability of a podium&#39;, x = &#39;Starting Grid Position&#39;, title = &#39;Bahrain Gand Prix&#39;, subtitle = &#39;Probability of a podium&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), legend.position = &#39;bottom&#39;) And now, I’ll construct probability distributions for each finishing position given a starting a position at the Bahrain GP. bahrain_preds_se_full &lt;- predictions_bahrain %&gt;% mutate(starting = 1:n()) %&gt;% pivot_longer(- starting, names_to = &#39;placing&#39;, values_to = &#39;prob&#39;) %&gt;% bind_cols(standard_errors_bahrain %&gt;% mutate(starting = 1:n()) %&gt;% pivot_longer(- starting, names_to = &#39;placing&#39;, values_to = &#39;se&#39;) %&gt;% dplyr::select(se)) %&gt;% mutate(placing = as.numeric(str_remove(placing, &#39;prob_&#39;))) %&gt;% mutate(prob = exp(prob)) %&gt;% mutate(prob = prob / (1 + prob)) %&gt;% group_by(starting) %&gt;% mutate(discrete_prob = ifelse(is.na(prob - lag(prob)), prob, (prob - lag(prob)))) %&gt;% ungroup() bahrain_preds_se_full %&gt;% mutate(starting_pos = as.factor(paste0(&#39;P&#39;,starting)), starting_pos = fct_reorder(starting_pos, starting)) %&gt;% ggplot(aes(placing, discrete_prob)) + geom_point() + geom_segment(aes(x = placing, xend = placing, y = 0, yend = discrete_prob), size = 2, alpha = 0.75, col = &#39;maroon&#39;) + theme_bw() + facet_wrap(~ starting, labeller = label_both) + labs(x = &#39;Final Race Classification&#39;, y = &#39;Probability&#39;, title = expression(Discrete~Probability~of~finishing~P[n]), subtitle = &#39;Bahrain Grand Prix: 2014 - 2023&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) + scale_x_continuous(breaks = c(1, 3, 6, 9, 12, 15, 18), labels = c(1, 3, 6, 9, 12, 15, 18)) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), legend.position = &#39;bottom&#39;) 6.4.2 Saudi Arabia The second race of the 2023 season takes place in Saudi Arabia at the Jeddah Corniche Circuit. Jeddah is the fastest street circuit in Formula 1. How does it compare to other street circuits? # Fit model saudiarabia_model &lt;- vglm(Position_race ~ Position_grid, family = cumulative(parallel = T, reverse = F), data = grids_and_races %&gt;% filter(Race == &#39;saudi-arabia&#39;)) # Make Predictions for each Finishing position (1:20), using each Starting Position (1:20) predictions_saudiarabia &lt;- as.data.frame(predictvglm(saudiarabia_model, newdata = starting_grid, se.fit = T)$fitted.values) # rename the columns names(predictions_saudiarabia) &lt;- placing_df$prob[1:17] # Extract the standard error for each prediction standard_errors_saudiarabia &lt;- as.data.frame(predictvglm(saudiarabia_model, newdata = starting_grid, se.fit = T)$se.fit) # rename the standard error names(standard_errors_saudiarabia) &lt;- placing_df$se[1:17] Here, I will plot the probability of a win at the Saudi Arabian Grand Prix. starting_grid %&gt;% bind_cols(pr_1 = predictions_saudiarabia$prob_1, se_1 = standard_errors_saudiarabia$se_1) %&gt;% mutate(pr = exp(pr_1), lower = exp(pr_1 - se_1 * 1.96), upper = exp(pr_1 + se_1 * 1.96)) %&gt;% mutate( pr = pr / (1 + pr), lower = lower / (1 + lower), upper = upper / (1 + upper)) %&gt;% ggplot(aes(Position_grid, y = pr, ymin = lower, ymax = upper)) + geom_pointrange() + theme_bw() + labs( y = &#39;Probability of finishing P1&#39;, x = &#39;Starting Grid Position&#39;, title = &#39;Saudi Arabia&#39;, subtitle = &#39;Probability of a win&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), legend.position = &#39;bottom&#39;) And here are probabilities of a podium. starting_grid %&gt;% bind_cols(pr_3 = predictions_saudiarabia$prob_3, se_3 = standard_errors_saudiarabia$se_3) %&gt;% mutate(pr = exp(pr_3), lower = exp(pr_3 - se_3 * 1.96), upper = exp(pr_3 + se_3 * 1.96)) %&gt;% mutate( pr = pr / (1 + pr), lower = lower / (1 + lower), upper = upper / (1 + upper)) %&gt;% ggplot(aes(Position_grid, y = pr, ymin = lower, ymax = upper)) + geom_pointrange() + theme_bw() + labs( y = &#39;Probability of finishing on the podium&#39;, x = &#39;Starting Grid Position&#39;, title = &#39;Saudi Arabia&#39;, subtitle = &#39;Probability of a podium&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), legend.position = &#39;bottom&#39;) Here’s the full distribution for each starting grid position. saudiarabia_preds_se_full &lt;- predictions_saudiarabia %&gt;% mutate(starting = 1:n()) %&gt;% pivot_longer(- starting, names_to = &#39;placing&#39;, values_to = &#39;prob&#39;) %&gt;% bind_cols(standard_errors_saudiarabia %&gt;% mutate(starting = 1:n()) %&gt;% pivot_longer(- starting, names_to = &#39;placing&#39;, values_to = &#39;se&#39;) %&gt;% dplyr::select(se)) %&gt;% mutate(placing = as.numeric(str_remove(placing, &#39;prob_&#39;))) %&gt;% mutate(prob = exp(prob)) %&gt;% mutate(prob = prob / (1 + prob)) %&gt;% group_by(starting) %&gt;% mutate(discrete_prob = ifelse(is.na(prob - lag(prob)), prob, (prob - lag(prob)))) %&gt;% ungroup() saudiarabia_preds_se_full %&gt;% mutate(starting_pos = as.factor(paste0(&#39;P&#39;,starting)), starting_pos = fct_reorder(starting_pos, starting)) %&gt;% ggplot(aes(placing, discrete_prob)) + geom_segment(aes(x = placing, xend = placing, y = 0, yend = discrete_prob), size = 2, alpha = 0.75, col = &#39;darkgreen&#39;) + geom_point() + theme_bw() + facet_wrap(~ starting, labeller = label_both) + labs(x = &#39;Final Race Classification&#39;, y = &#39;Probability&#39;, title = expression(Discrete~Probability~of~finishing~P[n]), subtitle = &#39;Saudi Arabian Grand Prix: 2021 - 2023&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) + scale_x_continuous(breaks = c(1, 3, 6, 9, 12, 15, 18), labels = c(1, 3, 6, 9, 12, 15, 18)) There are only three races to construct this Saudi Arabian model, so there’s considerable variability in the estimated probabilities. That being said, we can still compare these point estimates to a much slower street circuit like Monaco. 6.4.3 Monaco The most prestigious race on the Formula One calender takes place at the Circuit de Monaco. The Monaco Grand Prix takes place on the very narrow streets of Monte Carlo with plenty of elevation change and slow tight corners. In fact, it is the slowest circuit on the calender and includes the slowest corner of the entire season. These low speeds, tight quarters, and slow corners combine to produce the toughest circuit to overtake on. Over it’s history, this Grand Prix averages about a dozen overtakes per race. I’m interested in knowing if these characteristics will show up when we model this race. Theoretically, the Monaco Grand Prix should have the tightest distributions. Let’s see if the data support this! monaco_model &lt;- vglm(Position_race ~ Position_grid, family = cumulative(parallel = T, reverse = F), data = grids_and_races %&gt;% filter(Race == &#39;monaco&#39;)) After fitting the model, I will make predictions with the following code: # Make Predictions for each Finishing position (1:20), using each Starting Position (1:20) predictions_monaco &lt;- as.data.frame(predictvglm(monaco_model, newdata = starting_grid, se.fit = T)$fitted.values) # rename the columns names(predictions_monaco) &lt;- placing_df$prob[1:18] # Extract the standard error for each prediction standard_errors_monaco &lt;- as.data.frame(predictvglm(monaco_model, newdata = starting_grid, se.fit = T)$se.fit) # rename the standard error names(standard_errors_monaco) &lt;- placing_df$se[1:18] Now, I’ll plot the Probability(P1 finish | a starting grid) at Monaco: starting_grid %&gt;% bind_cols(pr_1 = predictions_monaco$prob_1, se_1 = standard_errors_monaco$se_1) %&gt;% mutate(pr = exp(pr_1), lower = exp(pr_1 - se_1 * 1.96), upper = exp(pr_1 + se_1 * 1.96)) %&gt;% mutate( pr = pr / (1 + pr), lower = lower / (1 + lower), upper = upper / (1 + upper)) %&gt;% ggplot(aes(Position_grid, y = pr, ymin = lower, ymax = upper)) + geom_pointrange() + theme_bw() + labs( y = &#39;Probability of finishing P1&#39;, x = &#39;Starting Grid Position&#39;, title = &#39;Monaco&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) monaco_preds_se_full &lt;- predictions_monaco %&gt;% mutate(starting = 1:n()) %&gt;% pivot_longer(- starting, names_to = &#39;placing&#39;, values_to = &#39;prob&#39;) %&gt;% bind_cols(standard_errors_monaco %&gt;% mutate(starting = 1:n()) %&gt;% pivot_longer(- starting, names_to = &#39;placing&#39;, values_to = &#39;se&#39;) %&gt;% dplyr::select(se)) %&gt;% mutate(placing = as.numeric(str_remove(placing, &#39;prob_&#39;))) %&gt;% mutate(prob = exp(prob)) %&gt;% mutate(prob = prob / (1 + prob)) %&gt;% group_by(starting) %&gt;% mutate(discrete_prob = ifelse(is.na(prob - lag(prob)), prob, (prob - lag(prob)))) %&gt;% ungroup() Here’s a plot of all the distributions for Monaco: monaco_preds_se_full %&gt;% mutate(starting_pos = as.factor(paste0(&#39;P&#39;,starting)), starting_pos = fct_reorder(starting_pos, starting)) %&gt;% ggplot(aes(placing, discrete_prob)) + geom_point() + geom_segment(aes(x = placing, xend = placing, y = 0, yend = discrete_prob), size = 2, alpha = 0.5, col = &#39;red&#39;) + theme_bw() + facet_wrap(~ starting, labeller = label_both) + labs(x = &#39;Final Race Classification&#39;, y = &#39;Probability&#39;, title = expression(Discrete~Probability~of~finishing~P[n]), subtitle = &#39;Monaco Grand Prix: 2014 - 2023&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) Let’s compare the fastest street circuit (Jeddah) to the slowest (Monaco). saudiarabia_preds_se_full %&gt;% dplyr::select(starting, placing, discrete_prob) %&gt;% left_join(monaco_preds_se_full %&gt;% dplyr::select(starting, placing, discrete_prob), by = c(&quot;starting&quot;, &quot;placing&quot;), suffix = c(&quot;_saudiarabia&quot;, &quot;_monaco&quot;)) %&gt;% mutate(starting_pos = as.factor(paste0(&#39;P&#39;,starting)), starting_pos = fct_reorder(starting_pos, starting), delta = ifelse(discrete_prob_monaco &gt; discrete_prob_saudiarabia, &#39;monaco&#39;, &#39;saudiarabia&#39;)) %&gt;% filter(starting_pos %in% c(&#39;P1&#39;, &#39;P2&#39;, &#39;P3&#39;, &#39;P4&#39;)) %&gt;% ggplot() + geom_segment(aes(x = placing, xend = placing, y = discrete_prob_saudiarabia, yend = discrete_prob_monaco, col = delta), size = 2, alpha = 0.5) + geom_point(size = 2, pch = 21, col = &#39;black&#39;, aes(placing, discrete_prob_saudiarabia, fill = &#39;Saudi Arabia&#39;), alpha = 0.75, show.legend = F) + geom_point(size = 2, pch = 21, col = &#39;black&#39;, aes(placing, discrete_prob_monaco, fill = &#39;Monaco&#39;), alpha = 0.75, show.legend = F) + scale_color_manual(&quot;&quot;, values = c(&#39;Saudi Arabia&#39; = &#39;green&#39;, &#39;Monaco&#39; = &#39;red&#39;)) + scale_fill_manual(&quot;&quot;, values = c(&#39;Saudi Arabia&#39; = &#39;green&#39;, &#39;Monaco&#39; = &#39;red&#39;)) + theme_bw() + facet_wrap(~ starting, labeller = label_both) + labs(x = &#39;Final Race Classification&#39;, y = &#39;Probability&#39;, title = expression(Discrete~Probability~of~finishing~P[n]), subtitle = &#39;Saudia Arabia (2021 - 2023) vs Monaco (2014 - 2023)&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) If we focus on the first 2 rows of the starting grid, we do see those cars have higher podium probabilities in Monaco. Is it easier to win in Jeddah from further down the grid? saudiarabia_preds_se_full %&gt;% dplyr::select(starting, placing, discrete_prob) %&gt;% left_join(monaco_preds_se_full %&gt;% dplyr::select(starting, placing, discrete_prob), by = c(&quot;starting&quot;, &quot;placing&quot;), suffix = c(&quot;_saudiarabia&quot;, &quot;_monaco&quot;)) %&gt;% mutate(starting_pos = as.factor(paste0(&#39;P&#39;,starting)), starting_pos = fct_reorder(starting_pos, starting), delta = ifelse(discrete_prob_monaco &gt; discrete_prob_saudiarabia, &#39;monaco&#39;, &#39;saudiarabia&#39;)) %&gt;% filter(placing == 1) %&gt;% ggplot() + geom_segment(aes(x = starting, xend = starting, y = discrete_prob_saudiarabia, yend = discrete_prob_monaco, col = delta), size = 1.5, alpha = 0.5) + geom_point(size = 2, pch = 21, col = &#39;black&#39;, aes(starting, discrete_prob_saudiarabia), fill = &#39;green&#39;, alpha = 0.75) + geom_point(size = 2, pch = 21, col = &#39;black&#39;, aes(starting, discrete_prob_monaco), fill = &#39;red&#39;, alpha = 0.75) + theme_bw() + scale_color_manual(&quot;&quot;, values = c(&#39;Saudi Arabia&#39; = &#39;green&#39;, &#39;Monaco&#39; = &#39;red&#39;)) + labs(x = &#39;Starting Grid Position&#39;, y = &#39;Probability of a win&#39;, title = expression(Discrete~Probability~of~finishing~P[1]), subtitle = &#39;Saudi Arabia (2021 - 2023) vs Monaco (2014 - 2023)&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) + scale_x_continuous(labels = seq(1, 20, 1), breaks = seq(1, 20, 1)) It looks like that is likely the case. Cars at the front of the grid are rewarded more in Monaco, while cars further back in Saudi Arabia have a slightly better chance of a win than they do in Monaco. It looks like there is just a much stronger advantage to starting at the front of the grid in Monaco. 6.4.4 Brazil The Sao Paulo Grand Prix is perhaps the easiest circuit to overtake on. Let’s compare the model predictions and probabilities for Brazil to those of Monaco. brazil_model &lt;- vglm(Position_race ~ Position_grid, family = cumulative(parallel = T, reverse = F), data = grids_and_races %&gt;% filter(Race == &#39;brazil&#39;)) As before, I’ll make predictions for the grid: # Make Predictions for each Finishing position (1:20), using each Starting Position (1:20) predictions_brazil&lt;- as.data.frame(predictvglm(brazil_model, newdata = starting_grid, se.fit = T)$fitted.values) # rename the columns names(predictions_brazil) &lt;- placing_df$prob[1:18] # Extract the standard error for each prediction standard_errors_brazil &lt;- as.data.frame(predictvglm(brazil_model, newdata = starting_grid, se.fit = T)$se.fit) # rename the standard error names(standard_errors_brazil) &lt;- placing_df$se[1:18] Now, I’ll compute the probability(P1 finish | a starting grid) in Brazil: starting_grid %&gt;% bind_cols(pr_1 = predictions_brazil$prob_1, se_1 = standard_errors_brazil$se_1) %&gt;% mutate(pr = exp(pr_1), lower = exp(pr_1 - se_1 * 1.96), upper = exp(pr_1 + se_1 * 1.96)) %&gt;% mutate( pr = pr / (1 + pr), lower = lower / (1 + lower), upper = upper / (1 + upper)) %&gt;% ggplot(aes(Position_grid, y = pr, ymin = lower, ymax = upper)) + geom_pointrange() + theme_bw() + labs( y = &#39;Probability of finishing P1&#39;, x = &#39;Starting Grid Position&#39;, title = &#39;Brazil&#39;, subtitle = &#39;2014 - 2023&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) And, I’ll plot the discrete probability distributions here: brazil_preds_se_full &lt;- predictions_brazil %&gt;% mutate(starting = 1:n()) %&gt;% pivot_longer(- starting, names_to = &#39;placing&#39;, values_to = &#39;prob&#39;) %&gt;% bind_cols(standard_errors_brazil %&gt;% mutate(starting = 1:n()) %&gt;% pivot_longer(- starting, names_to = &#39;placing&#39;, values_to = &#39;se&#39;) %&gt;% dplyr::select(se)) %&gt;% mutate(placing = as.numeric(str_remove(placing, &#39;prob_&#39;))) %&gt;% mutate(prob = exp(prob)) %&gt;% mutate(prob = prob / (1 + prob)) %&gt;% group_by(starting) %&gt;% mutate(discrete_prob = ifelse(is.na(prob - lag(prob)), prob, (prob - lag(prob)))) %&gt;% ungroup() brazil_preds_se_full %&gt;% mutate(starting_pos = as.factor(paste0(&#39;P&#39;,starting)), starting_pos = fct_reorder(starting_pos, starting)) %&gt;% ggplot(aes(placing, discrete_prob)) + geom_segment(aes(x = placing, xend = placing, y = 0, yend = discrete_prob), size = 2, alpha = 0.5, col = &#39;green&#39;) + geom_point() + theme_bw() + facet_wrap(~ starting, labeller = label_both) + labs(x = &#39;Final Race Classification&#39;, y = &#39;Probability&#39;, title = expression(Discrete~Probability~of~finishing~P[n]), subtitle = &#39;Sao Paulo Grand Prix: 2014 - 2023&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) How does Brazil compare to Monaco? Notice in the plot below that the red dots (Monaco) demonstrate higher probabilities for podium placings if a driver starts on the first 2 rows of the grid, compared to the green dots (Brazil). brazil_preds_se_full %&gt;% dplyr::select(starting, placing, discrete_prob) %&gt;% left_join(monaco_preds_se_full %&gt;% dplyr::select(starting, placing, discrete_prob), by = c(&quot;starting&quot;, &quot;placing&quot;), suffix = c(&quot;_brazil&quot;, &quot;_monaco&quot;)) %&gt;% mutate(starting_pos = as.factor(paste0(&#39;P&#39;,starting)), starting_pos = fct_reorder(starting_pos, starting), delta = ifelse(discrete_prob_monaco &gt; discrete_prob_brazil, &#39;monaco&#39;, &#39;brazil&#39;)) %&gt;% filter(starting_pos %in% c(&#39;P1&#39;, &#39;P2&#39;, &#39;P3&#39;, &#39;P4&#39;)) %&gt;% ggplot() + geom_segment(aes(x = placing, xend = placing, y = discrete_prob_brazil, yend = discrete_prob_monaco, col = delta), size = 2, alpha = 0.5) + geom_point(size = 2, pch = 21, col = &#39;black&#39;, aes(placing, discrete_prob_brazil, fill = &#39;Brazil&#39;), alpha = 0.75, show.legend = F) + geom_point(size = 2, pch = 21, col = &#39;black&#39;, aes(placing, discrete_prob_monaco, fill = &#39;Monaco&#39;), alpha = 0.75, show.legend = F) + scale_color_manual(&quot;&quot;, values = c(&#39;Brazil&#39; = &#39;green&#39;, &#39;Monaco&#39; = &#39;red&#39;)) + scale_fill_manual(&quot;&quot;, values = c(&#39;Brazil&#39; = &#39;green&#39;, &#39;Monaco&#39; = &#39;red&#39;)) + theme_bw() + facet_wrap(~ starting, labeller = label_both) + labs(x = &#39;Final Race Classification&#39;, y = &#39;Probability&#39;, title = expression(Discrete~Probability~of~finishing~P[n]), subtitle = title_color_coder(&quot;&quot;, &quot;Sao Paulo&quot;, &#39;green&#39;, &quot; vs &quot;, &quot;Monaco&quot;, &#39;red&#39; ,&quot;: 2014 - 2023&quot;)) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = ggtext::element_markdown(hjust = 0.5, size = 12)) We can also compare the probability of finishing P1 given any starting grid position between these two Grands Prix. brazil_preds_se_full %&gt;% dplyr::select(starting, placing, discrete_prob) %&gt;% left_join(monaco_preds_se_full %&gt;% dplyr::select(starting, placing, discrete_prob), by = c(&quot;starting&quot;, &quot;placing&quot;), suffix = c(&quot;_brazil&quot;, &quot;_monaco&quot;)) %&gt;% mutate(starting_pos = as.factor(paste0(&#39;P&#39;,starting)), starting_pos = fct_reorder(starting_pos, starting), delta = ifelse(discrete_prob_monaco &gt; discrete_prob_brazil, &#39;monaco&#39;, &#39;brazil&#39;)) %&gt;% filter(placing == 1) %&gt;% ggplot() + geom_segment(aes(x = starting, xend = starting, y = discrete_prob_brazil, yend = discrete_prob_monaco, col = delta), size = 1.5, alpha = 0.5) + geom_point(size = 2, pch = 21, col = &#39;black&#39;, aes(starting, discrete_prob_brazil), fill = &#39;green&#39;, alpha = 0.75) + geom_point(size = 2, pch = 21, col = &#39;black&#39;, aes(starting, discrete_prob_monaco), fill = &#39;red&#39;, alpha = 0.75) + theme_bw() + scale_color_manual(&quot;&quot;, values = c(&#39;Brazil&#39; = &#39;green&#39;, &#39;Monaco&#39; = &#39;red&#39;)) + labs(x = &#39;Starting Position&#39;, y = &#39;Probability of a race win&#39;, title = expression(Discrete~Probability~of~finishing~P[1]), subtitle = title_color_coder(&quot;&quot;, &quot;Sao Paulo&quot;, &#39;green&#39;, &quot; vs &quot;, &quot;Monaco&quot;, &#39;red&#39; ,&quot;: 2014 - 2023&quot;)) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = ggtext::element_markdown(hjust = 0.5, size = 12)) Relative to Brazil, there seems to be a distinct advantage in starting the Monaco Grand Prix from the front row. For example, starting from P1 at Monaco carries an approximately 33% chance of winning, while pole position in Brazil results in just a 26% win probability. Compared to Monaco, a driver has a higher likelihood of winning from any other row on the gird at the Sao Paulo Grand Prix. Stated plainly… Qualifying on the front row in Monaco is very important! In Sao Paulo, a driver can make up positions. 6.5 Appendix: Probability distributions for every Grand Prix In case you are interested in any particular circuit, I will build a model and plot results for each Grand Prix on the schedule using data from 2014 through 2023 (or whatever is available). 6.5.1 Bahrain # These predictions were generated earlier in the chapter bahrain_preds_se_full %&gt;% mutate(starting_pos = as.factor(paste0(&#39;P&#39;,starting)), starting_pos = fct_reorder(starting_pos, starting)) %&gt;% ggplot(aes(placing, discrete_prob)) + geom_point() + geom_segment(aes(x = placing, xend = placing, y = 0, yend = discrete_prob), size = 2, alpha = 0.75, col = &#39;maroon&#39;) + theme_bw() + facet_wrap(~ starting, labeller = label_both) + labs(x = &#39;Final Race Classification&#39;, y = &#39;Probability&#39;, title = expression(Discrete~Probability~of~finishing~P[n]), subtitle = &#39;Bahrain Grand Prix: 2014 - 2023&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) + scale_x_continuous(breaks = c(1, 3, 6, 9, 12, 15, 18), labels = c(1, 3, 6, 9, 12, 15, 18)) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), legend.position = &#39;bottom&#39;) 6.5.2 Saudi Arabia # These predictions were generated earlier in the chapter saudiarabia_preds_se_full %&gt;% mutate(starting_pos = as.factor(paste0(&#39;P&#39;,starting)), starting_pos = fct_reorder(starting_pos, starting)) %&gt;% ggplot(aes(placing, discrete_prob)) + geom_segment(aes(x = placing, xend = placing, y = 0, yend = discrete_prob), size = 2, alpha = 0.75, col = &#39;darkgreen&#39;) + geom_point() + theme_bw() + facet_wrap(~ starting, labeller = label_both) + labs(x = &#39;Final Race Classification&#39;, y = &#39;Probability&#39;, title = expression(Discrete~Probability~of~finishing~P[n]), subtitle = &#39;Saudi Arabian Grand Prix: 2021 - 2023&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) + scale_x_continuous(breaks = c(1, 3, 6, 9, 12, 15, 18), labels = c(1, 3, 6, 9, 12, 15, 18)) 6.5.3 Australia australia_model &lt;- vglm(Position_race ~ Position_grid, family = cumulative(parallel = T, reverse = F), data = grids_and_races %&gt;% filter(Race == &#39;australia&#39;)) # Make Predictions for each Finishing position (1:20), using each Starting Position (1:20) australia_predictions &lt;- as.data.frame(predictvglm(australia_model, newdata = starting_grid, se.fit = T)$fitted.values) # rename the columns names(australia_predictions) &lt;- placing_df$prob[1:16] # Extract the standard error for each prediction australia_standard_errors &lt;- as.data.frame(predictvglm(australia_model, newdata = starting_grid, se.fit = T)$se.fit) # rename the standard error names(australia_standard_errors) &lt;- placing_df$se[1:16] # tidy up probabilities australia_preds_se_full &lt;- australia_predictions %&gt;% mutate(starting = 1:n()) %&gt;% pivot_longer(- starting, names_to = &#39;placing&#39;, values_to = &#39;prob&#39;) %&gt;% bind_cols(australia_standard_errors %&gt;% mutate(starting = 1:n()) %&gt;% pivot_longer(- starting, names_to = &#39;placing&#39;, values_to = &#39;se&#39;) %&gt;% dplyr::select(se)) %&gt;% mutate(placing = as.numeric(str_remove(placing, &#39;prob_&#39;))) %&gt;% mutate(prob = exp(prob)) %&gt;% mutate(prob = prob / (1 + prob)) %&gt;% group_by(starting) %&gt;% mutate(discrete_prob = ifelse(is.na(prob - lag(prob)), prob, (prob - lag(prob)))) %&gt;% ungroup() # Plot australia_preds_se_full %&gt;% mutate(starting_pos = as.factor(paste0(&#39;P&#39;,starting)), starting_pos = fct_reorder(starting_pos, starting)) %&gt;% ggplot(aes(placing, discrete_prob)) + geom_point() + geom_segment(aes(x = placing, xend = placing, y = 0, yend = discrete_prob), size = 2, alpha = 0.5, col = &#39;navy&#39;) + theme_bw() + facet_wrap(~ starting, labeller = label_both) + labs(x = &#39;Final Race Classification&#39;, y = &#39;Probability&#39;, title = expression(Discrete~Probability~of~finishing~P[n]), subtitle = &#39;Australian Grand Prix: 2014 - 2023&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) 6.5.4 Azerbaijan azerbaijan_model &lt;- vglm(Position_race ~ Position_grid, family = cumulative(parallel = T, reverse = F), data = grids_and_races %&gt;% filter(Race == &#39;azerbaijan&#39;)) # Make Predictions for each Finishing position (1:20), using each Starting Position (1:20) azerbaijan_predictions &lt;- as.data.frame(predictvglm(azerbaijan_model, newdata = starting_grid, se.fit = T)$fitted.values) # rename the columns names(azerbaijan_predictions) &lt;- placing_df$prob[1:17] # Extract the standard error for each prediction azerbaijan_standard_errors &lt;- as.data.frame(predictvglm(azerbaijan_model, newdata = starting_grid, se.fit = T)$se.fit) # rename the standard error names(azerbaijan_standard_errors) &lt;- placing_df$se[1:17] # tidy up probabilities azerbaijan_preds_se_full &lt;- azerbaijan_predictions %&gt;% mutate(starting = 1:n()) %&gt;% pivot_longer(- starting, names_to = &#39;placing&#39;, values_to = &#39;prob&#39;) %&gt;% bind_cols(azerbaijan_standard_errors %&gt;% mutate(starting = 1:n()) %&gt;% pivot_longer(- starting, names_to = &#39;placing&#39;, values_to = &#39;se&#39;) %&gt;% dplyr::select(se)) %&gt;% mutate(placing = as.numeric(str_remove(placing, &#39;prob_&#39;))) %&gt;% mutate(prob = exp(prob)) %&gt;% mutate(prob = prob / (1 + prob)) %&gt;% group_by(starting) %&gt;% mutate(discrete_prob = ifelse(is.na(prob - lag(prob)), prob, (prob - lag(prob)))) %&gt;% ungroup() # Plot azerbaijan_preds_se_full %&gt;% mutate(starting_pos = as.factor(paste0(&#39;P&#39;,starting)), starting_pos = fct_reorder(starting_pos, starting)) %&gt;% ggplot(aes(placing, discrete_prob)) + geom_point() + geom_segment(aes(x = placing, xend = placing, y = 0, yend = discrete_prob), size = 2, alpha = 0.5, col = &#39;darkgreen&#39;) + theme_bw() + facet_wrap(~ starting, labeller = label_both) + labs(x = &#39;Final Race Classification&#39;, y = &#39;Probability&#39;, title = expression(Discrete~Probability~of~finishing~P[n]), subtitle = &#39;Azerbaijan Grand Prix: 2017 - 2023&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) 6.5.5 Miami miami_model &lt;- vglm(Position_race ~ Position_grid, family = cumulative(parallel = T, reverse = F), data = grids_and_races %&gt;% filter(Race == &#39;miami&#39;)) # Make Predictions for each Finishing position (1:20), using each Starting Position (1:20) miami_predictions &lt;- as.data.frame(predictvglm(miami_model, newdata = starting_grid, se.fit = T)$fitted.values) # rename the columns names(miami_predictions) &lt;- placing_df$prob[1:19] # Extract the standard error for each prediction miami_standard_errors &lt;- as.data.frame(predictvglm(miami_model, newdata = starting_grid, se.fit = T)$se.fit) # rename the standard error names(miami_standard_errors) &lt;- placing_df$se[1:19] # tidy up probabilities miami_preds_se_full &lt;- miami_predictions %&gt;% mutate(starting = 1:n()) %&gt;% pivot_longer(- starting, names_to = &#39;placing&#39;, values_to = &#39;prob&#39;) %&gt;% bind_cols(miami_standard_errors %&gt;% mutate(starting = 1:n()) %&gt;% pivot_longer(- starting, names_to = &#39;placing&#39;, values_to = &#39;se&#39;) %&gt;% dplyr::select(se)) %&gt;% mutate(placing = as.numeric(str_remove(placing, &#39;prob_&#39;))) %&gt;% mutate(prob = exp(prob)) %&gt;% mutate(prob = prob / (1 + prob)) %&gt;% group_by(starting) %&gt;% mutate(discrete_prob = ifelse(is.na(prob - lag(prob)), prob, (prob - lag(prob)))) %&gt;% ungroup() # Plot miami_preds_se_full %&gt;% mutate(starting_pos = as.factor(paste0(&#39;P&#39;,starting)), starting_pos = fct_reorder(starting_pos, starting)) %&gt;% ggplot(aes(placing, discrete_prob)) + geom_point() + geom_segment(aes(x = placing, xend = placing, y = 0, yend = discrete_prob), size = 2, alpha = 0.5, col = &#39;darkorange&#39;) + theme_bw() + facet_wrap(~ starting, labeller = label_both) + labs(x = &#39;Final Race Classification&#39;, y = &#39;Probability&#39;, title = expression(Discrete~Probability~of~finishing~P[n]), subtitle = &#39;Miami Grand Prix: 2022 - 2023&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) 6.5.6 Emilia Romagna emilia_model &lt;- vglm(Position_race ~ Position_grid, family = cumulative(parallel = T, reverse = F), data = grids_and_races %&gt;% filter(Race == &#39;emilia&#39;)) # Make Predictions for each Finishing position (1:20), using each Starting Position (1:20) emilia_predictions &lt;- as.data.frame(predictvglm(emilia_model, newdata = starting_grid, se.fit = T)$fitted.values) # rename the columns names(emilia_predictions) &lt;- placing_df$prob[1:16] # Extract the standard error for each prediction emilia_standard_errors &lt;- as.data.frame(predictvglm(emilia_model, newdata = starting_grid, se.fit = T)$se.fit) # rename the standard error names(emilia_standard_errors) &lt;- placing_df$se[1:16] # tidy up probabilities emilia_preds_se_full &lt;- emilia_predictions %&gt;% mutate(starting = 1:n()) %&gt;% pivot_longer(- starting, names_to = &#39;placing&#39;, values_to = &#39;prob&#39;) %&gt;% bind_cols(emilia_standard_errors %&gt;% mutate(starting = 1:n()) %&gt;% pivot_longer(- starting, names_to = &#39;placing&#39;, values_to = &#39;se&#39;) %&gt;% dplyr::select(se)) %&gt;% mutate(placing = as.numeric(str_remove(placing, &#39;prob_&#39;))) %&gt;% mutate(prob = exp(prob)) %&gt;% mutate(prob = prob / (1 + prob)) %&gt;% group_by(starting) %&gt;% mutate(discrete_prob = ifelse(is.na(prob - lag(prob)), prob, (prob - lag(prob)))) %&gt;% ungroup() # Plot emilia_preds_se_full %&gt;% mutate(starting_pos = as.factor(paste0(&#39;P&#39;,starting)), starting_pos = fct_reorder(starting_pos, starting)) %&gt;% ggplot(aes(placing, discrete_prob)) + geom_point() + geom_segment(aes(x = placing, xend = placing, y = 0, yend = discrete_prob), size = 2, alpha = 0.5, col = &#39;red&#39;) + theme_bw() + facet_wrap(~ starting, labeller = label_both) + labs(x = &#39;Final Race Classification&#39;, y = &#39;Probability&#39;, title = expression(Discrete~Probability~of~finishing~P[n]), subtitle = &#39;Emilia Romagna Grand Prix: 2014 - 2023&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) 6.5.7 Monaco # These predictions were generated earlier in the chapter monaco_preds_se_full %&gt;% mutate(starting_pos = as.factor(paste0(&#39;P&#39;,starting)), starting_pos = fct_reorder(starting_pos, starting)) %&gt;% ggplot(aes(placing, discrete_prob)) + geom_segment(aes(x = placing, xend = placing, y = 0, yend = discrete_prob), size = 2, alpha = 0.5, col = &#39;red&#39;) + geom_point() + theme_bw() + facet_wrap(~ starting, labeller = label_both) + labs(x = &#39;Final Race Classification&#39;, y = &#39;Probability&#39;, title = expression(Discrete~Probability~of~finishing~P[n]), subtitle = &#39;Monaco Grand Prix: 2014 - 2023&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) 6.5.8 Spain spain_model &lt;- vglm(Position_race ~ Position_grid, family = cumulative(parallel = T, reverse = F), data = grids_and_races %&gt;% filter(Race == &#39;spain&#39;)) # Make Predictions for each Finishing position (1:20), using each Starting Position (1:20) spain_predictions &lt;- as.data.frame(predictvglm(spain_model, newdata = starting_grid, se.fit = T)$fitted.values) # rename the columns names(spain_predictions) &lt;- placing_df$prob[1:19] # Extract the standard error for each prediction spain_standard_errors &lt;- as.data.frame(predictvglm(spain_model, newdata = starting_grid, se.fit = T)$se.fit) # rename the standard error names(spain_standard_errors) &lt;- placing_df$se[1:19] # tidy up probabilities spain_preds_se_full &lt;- spain_predictions %&gt;% mutate(starting = 1:n()) %&gt;% pivot_longer(- starting, names_to = &#39;placing&#39;, values_to = &#39;prob&#39;) %&gt;% bind_cols(spain_standard_errors %&gt;% mutate(starting = 1:n()) %&gt;% pivot_longer(- starting, names_to = &#39;placing&#39;, values_to = &#39;se&#39;) %&gt;% dplyr::select(se)) %&gt;% mutate(placing = as.numeric(str_remove(placing, &#39;prob_&#39;))) %&gt;% mutate(prob = exp(prob)) %&gt;% mutate(prob = prob / (1 + prob)) %&gt;% group_by(starting) %&gt;% mutate(discrete_prob = ifelse(is.na(prob - lag(prob)), prob, (prob - lag(prob)))) %&gt;% ungroup() # Plot spain_preds_se_full %&gt;% mutate(starting_pos = as.factor(paste0(&#39;P&#39;,starting)), starting_pos = fct_reorder(starting_pos, starting)) %&gt;% ggplot(aes(placing, discrete_prob)) + geom_point() + geom_segment(aes(x = placing, xend = placing, y = 0, yend = discrete_prob), size = 2, alpha = 0.5, col = &#39;darkred&#39;) + theme_bw() + facet_wrap(~ starting, labeller = label_both) + labs(x = &#39;Final Race Classification&#39;, y = &#39;Probability&#39;, title = expression(Discrete~Probability~of~finishing~P[n]), subtitle = &#39;Spanish Grand Prix: 2014 - 2023&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) 6.5.9 Canada canada_model &lt;- vglm(Position_race ~ Position_grid, family = cumulative(parallel = T, reverse = F), data = grids_and_races %&gt;% filter(Race == &#39;canada&#39;)) # Make Predictions for each Finishing position (1:20), using each Starting Position (1:20) canada_predictions &lt;- as.data.frame(predictvglm(canada_model, newdata = starting_grid, se.fit = T)$fitted.values) # rename the columns names(canada_predictions) &lt;- placing_df$prob[1:18] # Extract the standard error for each prediction canada_standard_errors &lt;- as.data.frame(predictvglm(canada_model, newdata = starting_grid, se.fit = T)$se.fit) # rename the standard error names(canada_standard_errors) &lt;- placing_df$se[1:18] # tidy up probabilities canada_preds_se_full &lt;- canada_predictions %&gt;% mutate(starting = 1:n()) %&gt;% pivot_longer(- starting, names_to = &#39;placing&#39;, values_to = &#39;prob&#39;) %&gt;% bind_cols(canada_standard_errors %&gt;% mutate(starting = 1:n()) %&gt;% pivot_longer(- starting, names_to = &#39;placing&#39;, values_to = &#39;se&#39;) %&gt;% dplyr::select(se)) %&gt;% mutate(placing = as.numeric(str_remove(placing, &#39;prob_&#39;))) %&gt;% mutate(prob = exp(prob)) %&gt;% mutate(prob = prob / (1 + prob)) %&gt;% group_by(starting) %&gt;% mutate(discrete_prob = ifelse(is.na(prob - lag(prob)), prob, (prob - lag(prob)))) %&gt;% ungroup() # Plot canada_preds_se_full %&gt;% mutate(starting_pos = as.factor(paste0(&#39;P&#39;,starting)), starting_pos = fct_reorder(starting_pos, starting)) %&gt;% ggplot(aes(placing, discrete_prob)) + geom_point() + geom_segment(aes(x = placing, xend = placing, y = 0, yend = discrete_prob), size = 2, alpha = 0.5, col = &#39;red&#39;) + theme_bw() + facet_wrap(~ starting, labeller = label_both) + labs(x = &#39;Final Race Classification&#39;, y = &#39;Probability&#39;, title = expression(Discrete~Probability~of~finishing~P[n]), subtitle = &#39;Canadian Grand Prix: 2014 - 2023&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) 6.5.10 Austria austria_model &lt;- vglm(Position_race ~ Position_grid, family = cumulative(parallel = T, reverse = F), data = grids_and_races %&gt;% filter(Race == &#39;austria&#39;)) # Make Predictions for each Finishing position (1:20), using each Starting Position (1:20) austria_predictions &lt;- as.data.frame(predictvglm(austria_model, newdata = starting_grid, se.fit = T)$fitted.values) # rename the columns names(austria_predictions) &lt;- placing_df$prob[1:19] # Extract the standard error for each prediction austria_standard_errors &lt;- as.data.frame(predictvglm(austria_model, newdata = starting_grid, se.fit = T)$se.fit) # rename the standard error names(austria_standard_errors) &lt;- placing_df$se[1:19] # tidy up probabilities austria_preds_se_full &lt;- austria_predictions %&gt;% mutate(starting = 1:n()) %&gt;% pivot_longer(- starting, names_to = &#39;placing&#39;, values_to = &#39;prob&#39;) %&gt;% bind_cols(austria_standard_errors %&gt;% mutate(starting = 1:n()) %&gt;% pivot_longer(- starting, names_to = &#39;placing&#39;, values_to = &#39;se&#39;) %&gt;% dplyr::select(se)) %&gt;% mutate(placing = as.numeric(str_remove(placing, &#39;prob_&#39;))) %&gt;% mutate(prob = exp(prob)) %&gt;% mutate(prob = prob / (1 + prob)) %&gt;% group_by(starting) %&gt;% mutate(discrete_prob = ifelse(is.na(prob - lag(prob)), prob, (prob - lag(prob)))) %&gt;% ungroup() # Plot austria_preds_se_full %&gt;% mutate(starting_pos = as.factor(paste0(&#39;P&#39;,starting)), starting_pos = fct_reorder(starting_pos, starting)) %&gt;% ggplot(aes(placing, discrete_prob)) + geom_point() + geom_segment(aes(x = placing, xend = placing, y = 0, yend = discrete_prob), size = 2, alpha = 0.5, col = &#39;darkred&#39;) + theme_bw() + facet_wrap(~ starting, labeller = label_both) + labs(x = &#39;Final Race Classification&#39;, y = &#39;Probability&#39;, title = expression(Discrete~Probability~of~finishing~P[n]), subtitle = &#39;Austrian Grand Prix: 2014 - 2023&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) 6.5.11 Great Britain british_model &lt;- vglm(Position_race ~ Position_grid, family = cumulative(parallel = T, reverse = F), data = grids_and_races %&gt;% filter(Race == &#39;great-britain&#39;)) # Make Predictions for each Finishing position (1:20), using each Starting Position (1:20) british_predictions &lt;- as.data.frame(predictvglm(british_model, newdata = starting_grid, se.fit = T)$fitted.values) # rename the columns names(british_predictions) &lt;- placing_df$prob[1:17] # Extract the standard error for each prediction british_standard_errors &lt;- as.data.frame(predictvglm(british_model, newdata = starting_grid, se.fit = T)$se.fit) # rename the standard error names(british_standard_errors) &lt;- placing_df$se[1:17] # tidy up probabilities british_preds_se_full &lt;- british_predictions %&gt;% mutate(starting = 1:n()) %&gt;% pivot_longer(- starting, names_to = &#39;placing&#39;, values_to = &#39;prob&#39;) %&gt;% bind_cols(british_standard_errors %&gt;% mutate(starting = 1:n()) %&gt;% pivot_longer(- starting, names_to = &#39;placing&#39;, values_to = &#39;se&#39;) %&gt;% dplyr::select(se)) %&gt;% mutate(placing = as.numeric(str_remove(placing, &#39;prob_&#39;))) %&gt;% mutate(prob = exp(prob)) %&gt;% mutate(prob = prob / (1 + prob)) %&gt;% group_by(starting) %&gt;% mutate(discrete_prob = ifelse(is.na(prob - lag(prob)), prob, (prob - lag(prob)))) %&gt;% ungroup() # Plot british_preds_se_full %&gt;% mutate(starting_pos = as.factor(paste0(&#39;P&#39;,starting)), starting_pos = fct_reorder(starting_pos, starting)) %&gt;% ggplot(aes(placing, discrete_prob)) + geom_point() + geom_segment(aes(x = placing, xend = placing, y = 0, yend = discrete_prob), size = 2, alpha = 0.5, col = &#39;navy&#39;) + theme_bw() + facet_wrap(~ starting, labeller = label_both) + labs(x = &#39;Final Race Classification&#39;, y = &#39;Probability&#39;, title = expression(Discrete~Probability~of~finishing~P[n]), subtitle = &#39;British Grand Prix: 2014 - 2023&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) 6.5.12 Hungary hungary_model &lt;- vglm(Position_race ~ Position_grid, family = cumulative(parallel = T, reverse = F), data = grids_and_races %&gt;% filter(Race == &#39;hungary&#39;)) # Make Predictions for each Finishing position (1:20), using each Starting Position (1:20) hungary_predictions &lt;- as.data.frame(predictvglm(hungary_model, newdata = starting_grid, se.fit = T)$fitted.values) # rename the columns names(hungary_predictions) &lt;- placing_df$prob[1:20] # Extract the standard error for each prediction hungary_standard_errors &lt;- as.data.frame(predictvglm(hungary_model, newdata = starting_grid, se.fit = T)$se.fit) # rename the standard error names(hungary_standard_errors) &lt;- placing_df$se[1:20] # tidy up probabilities hungary_preds_se_full &lt;- hungary_predictions %&gt;% mutate(starting = 1:n()) %&gt;% pivot_longer(- starting, names_to = &#39;placing&#39;, values_to = &#39;prob&#39;) %&gt;% bind_cols(hungary_standard_errors %&gt;% mutate(starting = 1:n()) %&gt;% pivot_longer(- starting, names_to = &#39;placing&#39;, values_to = &#39;se&#39;) %&gt;% dplyr::select(se)) %&gt;% mutate(placing = as.numeric(str_remove(placing, &#39;prob_&#39;))) %&gt;% mutate(prob = exp(prob)) %&gt;% mutate(prob = prob / (1 + prob)) %&gt;% group_by(starting) %&gt;% mutate(discrete_prob = ifelse(is.na(prob - lag(prob)), prob, (prob - lag(prob)))) %&gt;% ungroup() # Plot hungary_preds_se_full %&gt;% mutate(starting_pos = as.factor(paste0(&#39;P&#39;,starting)), starting_pos = fct_reorder(starting_pos, starting)) %&gt;% ggplot(aes(placing, discrete_prob)) + geom_point() + geom_segment(aes(x = placing, xend = placing, y = 0, yend = discrete_prob), size = 2, alpha = 0.5, col = &#39;forestgreen&#39;) + theme_bw() + facet_wrap(~ starting, labeller = label_both) + labs(x = &#39;Final Race Classification&#39;, y = &#39;Probability&#39;, title = expression(Discrete~Probability~of~finishing~P[n]), subtitle = &#39;Hungarian Grand Prix: 2014 - 2023&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) 6.5.13 Belgium belgium_model &lt;- vglm(Position_race ~ Position_grid, family = cumulative(parallel = T, reverse = F), data = grids_and_races %&gt;% filter(Race == &#39;belgium&#39;)) # Make Predictions for each Finishing position (1:20), using each Starting Position (1:20) belgium_predictions &lt;- as.data.frame(predictvglm(belgium_model, newdata = starting_grid, se.fit = T)$fitted.values) # rename the columns names(belgium_predictions) &lt;- placing_df$prob[1:19] # Extract the standard error for each prediction belgium_standard_errors &lt;- as.data.frame(predictvglm(belgium_model, newdata = starting_grid, se.fit = T)$se.fit) # rename the standard error names(belgium_standard_errors) &lt;- placing_df$se[1:19] # tidy up probabilities belgium_preds_se_full &lt;- belgium_predictions %&gt;% mutate(starting = 1:n()) %&gt;% pivot_longer(- starting, names_to = &#39;placing&#39;, values_to = &#39;prob&#39;) %&gt;% bind_cols(belgium_standard_errors %&gt;% mutate(starting = 1:n()) %&gt;% pivot_longer(- starting, names_to = &#39;placing&#39;, values_to = &#39;se&#39;) %&gt;% dplyr::select(se)) %&gt;% mutate(placing = as.numeric(str_remove(placing, &#39;prob_&#39;))) %&gt;% mutate(prob = exp(prob)) %&gt;% mutate(prob = prob / (1 + prob)) %&gt;% group_by(starting) %&gt;% mutate(discrete_prob = ifelse(is.na(prob - lag(prob)), prob, (prob - lag(prob)))) %&gt;% ungroup() # Plot belgium_preds_se_full %&gt;% mutate(starting_pos = as.factor(paste0(&#39;P&#39;,starting)), starting_pos = fct_reorder(starting_pos, starting)) %&gt;% ggplot(aes(placing, discrete_prob)) + geom_point() + geom_segment(aes(x = placing, xend = placing, y = 0, yend = discrete_prob), size = 2, alpha = 0.5, col = &#39;black&#39;) + theme_bw() + facet_wrap(~ starting, labeller = label_both) + labs(x = &#39;Final Race Classification&#39;, y = &#39;Probability&#39;, title = expression(Discrete~Probability~of~finishing~P[n]), subtitle = &#39;Belgian Grand Prix: 2014 - 2023&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) 6.5.14 Netherlands netherlands_model &lt;- vglm(Position_race ~ Position_grid, family = cumulative(parallel = T, reverse = F), data = grids_and_races %&gt;% filter(Race == &#39;netherlands&#39;)) # Make Predictions for each Finishing position (1:20), using each Starting Position (1:20) netherlands_predictions &lt;- as.data.frame(predictvglm(netherlands_model, newdata = starting_grid, se.fit = T)$fitted.values) # rename the columns names(netherlands_predictions) &lt;- placing_df$prob[1:17] # Extract the standard error for each prediction netherlands_standard_errors &lt;- as.data.frame(predictvglm(netherlands_model, newdata = starting_grid, se.fit = T)$se.fit) # rename the standard error names(netherlands_standard_errors) &lt;- placing_df$se[1:17] # tidy up probabilities netherlands_preds_se_full &lt;- netherlands_predictions %&gt;% mutate(starting = 1:n()) %&gt;% pivot_longer(- starting, names_to = &#39;placing&#39;, values_to = &#39;prob&#39;) %&gt;% bind_cols(netherlands_standard_errors %&gt;% mutate(starting = 1:n()) %&gt;% pivot_longer(- starting, names_to = &#39;placing&#39;, values_to = &#39;se&#39;) %&gt;% dplyr::select(se)) %&gt;% mutate(placing = as.numeric(str_remove(placing, &#39;prob_&#39;))) %&gt;% mutate(prob = exp(prob)) %&gt;% mutate(prob = prob / (1 + prob)) %&gt;% group_by(starting) %&gt;% mutate(discrete_prob = ifelse(is.na(prob - lag(prob)), prob, (prob - lag(prob)))) %&gt;% ungroup() # Plot netherlands_preds_se_full %&gt;% mutate(starting_pos = as.factor(paste0(&#39;P&#39;,starting)), starting_pos = fct_reorder(starting_pos, starting)) %&gt;% ggplot(aes(placing, discrete_prob)) + geom_point() + geom_segment(aes(x = placing, xend = placing, y = 0, yend = discrete_prob), size = 2, alpha = 0.5, col = &#39;orange4&#39;) + theme_bw() + facet_wrap(~ starting, labeller = label_both) + labs(x = &#39;Final Race Classification&#39;, y = &#39;Probability&#39;, title = expression(Discrete~Probability~of~finishing~P[n]), subtitle = &#39;Dutch Grand Prix: 2021 - 2023&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) 6.5.15 Monza italy_model &lt;- vglm(Position_race ~ Position_grid, family = cumulative(parallel = T, reverse = F), data = grids_and_races %&gt;% filter(Race == &#39;italy&#39;)) # Make Predictions for each Finishing position (1:20), using each Starting Position (1:20) italy_predictions &lt;- as.data.frame(predictvglm(italy_model, newdata = starting_grid, se.fit = T)$fitted.values) # rename the columns names(italy_predictions) &lt;- placing_df$prob[1:18] # Extract the standard error for each prediction italy_standard_errors &lt;- as.data.frame(predictvglm(italy_model, newdata = starting_grid, se.fit = T)$se.fit) # rename the standard error names(italy_standard_errors) &lt;- placing_df$se[1:18] # tidy up probabilities italy_preds_se_full &lt;- italy_predictions %&gt;% mutate(starting = 1:n()) %&gt;% pivot_longer(- starting, names_to = &#39;placing&#39;, values_to = &#39;prob&#39;) %&gt;% bind_cols(italy_standard_errors %&gt;% mutate(starting = 1:n()) %&gt;% pivot_longer(- starting, names_to = &#39;placing&#39;, values_to = &#39;se&#39;) %&gt;% dplyr::select(se)) %&gt;% mutate(placing = as.numeric(str_remove(placing, &#39;prob_&#39;))) %&gt;% mutate(prob = exp(prob)) %&gt;% mutate(prob = prob / (1 + prob)) %&gt;% group_by(starting) %&gt;% mutate(discrete_prob = ifelse(is.na(prob - lag(prob)), prob, (prob - lag(prob)))) %&gt;% ungroup() # Plot italy_preds_se_full %&gt;% mutate(starting_pos = as.factor(paste0(&#39;P&#39;,starting)), starting_pos = fct_reorder(starting_pos, starting)) %&gt;% ggplot(aes(placing, discrete_prob)) + geom_point() + geom_segment(aes(x = placing, xend = placing, y = 0, yend = discrete_prob), size = 2, alpha = 0.5, col = &#39;green4&#39;) + theme_bw() + facet_wrap(~ starting, labeller = label_both) + labs(x = &#39;Final Race Classification&#39;, y = &#39;Probability&#39;, title = expression(Discrete~Probability~of~finishing~P[n]), subtitle = &#39;Italian Grand Prix: 2014 - 2023&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) 6.5.16 Singapore singapore_model &lt;- vglm(Position_race ~ Position_grid, family = cumulative(parallel = T, reverse = F), data = grids_and_races %&gt;% filter(Race == &#39;singapore&#39;)) # Make Predictions for each Finishing position (1:20), using each Starting Position (1:20) singapore_predictions &lt;- as.data.frame(predictvglm(singapore_model, newdata = starting_grid, se.fit = T)$fitted.values) # rename the columns names(singapore_predictions) &lt;- placing_df$prob[1:18] # Extract the standard error for each prediction singapore_standard_errors &lt;- as.data.frame(predictvglm(singapore_model, newdata = starting_grid, se.fit = T)$se.fit) # rename the standard error names(singapore_standard_errors) &lt;- placing_df$se[1:18] # tidy up probabilities singapore_preds_se_full &lt;- singapore_predictions %&gt;% mutate(starting = 1:n()) %&gt;% pivot_longer(- starting, names_to = &#39;placing&#39;, values_to = &#39;prob&#39;) %&gt;% bind_cols(singapore_standard_errors %&gt;% mutate(starting = 1:n()) %&gt;% pivot_longer(- starting, names_to = &#39;placing&#39;, values_to = &#39;se&#39;) %&gt;% dplyr::select(se)) %&gt;% mutate(placing = as.numeric(str_remove(placing, &#39;prob_&#39;))) %&gt;% mutate(prob = exp(prob)) %&gt;% mutate(prob = prob / (1 + prob)) %&gt;% group_by(starting) %&gt;% mutate(discrete_prob = ifelse(is.na(prob - lag(prob)), prob, (prob - lag(prob)))) %&gt;% ungroup() # Plot singapore_preds_se_full %&gt;% mutate(starting_pos = as.factor(paste0(&#39;P&#39;,starting)), starting_pos = fct_reorder(starting_pos, starting)) %&gt;% ggplot(aes(placing, discrete_prob)) + geom_point() + geom_segment(aes(x = placing, xend = placing, y = 0, yend = discrete_prob), size = 2, alpha = 0.5, col = &#39;red&#39;) + theme_bw() + facet_wrap(~ starting, labeller = label_both) + labs(x = &#39;Final Race Classification&#39;, y = &#39;Probability&#39;, title = expression(Discrete~Probability~of~finishing~P[n]), subtitle = &#39;Singapore Grand Prix: 2014 - 2023&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) 6.5.17 Japan japan_model &lt;- vglm(Position_race ~ Position_grid, family = cumulative(parallel = T, reverse = F), data = grids_and_races %&gt;% filter(Race == &#39;japan&#39;)) # Make Predictions for each Finishing position (1:20), using each Starting Position (1:20) japan_predictions &lt;- as.data.frame(predictvglm(japan_model, newdata = starting_grid, se.fit = T)$fitted.values) # rename the columns names(japan_predictions) &lt;- placing_df$prob[1:21] # Extract the standard error for each prediction japan_standard_errors &lt;- as.data.frame(predictvglm(japan_model, newdata = starting_grid, se.fit = T)$se.fit) # rename the standard error names(japan_standard_errors) &lt;- placing_df$se[1:21] # tidy up probabilities japan_preds_se_full &lt;- japan_predictions %&gt;% mutate(starting = 1:n()) %&gt;% pivot_longer(- starting, names_to = &#39;placing&#39;, values_to = &#39;prob&#39;) %&gt;% bind_cols(japan_standard_errors %&gt;% mutate(starting = 1:n()) %&gt;% pivot_longer(- starting, names_to = &#39;placing&#39;, values_to = &#39;se&#39;) %&gt;% dplyr::select(se)) %&gt;% mutate(placing = as.numeric(str_remove(placing, &#39;prob_&#39;))) %&gt;% mutate(prob = exp(prob)) %&gt;% mutate(prob = prob / (1 + prob)) %&gt;% group_by(starting) %&gt;% mutate(discrete_prob = ifelse(is.na(prob - lag(prob)), prob, (prob - lag(prob)))) %&gt;% ungroup() # Plot japan_preds_se_full %&gt;% mutate(starting_pos = as.factor(paste0(&#39;P&#39;,starting)), starting_pos = fct_reorder(starting_pos, starting)) %&gt;% ggplot(aes(placing, discrete_prob)) + geom_point() + geom_segment(aes(x = placing, xend = placing, y = 0, yend = discrete_prob), size = 2, alpha = 0.5, col = &#39;darkred&#39;) + theme_bw() + facet_wrap(~ starting, labeller = label_both) + labs(x = &#39;Final Race Classification&#39;, y = &#39;Probability&#39;, title = expression(Discrete~Probability~of~finishing~P[n]), subtitle = &#39;Japanese Grand Prix: 2014 - 2023&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) 6.5.18 Qatar qatar_model &lt;- vglm(Position_race ~ Position_grid, family = cumulative(parallel = T, reverse = F), data = grids_and_races %&gt;% filter(Race == &#39;qatar&#39;)) # Make Predictions for each Finishing position (1:20), using each Starting Position (1:20) qatar_predictions &lt;- as.data.frame(predictvglm(qatar_model, newdata = starting_grid, se.fit = T)$fitted.values) # rename the columns names(qatar_predictions) &lt;- placing_df$prob[1:17] # Extract the standard error for each prediction qatar_standard_errors &lt;- as.data.frame(predictvglm(qatar_model, newdata = starting_grid, se.fit = T)$se.fit) # rename the standard error names(qatar_standard_errors) &lt;- placing_df$se[1:17] # tidy up probabilities qatar_preds_se_full &lt;- qatar_predictions %&gt;% mutate(starting = 1:n()) %&gt;% pivot_longer(- starting, names_to = &#39;placing&#39;, values_to = &#39;prob&#39;) %&gt;% bind_cols(qatar_standard_errors %&gt;% mutate(starting = 1:n()) %&gt;% pivot_longer(- starting, names_to = &#39;placing&#39;, values_to = &#39;se&#39;) %&gt;% dplyr::select(se)) %&gt;% mutate(placing = as.numeric(str_remove(placing, &#39;prob_&#39;))) %&gt;% mutate(prob = exp(prob)) %&gt;% mutate(prob = prob / (1 + prob)) %&gt;% group_by(starting) %&gt;% mutate(discrete_prob = ifelse(is.na(prob - lag(prob)), prob, (prob - lag(prob)))) %&gt;% ungroup() # Plot qatar_preds_se_full %&gt;% mutate(starting_pos = as.factor(paste0(&#39;P&#39;,starting)), starting_pos = fct_reorder(starting_pos, starting)) %&gt;% ggplot(aes(placing, discrete_prob)) + geom_point() + geom_segment(aes(x = placing, xend = placing, y = 0, yend = discrete_prob), size = 2, alpha = 0.5, col = &#39;maroon&#39;) + theme_bw() + facet_wrap(~ starting, labeller = label_both) + labs(x = &#39;Final Race Classification&#39;, y = &#39;Probability&#39;, title = expression(Discrete~Probability~of~finishing~P[n]), subtitle = &#39;Qatar Grand Prix: 2021 &amp; 2023&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) 6.5.19 United States (Circuit of the Americas in Austin, Texas) library(ggstar) united_states_model &lt;- vglm(Position_race ~ Position_grid, family = cumulative(parallel = T, reverse = F), data = grids_and_races %&gt;% filter(Race == &#39;united-states&#39;)) # Make Predictions for each Finishing position (1:20), using each Starting Position (1:20) united_states_predictions &lt;- as.data.frame(predictvglm(united_states_model, newdata = starting_grid, se.fit = T)$fitted.values) # rename the columns names(united_states_predictions) &lt;- placing_df$prob[1:17] # Extract the standard error for each prediction united_states_standard_errors &lt;- as.data.frame(predictvglm(united_states_model, newdata = starting_grid, se.fit = T)$se.fit) # rename the standard error names(united_states_standard_errors) &lt;- placing_df$se[1:17] # tidy up probabilities united_states_preds_se_full &lt;- united_states_predictions %&gt;% mutate(starting = 1:n()) %&gt;% pivot_longer(- starting, names_to = &#39;placing&#39;, values_to = &#39;prob&#39;) %&gt;% bind_cols(united_states_standard_errors %&gt;% mutate(starting = 1:n()) %&gt;% pivot_longer(- starting, names_to = &#39;placing&#39;, values_to = &#39;se&#39;) %&gt;% dplyr::select(se)) %&gt;% mutate(placing = as.numeric(str_remove(placing, &#39;prob_&#39;))) %&gt;% mutate(prob = exp(prob)) %&gt;% mutate(prob = prob / (1 + prob)) %&gt;% group_by(starting) %&gt;% mutate(discrete_prob = ifelse(is.na(prob - lag(prob)), prob, (prob - lag(prob)))) %&gt;% ungroup() # Plot united_states_preds_se_full %&gt;% mutate(starting_pos = as.factor(paste0(&#39;P&#39;,starting)), starting_pos = fct_reorder(starting_pos, starting)) %&gt;% ggplot(aes(placing, discrete_prob)) + geom_segment(aes(x = placing, xend = placing, y = 0, yend = discrete_prob), size = 2, alpha = 0.75, col = &#39;red&#39;) + geom_star(col = &#39;blue&#39;, size = 1.5, alpha = 1, fill = &#39;white&#39;) + theme_bw() + facet_wrap(~ starting, labeller = label_both) + labs(x = &#39;Final Race Classification&#39;, y = &#39;Probability&#39;, title = expression(Discrete~Probability~of~finishing~P[n]), subtitle = &#39;United States Grand Prix: 2014 - 2023&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) Plotting stars in ggplot The ggstar package provides different geometries for ggplot2 to create more easily discernible shapes. For instance, you can use this package to plot specific triangles (i.e. right triangles), hearts, or ellipses. As an admittedly obnoxious American, I used the geom_star() function to plot stars for my data points. For more information on the ggstar package, visit this link: https://cran.r-project.org/web/packages/ggstar/vignettes/ggstar.html 6.5.20 Mexico mexico_model &lt;- vglm(Position_race ~ Position_grid, family = cumulative(parallel = T, reverse = F), data = grids_and_races %&gt;% filter(Race == &#39;mexico&#39;)) # Make Predictions for each Finishing position (1:20), using each Starting Position (1:20) mexico_predictions &lt;- as.data.frame(predictvglm(mexico_model, newdata = starting_grid, se.fit = T)$fitted.values) # rename the columns names(mexico_predictions) &lt;- placing_df$prob[1:19] # Extract the standard error for each prediction mexico_standard_errors &lt;- as.data.frame(predictvglm(mexico_model, newdata = starting_grid, se.fit = T)$se.fit) # rename the standard error names(mexico_standard_errors) &lt;- placing_df$se[1:19] # tidy up probabilities mexico_preds_se_full &lt;- mexico_predictions %&gt;% mutate(starting = 1:n()) %&gt;% pivot_longer(- starting, names_to = &#39;placing&#39;, values_to = &#39;prob&#39;) %&gt;% bind_cols(mexico_standard_errors %&gt;% mutate(starting = 1:n()) %&gt;% pivot_longer(- starting, names_to = &#39;placing&#39;, values_to = &#39;se&#39;) %&gt;% dplyr::select(se)) %&gt;% mutate(placing = as.numeric(str_remove(placing, &#39;prob_&#39;))) %&gt;% mutate(prob = exp(prob)) %&gt;% mutate(prob = prob / (1 + prob)) %&gt;% group_by(starting) %&gt;% mutate(discrete_prob = ifelse(is.na(prob - lag(prob)), prob, (prob - lag(prob)))) %&gt;% ungroup() # Plot mexico_preds_se_full %&gt;% mutate(starting_pos = as.factor(paste0(&#39;P&#39;,starting)), starting_pos = fct_reorder(starting_pos, starting)) %&gt;% ggplot(aes(placing, discrete_prob)) + geom_point() + geom_segment(aes(x = placing, xend = placing, y = 0, yend = discrete_prob), size = 2, alpha = 0.5, col = &#39;darkgreen&#39;) + theme_bw() + facet_wrap(~ starting, labeller = label_both) + labs(x = &#39;Final Race Classification&#39;, y = &#39;Probability&#39;, title = expression(Discrete~Probability~of~finishing~P[n]), subtitle = &#39;Mexican Grand Prix: 2015 - 2023&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) 6.5.21 Brazil # These predictions were generated earlier in the chapter brazil_preds_se_full %&gt;% mutate(starting_pos = as.factor(paste0(&#39;P&#39;,starting)), starting_pos = fct_reorder(starting_pos, starting)) %&gt;% ggplot(aes(placing, discrete_prob)) + geom_segment(aes(x = placing, xend = placing, y = 0, yend = discrete_prob), size = 2, alpha = 0.5, col = &#39;green&#39;) + geom_point() + theme_bw() + facet_wrap(~ starting, labeller = label_both) + labs(x = &#39;Final Race Classification&#39;, y = &#39;Probability&#39;, title = expression(Discrete~Probability~of~finishing~P[n]), subtitle = &#39;Sao Paulo Grand Prix: 2014 - 2023&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) 6.5.22 Abu Dhabi abudhabi_model &lt;- vglm(Position_race ~ Position_grid, family = cumulative(parallel = T, reverse = F), data = grids_and_races %&gt;% filter(Race == &#39;abu-dhabi&#39;)) # Make Predictions for each Finishing position (1:20), using each Starting Position (1:20) abudhabi_predictions &lt;- as.data.frame(predictvglm(abudhabi_model, newdata = starting_grid, se.fit = T)$fitted.values) # rename the columns names(abudhabi_predictions) &lt;- placing_df$prob[1:19] # Extract the standard error for each prediction abudhabi_standard_errors &lt;- as.data.frame(predictvglm(abudhabi_model, newdata = starting_grid, se.fit = T)$se.fit) # rename the standard error names(abudhabi_standard_errors) &lt;- placing_df$se[1:19] # tidy up probabilities abudhabi_preds_se_full &lt;- abudhabi_predictions %&gt;% mutate(starting = 1:n()) %&gt;% pivot_longer(- starting, names_to = &#39;placing&#39;, values_to = &#39;prob&#39;) %&gt;% bind_cols(abudhabi_standard_errors %&gt;% mutate(starting = 1:n()) %&gt;% pivot_longer(- starting, names_to = &#39;placing&#39;, values_to = &#39;se&#39;) %&gt;% dplyr::select(se)) %&gt;% mutate(placing = as.numeric(str_remove(placing, &#39;prob_&#39;))) %&gt;% mutate(prob = exp(prob)) %&gt;% mutate(prob = prob / (1 + prob)) %&gt;% group_by(starting) %&gt;% mutate(discrete_prob = ifelse(is.na(prob - lag(prob)), prob, (prob - lag(prob)))) %&gt;% ungroup() # Plot abudhabi_preds_se_full %&gt;% mutate(starting_pos = as.factor(paste0(&#39;P&#39;,starting)), starting_pos = fct_reorder(starting_pos, starting)) %&gt;% ggplot(aes(placing, discrete_prob)) + geom_point() + geom_segment(aes(x = placing, xend = placing, y = 0, yend = discrete_prob), size = 2, alpha = 0.75, col = &#39;black&#39;) + theme_bw() + facet_wrap(~ starting, labeller = label_both) + labs(x = &#39;Final Race Classification&#39;, y = &#39;Probability&#39;, title = expression(Discrete~Probability~of~finishing~P[n]), subtitle = &#39;Abu Dhabi Grand Prix: 2014 - 2023&#39;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) 6.6 Next Chapter In these models I excluded DNFs (cars that did not finish) from the dataset. In the next chapter, I will instead focus on DNFs. Specifically, I will use logistic regression models to better understand factors that may influence the rate of DNFs. "],["dnf-did-not-finish.html", "Chapter 7 DNF (Did Not Finish) 7.1 Logistic Regression 7.2 Yearly Differences 7.3 Differences by constructor 7.4 DNFs by race of the season 7.5 Next Chapter", " Chapter 7 DNF (Did Not Finish) There are lots of reasons a car may DNF from a race. Mechanical failure, crashes, and even precautionary considerations can lead to a car retiring early from a race. DNFs are almost always unexpected, and can have substantial influence on the outcome of the race for other drivers (see Abu Dhabi 2021). But, are there any identifiable risk factors for DNFs? For example, are there differences among the constructors or engine suppliers? Are certain drivers more prone to be involved in a crash? We can explore these ideas with a model! Throughout this chapter, I will be attempting to answer the following question: What is the probability that a car DNFs? A DNF, naturally, is a binary event: a car finished the race, or it does not. This is important to note prior to modeling. If we were to try and use a linear model to model this binary response, the predicted outcome could be any number between negative infinity and positive infinity. Which …. doesn’t make sense. Generalized Linear Models (GLM) allow you to build a linear relationship between the response and predictors, even when their underlying relationship is not linear and variance is not constant. Recall from last chapter that we ran into a scenario where we failed to meet the assumptions of the linear regression model. This happens quite often actually! Without going into too much detail, there are three common situations where linear regression models are a poor choice: The response is a count The response is binary or a proportion The response is positive continuous 7.1 Logistic Regression A logistic regression model allows me to estimate the effect that a predictor variable (or more than one predictor) has on the probability of a given outcome. It is actually a simpler implementation of the ordinal regression model from last chapter. The ordinal regression model estimates the probability of many ordered levels of an outcome (i.e. P1, P2, P3, etc.), while a logistic regression model estimates the probability of only two possibilities: a car finishes or does not. A logistic regression model will use a transformation function, the logistic function, that allows us to relate the outcome (finish vs DNF) to a value between 0 and 1. So, the logistic function maps a typical probability range (0,1) to an unbounded range (-infinity, infinity). The logit function is defined mathematically as: \\[logit(x) = log(\\frac{x}{1-x})\\] The inverse logit function maps an unbounded range back to a probability range (0,1). \\[logit^{-1}(x) = \\frac{e^{x}}{1 + e^{x}}\\] To visualize how the logit and inverse logit functions work, I made the following figures. I create some sample data called x. I then plot x vs logit(x). Hopefully this helps! Here is a plot of the linear predictor vs linear predictors transformed to probabilities using the inverse logit function. # create sample data x &lt;- seq(-10, 10, 0.1) # apply an inverse logit function to x inv_logit_x &lt;- exp(x) / (1 + exp(x)) # Plot these two variables qplot(x, inv_logit_x) + labs(y=expression(paste(logit^-1,&quot;(x)&quot;))) + theme_bw() We can also use the plogis() function to perform the inverse logit transformation. See here: # Plot these two variables qplot(x, plogis(x)) + labs(y=expression(paste(logit^-1,&quot;(x)&quot;))) + theme_bw() A logistic regression model should allow us to analyze our data to understand how the probability of a DNF varies across various predictor variables. One research question that I have is: Does the probability of a DNF vary across years? Well, in an attempt to answer that question, one could simply calculate the proportion of DNFs from 2014 to 2023. There are better ways! To demonstrate how to use a logistic regression model to answer this type of question, I will start with an even simpler example. If we were to pool all years together and calculate the proportion of cars that DNF, that calculation would reveal that 17.2% of cars DNF from a race: mean(races_allyears_dnf$DNF, na.rm = T) ## [1] 0.1719205 A more statistically sound way to estimate this rate is to use a logistic regression model. The most basic logistic regression model would be an intercept-only model. Notice that the transformed intercept of this model is identical to the manually calculated proportion of 17.2%. dnf.model.0 &lt;- glm(DNF ~ 1, data = races_allyears_dnf, family = binomial(link = &#39;logit&#39;)) # Transform the coefficient to probability scale plogis(coef(dnf.model.0)) ## (Intercept) ## 0.1719205 Let’s expand this model in the next section. 7.2 Yearly Differences Ok, back to our original research question! Does the proportion of DNFs vary over time? One would think! I would assume that as a regulation era evolves, the engineers will build increasingly dependable cars. Let’s see if the data backs this up. In the code below, I will fit a logistic regression model using glm() and transform the coefficient estimates using the plogis() function. dnf.model.1 &lt;- glm(DNF ~ 0 + Year, data = races_allyears_dnf, family = binomial(link = &#39;logit&#39;)) # Transform the coefficient to probability scale data.frame(Probability = plogis(coef(dnf.model.1))) ## Probability ## Year2014 0.2088452 ## Year2015 0.1994681 ## Year2016 0.1709957 ## Year2017 0.2275000 ## Year2018 0.1952381 ## Year2019 0.1380952 ## Year2020 0.1617647 ## Year2021 0.1250000 ## Year2022 0.1642857 ## Year2023 0.1366743 It looks like the data support this to some degree. Just by looking at this table, it appears that the proportion is trending downward but also bouncing around from year to year. We can plot this data to make it a little more clear: tidy(dnf.model.1, exp = T) %&gt;% mutate(term = str_remove(term, &quot;Year&quot;)) %&gt;% ggplot(aes(term, estimate)) + geom_line(group = 1) + geom_point() + theme_bw() + labs(y = &#39;Estimated Probability of a DNF&#39;, x = &#39;Year&#39;, title = &#39;Yearly trend in DNF Probability&#39;) + scale_y_continuous(labels = scales::percent_format()) + theme(plot.title = element_text(hjust = 0.5)) If we were to look at the number of DNFs by track, some locations stand out. For instance, Baku, Australia, and the U.S. Grand Prix all have DNF percentages of at least 20% since 2014. races_allyears_dnf %&gt;% group_by(Circuit) %&gt;% summarize(`% of DNFs` = round(mean(DNF, na.rm = T), 3)) %&gt;% ungroup() %&gt;% arrange(desc(`% of DNFs`)) %&gt;% mutate(`% of DNFs` = scales::percent(`% of DNFs`)) %&gt;% gt() #usjkanfprw table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #usjkanfprw thead, #usjkanfprw tbody, #usjkanfprw tfoot, #usjkanfprw tr, #usjkanfprw td, #usjkanfprw th { border-style: none; } #usjkanfprw p { margin: 0; padding: 0; } #usjkanfprw .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #usjkanfprw .gt_caption { padding-top: 4px; padding-bottom: 4px; } #usjkanfprw .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #usjkanfprw .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #usjkanfprw .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #usjkanfprw .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #usjkanfprw .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #usjkanfprw .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #usjkanfprw .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #usjkanfprw .gt_column_spanner_outer:first-child { padding-left: 0; } #usjkanfprw .gt_column_spanner_outer:last-child { padding-right: 0; } #usjkanfprw .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #usjkanfprw .gt_spanner_row { border-bottom-style: hidden; } #usjkanfprw .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #usjkanfprw .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #usjkanfprw .gt_from_md > :first-child { margin-top: 0; } #usjkanfprw .gt_from_md > :last-child { margin-bottom: 0; } #usjkanfprw .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #usjkanfprw .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #usjkanfprw .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #usjkanfprw .gt_row_group_first td { border-top-width: 2px; } #usjkanfprw .gt_row_group_first th { border-top-width: 2px; } #usjkanfprw .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #usjkanfprw .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #usjkanfprw .gt_first_summary_row.thick { border-top-width: 2px; } #usjkanfprw .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #usjkanfprw .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #usjkanfprw .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #usjkanfprw .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #usjkanfprw .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #usjkanfprw .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #usjkanfprw .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #usjkanfprw .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #usjkanfprw .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #usjkanfprw .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #usjkanfprw .gt_left { text-align: left; } #usjkanfprw .gt_center { text-align: center; } #usjkanfprw .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #usjkanfprw .gt_font_normal { font-weight: normal; } #usjkanfprw .gt_font_bold { font-weight: bold; } #usjkanfprw .gt_font_italic { font-style: italic; } #usjkanfprw .gt_super { font-size: 65%; } #usjkanfprw .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #usjkanfprw .gt_asterisk { font-size: 100%; vertical-align: 0; } #usjkanfprw .gt_indent_1 { text-indent: 5px; } #usjkanfprw .gt_indent_2 { text-indent: 10px; } #usjkanfprw .gt_indent_3 { text-indent: 15px; } #usjkanfprw .gt_indent_4 { text-indent: 20px; } #usjkanfprw .gt_indent_5 { text-indent: 25px; } Circuit % of DNFs australia 26.10% azerbaijan 24.20% germany 22.10% singapore 21.50% malaysia 20.50% saudi-arabia 20.00% united-states 20.00% canada 19.50% great-britain 19.20% austria 19.10% monaco 18.50% italy 18.30% europe 18.20% bahrain 17.90% russia 16.60% belgium 16.20% brazil 15.60% mexico 15.40% hungary 15.20% las-vegas 15.00% styria 15.00% abu-dhabi 13.40% netherlands 13.30% japan 12.80% france 12.50% miami 12.50% spain 12.30% china 12.10% qatar 10.00% turkey 10.00% portugal 5.00% We can add an interaction term to this model, and make a yearly estimate for each circuit in our data. dnf.model.2 &lt;- glm(DNF ~ 0 + Year:Circuit, data = races_allyears_dnf, family = binomial(link = &#39;logit&#39;)) Here’s a comparison of this updated model’s output across a few different circuits: tidy(dnf.model.2, exp = T) %&gt;% mutate(term = str_remove(term, &quot;Year&quot;)) %&gt;% separate(term, sep = &#39;:Circuit&#39;, c(&quot;Year&quot;, &quot;Circuit&quot;)) %&gt;% filter(Circuit %in% c(&#39;monaco&#39;, &#39;united-states&#39;, &#39;great-britain&#39;)) %&gt;% ggplot(aes(Year, estimate, group = Circuit, col = Circuit)) + geom_line() + geom_point() + theme_bw() + labs(y = &#39;Estimated Probability of a DNF&#39;, x = &#39;Year&#39;, title = &#39;Yearly trend in DNF Probability&#39;) + scale_y_continuous(labels = scales::percent_format()) + theme( axis.title.x = element_text(size = 14), axis.title.y = element_text(size = 14), axis.text.x = element_text(vjust = 0.5, hjust=1, size = 12), axis.text.y = element_text(size = 12), plot.title = element_text(hjust = 0.5, size = 18), legend.text = element_text(size = 12), legend.title = element_text(hjust = 0.5, size = 14)) An alternative to fitting an estimate for each race-year combination is to model both the year and track estimates, but not as an interaction term. dnf.model.3 &lt;- glm(DNF ~ 0 + Year + Circuit, data = races_allyears_dnf, family = binomial(link = &#39;logit&#39;)) As we can see in the figure below, this model assumes a consistent yearly effect and adjusts the probability estimate up or down based on the circuit. So, for that rainy 2015 U.S. Grand Prix, this model gives a more sober result. emmeans(dnf.model.3, ~ Circuit + Year, type = &#39;response&#39;) %&gt;% as.data.frame() %&gt;% filter(Circuit %in% c(&#39;monaco&#39;, &#39;united-states&#39;, &#39;great-britain&#39;)) %&gt;% ggplot(aes(Year, prob, group = Circuit, col = Circuit)) + geom_line() + geom_point() + theme_bw() + labs(y = &#39;Estimated Probability of a DNF&#39;, x = &#39;Year&#39;, title = &#39;Yearly trend in DNF Probability&#39;) + scale_y_continuous(labels = scales::percent_format()) + theme( axis.title.x = element_text(size = 14), axis.title.y = element_text(size = 14), axis.text.x = element_text(vjust = 0.5, hjust=1, size = 12), axis.text.y = element_text(size = 12), plot.title = element_text(hjust = 0.5, size = 18), legend.text = element_text(size = 12), legend.title = element_text(hjust = 0.5, size = 14)) 7.3 Differences by constructor We could use a similar approach to estimate the probability of a DNF by constructor. One could hypothesize that the car with the best power unit, Mercedes, could run the engine at a more conservative mode, thus reducing the risk of failure. Let’s see if the data shows any significant shifts in DNF probability by constructor. dnf.model.4 &lt;- glm(DNF ~ 0 + Car, data = races_allyears_dnf, family = binomial(link = &#39;logit&#39;)) emmeans(dnf.model.4, ~ Car, type = &#39;response&#39;) %&gt;% as.data.frame() %&gt;% ggplot(aes(prob, fct_reorder(Car, desc(prob)))) + geom_line() + geom_point() + theme_bw() + labs(x = &#39;Estimated Probability of a DNF&#39;, y = &#39;Constructor&#39;, title = &#39;DNF Probability by Constructor&#39;) + scale_x_continuous(labels = scales::percent_format()) + theme(plot.title = element_text(hjust = 0.5)) There certainly appears to be some variability in DNF probability across the various constructors. During the Turbo-hybrid era, Mercedes unquestionably had the best power unit (overall). And according to this data, Mercedes also had the lowest probability of a DNF from 2014 to 2023 (Red Bull was quite reliable in 2023). In fact, McLaren using a Mercedes power unit had the third lowest DNF probability. Does this vary by year? dnf.model.5 &lt;- glm(DNF ~ 0 + Car:Year, data = races_allyears_dnf, family = binomial(link = &#39;logit&#39;)) After fitting the model, I’ll focus on the top 3 teams over this ten year period: Mercedes, Ferrari, and Red Bull. It looks like Mercedes had consistently low DNF rates, while Red Bull experienced an improvement over the last few years. emmeans(dnf.model.5, ~ Car:Year, type = &#39;response&#39;) %&gt;% as.data.frame() %&gt;% filter(!is.na(Year), !is.na(prob), Car %in% c(&#39;Mercedes&#39;, &#39;Ferrari&#39;, &#39;Red Bull Racing Honda RBPT&#39;, &#39;Red Bull Racing RBPT&#39;, &#39;Red Bull Racing Renault&#39;, &#39;Red Bull Racing Honda&#39;, &#39;Red Bull Racing TAG Heuer&#39;)) %&gt;% mutate(constructor = case_when( Car != &#39;Mercedes&#39; &amp; Car != &#39;Ferrari&#39; ~ &#39;Red Bull&#39;, Car == &#39;Mercedes&#39; ~ &#39;Mercedes&#39;, Car == &#39;Ferrari&#39; ~ &#39;Ferrari&#39;)) %&gt;% ggplot(aes(Year, prob, group = constructor, col = constructor)) + geom_path() + geom_point() + theme_bw() + labs(y = &#39;Estimated Probability of a DNF&#39;, col = &#39;Constructor&#39;, title = &#39;DNF Probability by Year &amp; Constructor&#39;) + scale_y_continuous(labels = scales::percent_format()) + theme( axis.title.x = element_text(size = 14), axis.title.y = element_text(size = 14), axis.text.x = element_text(vjust = 0.5, hjust=1, size = 12), axis.text.y = element_text(size = 12), plot.title = element_text(hjust = 0.5, size = 18), legend.text = element_text(size = 12), legend.title = element_text(hjust = 0.5, size = 14)) Here’s a look at each all teams, separated by year: library(tidytext) emmeans(dnf.model.5, ~ Car:Year, type = &#39;response&#39;) %&gt;% as.data.frame() %&gt;% filter(!is.na(prob)) %&gt;% group_by(Year) %&gt;% mutate(rank = rank(prob)) %&gt;% ungroup() %&gt;% mutate(constructor = reorder_within(Car, desc(rank), Year)) %&gt;% ggplot(aes(prob, constructor)) + geom_line() + geom_point() + theme_bw() + labs(x = &#39;Estimated Probability of a DNF&#39;, y = &#39;Constructor&#39;, title = &#39;DNF Probability by Constructor&#39;) + scale_x_continuous(labels = scales::percent_format()) + theme(plot.title = element_text(hjust = 0.5)) + facet_wrap(~ Year, scales = &#39;free_y&#39;, ncol = 3) + scale_y_reordered() 7.4 DNFs by race of the season Another possible explanatory variable of DNFs is race number of the season. There is probably more uncertainty about the engineering of the car at the beginning of a season. So, one could hypothesize that the probability of DNF’ing decreases over the course of a season. Let’s try and model this idea. First, we’ll need to label each race in chronological order. For example, Bahrain was race #1 in 2021, 2022, and 2023. # Number each race during each of our ten years circuit_numbers &lt;- races_allyears_dnf %&gt;% distinct(Year, Circuit) %&gt;% group_by(Year) %&gt;% mutate(race_number = 1:n()) # Add the race number to our race result dataframe races_allyears_dnf_numbered &lt;- races_allyears_dnf %&gt;% left_join(circuit_numbers, by = c(&#39;Year&#39;, &#39;Circuit&#39;)) We can fit a model to our data using race number of the season as a single predictor variable. dnf.model.6 &lt;- glm(DNF ~ race_number, data = races_allyears_dnf_numbered, family = binomial(link = &#39;logit&#39;)) If we look at the summary output below, we can see that the p-value is &gt; 0.05, demonstrating that this is not a statistically significant at the 0.05 level. What does this mean? See the text box below for more detail and resouces on p-values. Interpreting p-values What is a p-value? My favorite definition is provided in Statistics: Unlocking the Power of Data by Lock, Lock, Lock, Lock, and Lock. (It’s a family of statisticians with the last name Lock): p-value: the proportion of sample, if the null hypothesis is true, that would give a statistic as extreme, or more extreme, as the observed sample. Essentially, we use the p-value to measure strength of evidence, in a statistical sense. It can describe how unusual our data is. If our data is very unusual, it suggests there is evidence of a real relationship. I am taking some liberties with this explanation, so if you are interesting in legitimately learning about hypothesis testing and p-values, I highly recommend reading the Lock book and visiting this fantastic simulator: https://www.lock5stat.com/StatKey/ # print summary summary(dnf.model.6) ## ## Call: ## glm(formula = DNF ~ race_number, family = binomial(link = &quot;logit&quot;), ## data = races_allyears_dnf_numbered) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.6471 -0.6290 -0.6078 -0.5838 1.9418 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.44469 0.08239 -17.534 &lt;2e-16 *** ## race_number -0.01255 0.00712 -1.762 0.078 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 3785.2 on 4123 degrees of freedom ## Residual deviance: 3782.0 on 4122 degrees of freedom ## AIC: 3786 ## ## Number of Fisher Scoring iterations: 4 # exponentiate the coefficients exp(cbind(OR = coef(dnf.model.6), confint(dnf.model.6))) ## OR 2.5 % 97.5 % ## (Intercept) 0.2358199 0.2003353 0.2767322 ## race_number 0.9875320 0.9738144 1.0013839 So, let’s just accept that these results indicate a non-significant predictor in race number. What could cause that? For one, DNFs in this dataset include both crashes and mechanical failures. Obviously crashes should be more random, while mechanical failures may not be. Nonetheless, this is a book about data visualization. So… let’s at least plot these results. First, I’ll need to tidy of some model predictions for this model. # define the new data new.data.logreg &lt;- tibble(race_number = seq(1, 22, 1)) # compute the fitted lines and SE&#39;s predictions.model6 &lt;- predict(dnf.model.6, newdata = new.data.logreg, type = &quot;link&quot;, se.fit = TRUE) %&gt;% data.frame() %&gt;% mutate(ll = fit - 1.96 * se.fit, ul = fit + 1.96 * se.fit) %&gt;% dplyr::select( -se.fit) %&gt;% mutate_all(plogis) %&gt;% bind_cols(new.data.logreg) And then, we can plot observed data along with the logistic regression model fit line using this code: predictions.model6 %&gt;% ggplot(aes(x = race_number)) + geom_ribbon(aes(ymin = ll, ymax = ul), alpha = 0.5) + geom_line(aes(y = fit)) + stat_dots(data = races_allyears_dnf_numbered, aes(y = DNF, side = ifelse(DNF == 0, &quot;top&quot;, &quot;bottom&quot;), col= factor(DNF)), scale = 0.25, size = 5, alpha = 0.1, show.legend = T) + scale_color_manual(breaks = c(1,0),values = c(&quot;firebrick4&quot;, &quot;darkgrey&quot;)) + scale_y_continuous(&quot;Probability of a DNF&quot;, expand = c(0, 0)) + labs(x = &#39;Race number of the season&#39;, color=&quot;DNF = 1 \\nRace Finish = 0&quot;, title = &#39;Probability of DNF by race number of a season&#39;, subtitle = &#39;All Grands Prix: 2014 - 2023&#39;) + guides(colour = guide_legend(override.aes = list(alpha = 0.5, size = 0.5))) + theme_bw() + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) This figure perhaps further explains the reason behind the insignificant p-value. There is no meaningful trend in the proportion of DNFs over the course of a racing season. Adding dots to a ggplot In the figure above, I used stat_dots() to describe the distribution of data along the top and bottom of the x-axis. stat_dots() is one of many very useful functions included in the ggdist R package. The ggdist package provides a flexible set of ggplot2 geometries and statistics designed especially for visualizing distributions and uncertainty. For more information on plotting dots with ggdist read this great article: https://mjskay.github.io/ggdist/articles/dotsinterval.html Even though this model was a dud, we can attempt to add some clarity by including a year x race number interaction term. This updated interaction model will estimate a race number coefficient for each year of our data (2014 - 2023). dnf.model.7 &lt;- glm(DNF ~ race_number:Year, data = races_allyears_dnf_numbered, family = binomial(link = &#39;logit&#39;)) # print summary summary(dnf.model.7) ## ## Call: ## glm(formula = DNF ~ race_number:Year, family = binomial(link = &quot;logit&quot;), ## data = races_allyears_dnf_numbered) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.7011 -0.6438 -0.6067 -0.5360 2.1296 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.432272 0.084067 -17.037 &lt;2e-16 *** ## race_number:Year2014 -0.004151 0.013219 -0.314 0.7535 ## race_number:Year2015 0.003021 0.012967 0.233 0.8158 ## race_number:Year2016 -0.011753 0.011620 -1.011 0.3118 ## race_number:Year2017 0.007720 0.011900 0.649 0.5165 ## race_number:Year2018 -0.003605 0.011632 -0.310 0.7566 ## race_number:Year2019 -0.027829 0.012974 -2.145 0.0320 * ## race_number:Year2020 -0.033301 0.021577 -1.543 0.1227 ## race_number:Year2021 -0.036300 0.014261 -2.545 0.0109 * ## race_number:Year2022 -0.019308 0.013265 -1.456 0.1455 ## race_number:Year2023 -0.023999 0.012037 -1.994 0.0462 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 3785.2 on 4123 degrees of freedom ## Residual deviance: 3766.8 on 4113 degrees of freedom ## AIC: 3788.8 ## ## Number of Fisher Scoring iterations: 4 When we look at the summary output from the model, some years are significant while most are not. Oy vey! But, I’m not going to let this stop us from creating a cool figure. So, let’s proceed with a visualization. Before plotting, I’ll need to construct a prediction grid, make predictions using the model, and then tidy up the predictions ina dataframe. # Construct a prediction grid model.grid &lt;- expand.grid(race_number = seq(1, 22, 1), Year = c(&#39;2014&#39;, &#39;2015&#39;, &#39;2016&#39;, &#39;2017&#39;, &#39;2018&#39;, &#39;2019&#39;, &#39;2020&#39;, &#39;2021&#39;, &#39;2022&#39;, &#39;2023&#39;)) model.grid$prob &lt;- predict(dnf.model.7, type=&#39;response&#39;, newdata=model.grid) # compute the fitted lines and SE&#39;s dnf.model.lines &lt;- predict(dnf.model.7, newdata = model.grid, type = &quot;link&quot;, se.fit = TRUE) %&gt;% data.frame() %&gt;% mutate(ll = fit - 1.96 * se.fit, ul = fit + 1.96 * se.fit) %&gt;% dplyr::select( -se.fit) %&gt;% mutate_all(plogis) %&gt;% bind_cols(model.grid) %&gt;% mutate(uncertainty = ul-ll) Now, I’ll try plotting the model as a grid space, colored by the probability of a DNF. ggplot(dnf.model.lines) + geom_tile(aes(race_number, Year, fill=prob)) + scale_fill_viridis_c(labels = scales::percent_format())+ labs(x = &#39;Race number of the season&#39;, y = &#39;Year&#39;, fill= &#39;Estimated Probability of a DNF&#39;, title = &#39;Probability of a DNF by Year and Race Number of the Season&#39;, subtitle = &#39;All Grands Prix: 2014 - 2023&#39;) + theme_bw() + theme_tufte(base_family=&quot;Helvetica&quot;) + theme(plot.title = element_text(hjust = 0.5, size = 15), plot.subtitle = element_text(hjust = 0.5, size = 13), legend.position = &#39;bottom&#39;, legend.spacing.x = unit(0.5, &#39;cm&#39;), axis.title.x = element_text(size = 14), axis.title.y = element_text(size = 14), axis.text.x = element_text(vjust = 0.5, hjust=1, size = 12), axis.text.y = element_text(size = 12)) + scale_x_continuous(labels = seq(1, 22, 1), breaks = seq(1, 22, 1)) + guides(fill = guide_colorbar(barwidth = 12, barheight = 1.0, title.vjust = 0.75)) Pretty Interesting! Despite the varying amounts of statistical significance, this visualization does a good job of describing how DNF probability varies over the course of each season. For instance, in the early years of the turbo-hybrid era, DNFs were consistently high. The probability then proceeds to drop off toward the end of the era in 2019, 2020, and 2021. We see the same type of progression repeat itself under the new ground effects regulations in 2022 and 2023. 7.5 Next Chapter In the next chapter, we will visualize championship title fights. "],["the-wdc-world-drivers-championship.html", "Chapter 8 The WDC (World Drivers Championship)", " Chapter 8 The WDC (World Drivers Championship) Throughout a season, teams and drivers compete for the WCC (World Constructor Championship) and WDC (World Drivers Championship), respectively. The WCC is awarded to the team with the highest total points scored and the WDC is awarded to the driver with the highest total points. During some seasons, the champions are crowned early (i.e. 2023 and 2022), while other championships may not be decided until the final lap of the season (i.e. 2021). Throughout this chapter, I will plot the cumulative points scored for each driver, working backward chronologically. In 2023, we saw one of the most dominant seasons ever by a driver. Max won 19 of 22 races and finished on the podium in every race except Singapore. Lewis Hamilton and Sergio Perez fought for second place in the WDC. Additionally, Fernando Alonso made a welcome return to the front of the grid! # Pull race results races2023 &lt;- race_result_scraper(2023) races2023 %&gt;% mutate(rowid = 1:n()) %&gt;% group_by(Driver) %&gt;% mutate(cumulativePoints = cumsum(Points)) %&gt;% ungroup() %&gt;% ggplot(aes(fct_reorder(Race, rowid), cumulativePoints, group = Driver, col = Driver)) + geom_line() + theme_bw() + theme( axis.title.x = element_text(size = 14), axis.title.y = element_text(size = 14), axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1, size = 12), axis.text.y = element_text(size = 12), plot.title = element_text(hjust = 0.5, size = 18), legend.text = element_text(size = 12), legend.title = element_text(hjust = 0.5, size = 14)) + labs(x = &#39;Grand Prix&#39;, y = &#39;Cumulative Points&#39;, title = &#39;2023 Formula 1 WDC Race&#39;) + scale_color_manual(values = c(&quot;red&quot;, &quot;red&quot;, &quot;cyan3&quot;, &quot;cyan3&quot;, &quot;black&quot;, &quot;black&quot;, &quot;blue&quot;, &quot;blue&quot;, &quot;darkslateblue&quot;, &quot;darkslateblue&quot;, &quot;darkslateblue&quot;, &quot;darkslateblue&quot;, &quot;darkred&quot;, &quot;darkred&quot;, &quot;darkseagreen4&quot;, &quot;darkseagreen4&quot;, &quot;darkorange&quot;, &quot;darkorange&quot;, &quot;cornflowerblue&quot;, &quot;cornflowerblue&quot;, &quot;navy&quot;, &quot;navy&quot;), breaks = c(&quot;LEC&quot;, &quot;SAI&quot;, &quot;HAM&quot;, &quot;RUS&quot;, &quot;MAG&quot;, &quot;HUL&quot;, &quot;OCO&quot;, &quot;GAS&quot;, &quot;TSU&quot;, &quot;DEV&quot;, &quot;RIC&quot;, &quot;LAW&quot;, &quot;ZHO&quot;, &quot;BOT&quot;, &quot;STR&quot;, &quot;ALO&quot;, &quot;PIA&quot;, &quot;NOR&quot;, &quot;SAR&quot;, &quot;ALB&quot;, &quot;PER&quot;, &quot;VER&quot; )) + geom_text_repel(data = ~ subset(.x, Driver %in% c(&#39;VER&#39;, &#39;PER&#39;, &#39;HAM&#39;, &#39;ALO&#39;) &amp; Race == &#39;abu-dhabi&#39;), aes(label = Driver), nudge_x = 2, nudge_y = 3, show.legend = F) Labeling the end of a line in ggplot In the plot above, I used the geom_text_repel() function from the ggrepel package to label the top four point scoring drivers in the 2023 WDC. ggrepel provides geometries for ggplot2 to repel overlapping text labels. These two geometries are: geom_text_repel() geom_label_repel() These functions cause the text labels to repel away from each other, away from data points, and away from edges of the plotting area (panel). For more information on this package, visit: https://cran.r-project.org/web/packages/ggrepel/vignettes/ggrepel.html Note: In the figure above, I subset the data being fed to geom_text_repel(). In this example, I include four drivers (‘VER’, ‘PER’, ‘HAM’, ‘ALO’) and only the final race of the year (abu-dhabi). By using this argument, I am able to place the label only at the very end of the line. Otherwise, ggplot would label each race in the plot. Here’s a plot of the 2022 WDC fight. This year wasn’t exactly a nail-biting affair! The championship appears to be over before it even began. In reality, Verstappen mathematically clinched the championship after the Japanese Grand Prix. # Pull race results races2022 &lt;- race_result_scraper(2022) races2022 %&gt;% mutate(rowid = 1:n()) %&gt;% group_by(Driver) %&gt;% mutate(cumulativePoints = cumsum(Points)) %&gt;% ungroup() %&gt;% ggplot(aes(fct_reorder(Race, rowid), cumulativePoints, group = Driver, col = Driver)) + geom_line() + theme_bw() + theme( axis.title.x = element_text(size = 14), axis.title.y = element_text(size = 14), axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1, size = 12), axis.text.y = element_text(size = 12), plot.title = element_text(hjust = 0.5, size = 18), legend.text = element_text(size = 12), legend.title = element_text(hjust = 0.5, size = 14)) + labs(x = &#39;Grand Prix&#39;, y = &#39;Cumulative Points&#39;, title = &#39;2022 Formula 1 WDC Race&#39;) + scale_color_manual(values = c(&quot;red&quot;, &quot;red&quot;, &quot;cyan3&quot;, &quot;cyan3&quot;, &quot;black&quot;, &quot;black&quot;, &quot;blue&quot;, &quot;blue&quot;, &quot;darkslateblue&quot;, &quot;darkslateblue&quot;, &quot;darkred&quot;, &quot;darkred&quot;, &quot;darkseagreen4&quot;, &quot;darkseagreen4&quot;, &quot;darkseagreen4&quot;, &quot;darkorange&quot;, &quot;darkorange&quot;, &quot;cornflowerblue&quot;, &quot;cornflowerblue&quot;, &quot;cornflowerblue&quot;, &quot;navy&quot;, &quot;navy&quot;), breaks = c(&quot;LEC&quot;, &quot;SAI&quot;, &quot;HAM&quot;, &quot;RUS&quot;, &quot;MAG&quot;, &quot;MSC&quot;, &quot;OCO&quot;, &quot;ALO&quot;, &quot;GAS&quot;, &quot;TSU&quot;, &quot;ZHO&quot;, &quot;BOT&quot;, &quot;STR&quot;, &quot;VET&quot;, &quot;HUL&quot;, &quot;RIC&quot;, &quot;NOR&quot;, &quot;LAT&quot;, &quot;DEV&quot;, &quot;ALB&quot;, &quot;PER&quot;, &quot;VER&quot; )) + geom_text_repel(data = ~ subset(.x, Driver %in% c(&#39;VER&#39;, &#39;PER&#39;, &#39;LEC&#39;) &amp; Race == &#39;abu-dhabi&#39;), aes(label = Driver), nudge_x = 2, nudge_y = 3, show.legend = F) The 2021 Formula 1 season was epic! That year’s championship fight was not settled until the final lap of the season. Max Verstappen won the Drivers’ Championship for the first time in his career and became the first non-Mercedes driver in the turbo-hybrid era to win the World Championship. Max Verstappen and Lewis Hamilton engaged in a year-long fight for the title. Verstappen and Hamilton swapped the championship lead multiple times during the 2021 season and entered the season finale, the Abu Dhabi Grand Prix, tied on points. Ultimately, Max overtook Hamilton on the final lap of the Grand Prix after a safety car restart. A truly legendary season. This season also marked the first time since 2008 that the WDC did not race for the WCC (World Constructors Champion). # Pull race results races2021 &lt;- race_result_scraper(2021) races2021 %&gt;% mutate(rowid = 1:n()) %&gt;% group_by(Driver) %&gt;% mutate(cumulativePoints = cumsum(Points)) %&gt;% ungroup() %&gt;% ggplot(aes(fct_reorder(Race, rowid), cumulativePoints, group = Driver, col = Driver)) + geom_line() + theme_bw() + theme( axis.title.x = element_text(size = 14), axis.title.y = element_text(size = 14), axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1, size = 12), axis.text.y = element_text(size = 12), plot.title = element_text(hjust = 0.5, size = 18), legend.text = element_text(size = 12), legend.title = element_text(hjust = 0.5, size = 14)) + labs(x = &#39;Grand Prix&#39;, y = &#39;Cumulative Points&#39;, title = &#39;2021 Formula 1 WDC Race&#39;) + scale_color_manual(values = c(&quot;red&quot;, &quot;red&quot;, &quot;cyan3&quot;, &quot;cyan3&quot;, &quot;black&quot;, &quot;black&quot;, &quot;blue&quot;, &quot;blue&quot;, &quot;darkslateblue&quot;, &quot;darkslateblue&quot;, &quot;darkred&quot;, &quot;darkred&quot;, &quot;darkred&quot;, &quot;darkseagreen4&quot;, &quot;darkseagreen4&quot;, &quot;darkorange&quot;, &quot;darkorange&quot;, &quot;cornflowerblue&quot;, &quot;cornflowerblue&quot;, &quot;navy&quot;, &quot;navy&quot;), breaks = c(&quot;LEC&quot;, &quot;SAI&quot;, &quot;HAM&quot;, &quot;BOT&quot;, &quot;MAZ&quot;, &quot;MSC&quot;, &quot;OCO&quot;, &quot;ALO&quot;, &quot;GAS&quot;, &quot;TSU&quot;, &quot;RAI&quot;, &quot;GIO&quot;, &quot;KUB&quot;, &quot;STR&quot;, &quot;VET&quot;, &quot;RIC&quot;, &quot;NOR&quot;, &quot;LAT&quot;, &quot;RUS&quot;, &quot;PER&quot;, &quot;VER&quot; )) + geom_text_repel(data = ~ subset(.x, Driver %in% c(&#39;VER&#39;, &#39;HAM&#39;, &#39;BOT&#39;, &#39;PER&#39;) &amp; Race == &#39;abu-dhabi&#39;), aes(label = Driver), nudge_x = 1.5, nudge_y = 3, show.legend = F) The 2020 season was postponed until July due to the COVID-19 pandemic. A total of 17 races took place in 2020 because the schedule experienced several cancellations and new replacement races. Lewis Hamilton and Mercedes continued their dominance and won their seventh WDC and WCC, respectively. Mercedes locked up the WCC at the Emilia Romagna Grand Prix, while Lewis Hamilton tied Michael Schumacher’s record of seven WDC. # Pull race results races2020 &lt;- race_result_scraper(2020) races2020 %&gt;% mutate(rowid = 1:n()) %&gt;% group_by(Driver) %&gt;% mutate(cumulativePoints = cumsum(Points)) %&gt;% ungroup() %&gt;% ggplot(aes(fct_reorder(Race, rowid), cumulativePoints, group = Driver, col = Driver)) + geom_line() + theme_bw() + theme( axis.title.x = element_text(size = 14), axis.title.y = element_text(size = 14), axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1, size = 12), axis.text.y = element_text(size = 12), plot.title = element_text(hjust = 0.5, size = 18), legend.text = element_text(size = 12), legend.title = element_text(hjust = 0.5, size = 14)) + labs(x = &#39;Grand Prix&#39;, y = &#39;Cumulative Points&#39;, title = &#39;2020 Formula 1 WDC Race&#39;) + geom_text_repel(data = ~ subset(.x, Driver %in% c(&#39;VER&#39;, &#39;HAM&#39;, &#39;BOT&#39;, &#39;PER&#39;) &amp; Race == &#39;abu-dhabi&#39;), aes(label = Driver), nudge_x = 1.5, nudge_y = 3, show.legend = F) + scale_color_manual(values = c(&quot;red&quot;, &quot;red&quot;, &quot;cyan3&quot;, &quot;cyan3&quot;, &quot;black&quot;, &quot;black&quot;, &quot;black&quot;, &quot;yellow&quot;, &quot;yellow&quot;, &quot;darkslateblue&quot;, &quot;darkslateblue&quot;, &quot;darkred&quot;, &quot;darkred&quot;, &quot;pink&quot;, &quot;pink&quot;, &quot;pink&quot;, &quot;darkorange&quot;, &quot;darkorange&quot;, &quot;cornflowerblue&quot;, &quot;cornflowerblue&quot;, &quot;cornflowerblue&quot;, &quot;navy&quot;, &quot;navy&quot; ), breaks = c(&quot;LEC&quot;, &quot;VET&quot;, &quot;HAM&quot;, &quot;BOT&quot;, &quot;MAG&quot;, &quot;GRO&quot;, &quot;FIT&quot;, &quot;OCO&quot;, &quot;RIC&quot;, &quot;GAS&quot;, &quot;KVY&quot;, &quot;RAI&quot;, &quot;GIO&quot;, &quot;STR&quot;, &quot;PER&quot;, &quot;HUL&quot;, &quot;SAI&quot;, &quot;NOR&quot;, &quot;LAT&quot;, &quot;RUS&quot;, &quot;AIT&quot;, &quot;ALB&quot;, &quot;VER&quot; )) In 2019, Lewis Hamilton and Mercedes both defended their championships, winning their sixth. Max Verstappen was able to finish third, ahead of both Ferrari drivers, in the WDC. # Pull race results races2019 &lt;- race_result_scraper(2019) races2019 %&gt;% mutate(rowid = 1:n()) %&gt;% group_by(Driver) %&gt;% mutate(cumulativePoints = cumsum(Points)) %&gt;% ungroup() %&gt;% ggplot(aes(fct_reorder(Race, rowid), cumulativePoints, group = Driver, col = Driver)) + geom_line() + theme_bw() + theme( axis.title.x = element_text(size = 14), axis.title.y = element_text(size = 14), axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1, size = 12), axis.text.y = element_text(size = 12), plot.title = element_text(hjust = 0.5, size = 18), legend.text = element_text(size = 12), legend.title = element_text(hjust = 0.5, size = 14)) + labs(x = &#39;Grand Prix&#39;, y = &#39;Cumulative Points&#39;, title = &#39;2019 Formula 1 WDC Race&#39;) + geom_text_repel(data = ~ subset(.x, Driver %in% c(&#39;VER&#39;, &#39;HAM&#39;, &#39;BOT&#39;, &#39;LEC&#39;, &#39;VET&#39;) &amp; Race == &#39;abu-dhabi&#39;), aes(label = Driver), nudge_x = 1.5, nudge_y = 1.5, show.legend = F) + scale_color_manual(values = c(&quot;red&quot;, &quot;red&quot;, &quot;cyan3&quot;, &quot;cyan3&quot;, &quot;black&quot;, &quot;black&quot;, &quot;yellow&quot;, &quot;yellow&quot;, &quot;darkslateblue&quot;, &quot;darkred&quot;, &quot;darkred&quot;, &quot;pink&quot;, &quot;pink&quot;, &quot;pink&quot;, &quot;darkorange&quot;, &quot;darkorange&quot;, &quot;cornflowerblue&quot;, &quot;cornflowerblue&quot;, &quot;navy&quot;, &quot;navy&quot;, &quot;navy&quot;), breaks = c(&quot;LEC&quot;, &quot;VET&quot;, &quot;HAM&quot;, &quot;BOT&quot;, &quot;MAG&quot;, &quot;GRO&quot;, &quot;OCO&quot;, &quot;RIC&quot;, &quot;KVY&quot;, &quot;RAI&quot;, &quot;GIO&quot;, &quot;STR&quot;, &quot;PER&quot;, &quot;SAI&quot;, &quot;NOR&quot;, &quot;LAT&quot;, &quot;RUS&quot;, &quot;ALB&quot;, &quot;VER&quot;, &quot;GAS&quot; )) For the second straight year, Mercedes and Ferrari battled for the WCC. Two four-time WDCs, Lewis Hamilton and Sebastian Vettel, battled for a fifth championship. Hamilton and Vettel swapped the WDC lead five times throughout 2018. In fact, Vettel held the lead at the midway point of the season. But, in the end, Hamilton prevailed and secured his fifth title. # Pull race results races2018 &lt;- race_result_scraper(2018) races2018 %&gt;% mutate(rowid = 1:n()) %&gt;% group_by(Driver) %&gt;% mutate(cumulativePoints = cumsum(Points)) %&gt;% ungroup() %&gt;% ggplot(aes(fct_reorder(Race, rowid), cumulativePoints, group = Driver, col = Driver)) + geom_line() + theme_bw() + theme( axis.title.x = element_text(size = 14), axis.title.y = element_text(size = 14), axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1, size = 12), axis.text.y = element_text(size = 12), plot.title = element_text(hjust = 0.5, size = 18), legend.text = element_text(size = 12), legend.title = element_text(hjust = 0.5, size = 14)) + labs(x = &#39;Grand Prix&#39;, y = &#39;Cumulative Points&#39;, title = &#39;2018 Formula 1 WDC Race&#39;) + geom_text_repel(data = ~ subset(.x, Driver %in% c(&#39;VER&#39;, &#39;HAM&#39;, &#39;BOT&#39;, &#39;RAI&#39;, &#39;VET&#39;, &#39;RIC&#39;) &amp; Race == &#39;abu-dhabi&#39;), aes(label = Driver), nudge_x = 1.5, nudge_y = 1, show.legend = F) + scale_color_manual(values = c(&quot;red&quot;, &quot;red&quot;, &quot;cyan3&quot;, &quot;cyan3&quot;, &quot;black&quot;, &quot;black&quot;, &quot;yellow&quot;, &quot;yellow&quot;, &quot;darkslateblue&quot;, &quot;darkslateblue&quot;, &quot;darkred&quot;, &quot;darkred&quot;, &quot;pink&quot;, &quot;pink&quot;, &quot;darkorange&quot;, &quot;darkorange&quot;, &quot;cornflowerblue&quot;, &quot;cornflowerblue&quot;, &quot;navy&quot;, &quot;navy&quot;), breaks = c(&quot;RAI&quot;, &quot;VET&quot;, &quot;HAM&quot;, &quot;BOT&quot;, &quot;MAG&quot;, &quot;GRO&quot;, &quot;SAI&quot;, &quot;HUL&quot;, &quot;HAR&quot;, &quot;GAS&quot;, &quot;ERI&quot;, &quot;LEC&quot;, &quot;OCO&quot;, &quot;PER&quot;, &quot;VAN&quot;, &quot;ALO&quot;, &quot;STR&quot;, &quot;SIR&quot;, &quot;RIC&quot;, &quot;VER&quot; )) After three straight years of teammates battling for the WDC, there was a legitimate battle between teams in 2017. Lewis Hamilton battled four-time WDC Sebastian Vettel. Vettel lead the WDC for the first 12 rounds of the season, but Hamilton ultimately won his fourth championship by 46 points. # Pull race results races2017 &lt;- race_result_scraper(2017) races2017 %&gt;% mutate(rowid = 1:n()) %&gt;% group_by(Driver) %&gt;% mutate(cumulativePoints = cumsum(Points)) %&gt;% ungroup() %&gt;% ggplot(aes(fct_reorder(Race, rowid), cumulativePoints, group = Driver, col = Driver)) + geom_line() + theme_bw() + theme( axis.title.x = element_text(size = 14), axis.title.y = element_text(size = 14), axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1, size = 12), axis.text.y = element_text(size = 12), plot.title = element_text(hjust = 0.5, size = 18), legend.text = element_text(size = 12), legend.title = element_text(hjust = 0.5, size = 14)) + labs(x = &#39;Grand Prix&#39;, y = &#39;Cumulative Points&#39;, title = &#39;2017 Formula 1 WDC Race&#39;) + geom_text_repel(data = ~ subset(.x, Driver %in% c(&#39;RIC&#39;, &#39;HAM&#39;, &#39;BOT&#39;, &#39;RAI&#39;, &#39;VET&#39;, &#39;VER&#39;) &amp; Race == &#39;abu-dhabi&#39;), aes(label = Driver), nudge_x = 1.5, nudge_y = 1, show.legend = F) + scale_color_manual(values = c(&quot;red&quot;, &quot;red&quot;, &quot;cyan3&quot;, &quot;cyan3&quot;, &quot;black&quot;, &quot;black&quot;, &quot;yellow&quot;, &quot;yellow&quot;, &quot;yellow&quot;, &quot;darkslateblue&quot;, &quot;darkslateblue&quot;, &quot;darkslateblue&quot;, &quot;darkred&quot;, &quot;darkred&quot;, &quot;darkred&quot;, &quot;pink&quot;, &quot;pink&quot;, &quot;darkorange&quot;, &quot;darkorange&quot;, &quot;darkorange&quot;, &quot;cornflowerblue&quot;, &quot;cornflowerblue&quot;, &quot;cornflowerblue&quot;, &quot;navy&quot;, &quot;navy&quot;), breaks = c(&quot;RAI&quot;, &quot;VET&quot;, &quot;HAM&quot;, &quot;BOT&quot;, &quot;MAG&quot;, &quot;GRO&quot;, &quot;SAI&quot;, &quot;HUL&quot;, &quot;PAL&quot;, &quot;HAR&quot;, &quot;GAS&quot;, &quot;KYV&quot;, &quot;ERI&quot;, &quot;GIO&quot;, &quot;WEH&quot;, &quot;OCO&quot;, &quot;PER&quot;, &quot;VAN&quot;, &quot;ALO&quot;, &quot;BUT&quot;, &quot;STR&quot;, &quot;MAS&quot;, &quot;DIR&quot;, &quot;RIC&quot;, &quot;VER&quot; )) In 2016, Nico Rosberg won his first and only WDC title in the final race of the season. Rosberg beat his teammate Lewis Hamilton thanks to nine race wins and seven additional podiums. With this WDC title, Nico became only the second son of a champion to become champion himself. Pretty cool! Following this title win, Nico retired from Formula 1. # Pull race results races2016 &lt;- race_result_scraper(2016) races2016 %&gt;% mutate(rowid = 1:n()) %&gt;% group_by(Driver) %&gt;% mutate(cumulativePoints = cumsum(Points)) %&gt;% ungroup() %&gt;% ggplot(aes(fct_reorder(Race, rowid), cumulativePoints, group = Driver, col = Driver)) + geom_line() + theme_bw() + theme( axis.title.x = element_text(size = 14), axis.title.y = element_text(size = 14), axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1, size = 12), axis.text.y = element_text(size = 12), plot.title = element_text(hjust = 0.5, size = 18), legend.text = element_text(size = 12), legend.title = element_text(hjust = 0.5, size = 14)) + labs(x = &#39;Grand Prix&#39;, y = &#39;Cumulative Points&#39;, title = &#39;2016 Formula 1 WDC Race&#39;) + geom_text_repel(data = ~ subset(.x, Driver %in% c(&#39;RIC&#39;, &#39;HAM&#39;, &#39;ROS&#39;, &#39;RAI&#39;, &#39;VET&#39;, &#39;VER&#39;) &amp; Race == &#39;abu-dhabi&#39;), aes(label = Driver), nudge_x = 1.5, nudge_y = 1, show.legend = F) + scale_color_manual(values = c(&quot;red&quot;, &quot;red&quot;, &quot;cyan3&quot;, &quot;cyan3&quot;, &quot;black&quot;, &quot;black&quot;, &quot;yellow&quot;, &quot;yellow&quot;, &quot;darkslateblue&quot;, &quot;darkslateblue&quot;, &quot;darkred&quot;, &quot;darkred&quot;, &quot;pink&quot;, &quot;pink&quot;, &quot;darkorange&quot;, &quot;darkorange&quot;, &quot;darkorange&quot;, &quot;cornflowerblue&quot;, &quot;cornflowerblue&quot;, &quot;navy&quot;, &quot;navy&quot;, &quot;blue&quot;, &quot;blue&quot;, &quot;blue&quot;), breaks = c(&quot;RAI&quot;, &quot;VET&quot;, &quot;HAM&quot;, &quot;ROS&quot;, &quot;GUT&quot;, &quot;GRO&quot;, &quot;MAG&quot;, &quot;PAL&quot;, &quot;SAI&quot;, &quot;KYV&quot;, &quot;ERI&quot;, &quot;NAS&quot;, &quot;HUL&quot;, &quot;PER&quot;, &quot;VAN&quot;, &quot;ALO&quot;, &quot;BUT&quot;, &quot;BOT&quot;, &quot;MAS&quot;, &quot;RIC&quot;, &quot;VER&quot;, &quot;HAR&quot;, &quot;OCO&quot;, &quot;WEH&quot; )) Lewis Hamilton won his third overall and second straight WDC in 2015. He managed to beat his teammate Nico Rosberg and four-time WDC Sebastian Vettel. # Pull race results races2015 &lt;- race_result_scraper(2015) races2015 %&gt;% mutate(rowid = 1:n()) %&gt;% group_by(Driver) %&gt;% mutate(cumulativePoints = cumsum(Points)) %&gt;% ungroup() %&gt;% ggplot(aes(fct_reorder(Race, rowid), cumulativePoints, group = Driver, col = Driver)) + geom_line() + theme_bw() + theme( axis.title.x = element_text(size = 14), axis.title.y = element_text(size = 14), axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1, size = 12), axis.text.y = element_text(size = 12), plot.title = element_text(hjust = 0.5, size = 18), legend.text = element_text(size = 12), legend.title = element_text(hjust = 0.5, size = 14)) + labs(x = &#39;Grand Prix&#39;, y = &#39;Cumulative Points&#39;, title = &#39;2015 Formula 1 WDC Race&#39;) + geom_text_repel(data = ~ subset(.x, Driver %in% c(&#39;HAM&#39;, &#39;ROS&#39;, &#39;RAI&#39;, &#39;VET&#39;) &amp; Race == &#39;abu-dhabi&#39;), aes(label = Driver), nudge_x = 1.5, nudge_y = 1, show.legend = F) + scale_color_manual(values = c(&quot;red&quot;, &quot;red&quot;, &quot;cyan3&quot;, &quot;cyan3&quot;, &quot;black&quot;, &quot;black&quot;, &quot;grey&quot;, &quot;grey&quot;, &quot;grey&quot;, &quot;darkslateblue&quot;, &quot;darkslateblue&quot;, &quot;darkred&quot;, &quot;darkred&quot;, &quot;pink&quot;, &quot;pink&quot;, &quot;darkorange&quot;, &quot;darkorange&quot;, &quot;darkorange&quot;, &quot;cornflowerblue&quot;, &quot;cornflowerblue&quot;, &quot;navy&quot;, &quot;navy&quot;, &quot;blue&quot;, &quot;blue&quot;, &quot;blue&quot;), breaks = c(&quot;RAI&quot;, &quot;VET&quot;, &quot;HAM&quot;, &quot;ROS&quot;, &quot;MAL&quot;, &quot;GRO&quot;, &quot;STE&quot;, &quot;WEH&quot;, &quot;ROS&quot;, &quot;SAI&quot;, &quot;VER&quot;, &quot;ERI&quot;, &quot;NAS&quot;, &quot;HUL&quot;, &quot;PER&quot;, &quot;MAG&quot;, &quot;ALO&quot;, &quot;BUT&quot;, &quot;BOT&quot;, &quot;MAS&quot;, &quot;RIC&quot;, &quot;KYV&quot;, &quot;HAR&quot;, &quot;OCO&quot;, &quot;WEH&quot; )) 2014 was an important year in Formula 1. It marked the beginning of a Mercedes dynasty, the onset of a Lewis Hamilton reign, and the ushering in of a revised engine formula. In 2014, the previous era’s V8 engine was replaced with a turbocharged V6 engine that included an energy recovery system (ERS). Sebastian Vettel and Red Bull were the defending WDC and WCC, respectively. However, it became immediately evident that Mercedes would dominate this season. Lewis Hamilton won his second WDC (his previous title was with McLaren) and Mercedes dominated the WCC fight, beating Red Bull by nearly a 300 point margin. # Pull race results races2014 &lt;- race_result_scraper(2014) races2014 %&gt;% mutate(rowid = 1:n()) %&gt;% group_by(Driver) %&gt;% mutate(cumulativePoints = cumsum(Points)) %&gt;% ungroup() %&gt;% ggplot(aes(fct_reorder(Race, rowid), cumulativePoints, group = Driver, col = Driver)) + geom_line() + theme_bw() + theme( axis.title.x = element_text(size = 14), axis.title.y = element_text(size = 14), axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1, size = 12), axis.text.y = element_text(size = 12), plot.title = element_text(hjust = 0.5, size = 18), legend.text = element_text(size = 12), legend.title = element_text(hjust = 0.5, size = 14)) + labs(x = &#39;Grand Prix&#39;, y = &#39;Cumulative Points&#39;, title = &#39;2014 Formula 1 WDC Race&#39;) + geom_text_repel(data = ~ subset(.x, Driver %in% c(&#39;HAM&#39;, &#39;ROS&#39;, &#39;RIC&#39;, &#39;VET&#39;) &amp; Race == &#39;abu-dhabi&#39;), aes(label = Driver), nudge_x = 1.5, nudge_y = 1, show.legend = F) + scale_color_manual(values = c(&quot;red&quot;, &quot;red&quot;, &quot;cyan3&quot;, &quot;cyan3&quot;, &quot;black&quot;, &quot;black&quot;, &quot;grey&quot;, &quot;grey&quot;, &quot;grey&quot;, &quot;darkslateblue&quot;, &quot;darkslateblue&quot;, &quot;darkred&quot;, &quot;darkred&quot;, &quot;pink&quot;, &quot;pink&quot;, &quot;darkorange&quot;, &quot;darkorange&quot;, &quot;cornflowerblue&quot;, &quot;cornflowerblue&quot;, &quot;navy&quot;, &quot;navy&quot;, &quot;seagreen&quot;, &quot;seagreen&quot;, &quot;seagreen&quot;, &quot;seagreen&quot;), breaks = c(&quot;RAI&quot;, &quot;ALO&quot;, &quot;HAM&quot;, &quot;ROS&quot;, &quot;MAL&quot;, &quot;GRO&quot;, &quot;CHI&quot;, &quot;BIA&quot;, &quot;ROS&quot;, &quot;KYV&quot;, &quot;VER&quot;, &quot;SUT&quot;, &quot;GUT&quot;, &quot;HUL&quot;, &quot;PER&quot;, &quot;MAG&quot;, &quot;BUT&quot;, &quot;BOT&quot;, &quot;MAS&quot;, &quot;RIC&quot;, &quot;VET&quot;, &quot;ERI&quot;, &quot;STE&quot;, &quot;KOB&quot;, &quot;LOT&quot; )) "],["conclusions.html", "Chapter 9 Conclusions", " Chapter 9 Conclusions I sincerely hope this book extends your data visualization capabilities. Or, at the very least, I hope reading through this book was not a complete waste of your time! I owe so much of my career enjoyment to R and ggplot2, and I wanted to pay it forward, so to speak. I fully understand that the targeted audience for a book like this is quite small! Nonetheless, I welcome feedback, comments, and questions about this book or otherwise. Feel free to email me (speedandpowerlab@gmail.com) or find me on Linkedin. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
